{"queries":["Enterprise RAG system architecture","Real-world RAG system implementation","RAG system case studies enterprise","Chinese language enterprise RAG system best practices","Enterprise AI search system architecture"],"results":[{"id":"bocha-0","title":"费跃-构建企业级 RAG 系统的创新实践.pdf-在线下载-三个皮匠报告","url":"https://www.sgpjbg.com/bgdown/186916.html","snippet":"您的当前位置：首页 > 报告分类 > PDF报告下载 报告预览 费跃-构建企业级 RAG 系统的创新实践.pdf 编号：186916 PDF 36页 4.96MB 下载积分：VIP专享 下载格式选择 下载PDF 下载报告请您先登录！ 费跃-构建企业级 RAG 系统的创新实践.pdf 1、构建企业级RAG系统的创新实践演讲人：费跃阿里云/PAI 人工智能平台01010202030304040505目录背景介绍模块化RAG架构模块设计和优化企业级RAG能力集成总结背景介绍背景介绍检索增强生成（Retrieval Augmented Generation,RAG）从数据源中检索信息来辅助大语言模型（Large Language Model,LLM）生成答案。RAG的优势：准确性时效性数据安全准确性一致性可解释性效果优访问控制合规隐私数据管理数据安全低延迟可伸缩大规模知识库构建和查询高性能无缝集成可观测在线评估系统集成企业级RAG系统的挑战客户的知识库领域、格式、内容的多样性，效果难2、以保证需求具有多样性，常规RAG链路难以满足不同场景的定制化需求RAG优化是一个系统性工程，可靠性、高性能、高质量难以取舍数据隐私和安全问题，实现私有化部署和安全合规访问企业级RAG系统架构模块化RAG架构模块化RAG图片来源:https:/arxiv.org/pdf/2407.21059模块化RAG可扩展，适应不同场景的需求可调优，各模块可独立配置、评估、优化可维护，模块间松耦合高级RAG预检索+重排序检索优化：提高检索效率并加强检索块的利用率模块化RAG架构模块化设计白盒化：模块可以灵活添加/修改快速构建：可通过配置文件/UI dashboard修改模块配置代码开源：兼容LlamaInde3、x开源协议模块编排：通过编排和路由匹配不同场景需求模块评估：自动生成数据集，系统评估端到端和各模块性能模块设计和优化文档解析文档解析的挑战格式多样性格式多样性：企业级数据格式多样，需要适配不同类型文件的结构和内容内容复杂性内容复杂性：文档内容可能包含文本、图像、表格、公式、标题、代码块等多种复杂元素。非结构化文本非结构化文本：PDF或者扫描类文档解析难度高文档更新迭代文档更新迭代：随着数据不断更新，知识库中内容会过期失效文档解析非结构化文件非结构化文件转Markdown格式格式友好，清晰易读保存标题、表格、图片等元素信息复杂度降维，后续切分无需关注输入的文件类型结构化文件结构化文件JSON编码4、存储表格key-value信息自动行表列表检测合并单元格拆分PDF解析算法难点：版面识别/表格解析闭源PDF解析APIDocument MindLlama Parse开源PDF解析模型EasyOcrPDF-Extract-Kit文本切块非结构转Markdown为什么需要文本切块提高检索准确性 -减少索引内容的噪音 -embedding的序列长度限制提升模型生成效果 -更相关的上下文 -减少延迟切块策略的难点选择合适大小，太大检索效果差，太小信息缺失。保留文本块语义独立性、完整性文本切块策略固定长度切块优点:实现简单，性能块，大小固定缺点:不够聪明，语义被切断语义切块优点：语义信息完整，有利检索5、缺点：计算复杂，阈值难取递归分块优点：内容连贯完整，灵活缺点：计算复杂，效果受限于分隔符文档结构分块优点：保留文档结构、元素信息，内容连贯完整缺点：计算复杂，效果受限于文档结构LLM分块优点：效果最优缺点：速度慢，代价太高文本切块策略非结构转Markdown默认切块策略1.Markdown输入，降低解析复杂度2.按文档结构递归解析，语义连贯3.特殊元素处理表格标题图片列表代码块查询重写为什么需要查询重写用户的提问通常比较口语化，直接用问题检索效果不佳减少查询和文档之间的语义差异多轮对话中的检索，需要指代消解查询重写策略子问题查询，生成相关子问题，补充query的细节假设文档（HyDE）回溯提示6、（STEP-BACK Prompting)查询扩展，伪相关反馈提供领域知识补充查询路由数据源路由：根据问题选择特定数据源的信息：向量数据库：相似度搜索关系数据库：结构化数据查询图数据库：实体关系查询外部API：特定需求查询组件路由：根据问题选择特定的组件向量数据库（FAQ）LLMAgentPrompt路由：根据问题选择对应的prompt不同问题场景切换prompt多语言查询的prompt切换检索向量检索 Embedding模型选择MTEB榜单知识库的语言（中文/英文/多语言）文本的序列长度混合检索向量检索的劣势：特定术语匹配（产品名称或者型号等），私域数 友情提示 1、下载报告失败解决办法 2、PDF文件下载后，可能会被浏览器默认打开，此种情况可以点击浏览器菜单，保存网页到桌面，就可以正常下载了。 3、本站不支持迅雷下载，请使用电脑自带的IE浏览器，或者360浏览器、谷歌浏览器下载即可。 4、本站报告下载后的文档和图纸-无水印,预览文档经过压缩，下载后原文更清晰。 本文（费跃-构建企业级 RAG 系统的创新实践.pdf）为本站 （learning） 主动上传，三个皮匠报告文库仅提供信息存储空间，仅对用户上传内容的表现方式做保护处理，对上载内容本身不做任何修改或编辑。 若此文所含内容侵犯了您的版权或隐私，请立即通知三个皮匠报告文库（点击联系客服），我们立即给予删除！ 温馨提示：如果因为网速或其他原因下载失败请重新下载，重复下载不扣分。 下载提示 正在下载，请耐心等待... 如果您下载失败，请填写邮箱地址，系统会自动将文件发送到您的邮箱里 备注：如果文件较大，则可能需要等待一会收到邮件。 提交发送 若下次需重新下载，请登录后访问：我的下载记录 已下载完毕，关闭 全行业研究报告分享下载平台 0731-84720580 商务合作：really158d 友链申请 (QQ)：1737380874 关于我们 网站声明 免责声明 网站公告 我要入驻 更多 常见问题 用户协议 认证协议 版权申诉 关于我们 三个皮匠报告微信公众号 三个皮匠报告微信小程序 扫码咨询网站充值下载问题 友情链接： 营销自动化 亿欧智库 微播易 阿里妈妈 copyright@2008-2013 长沙景略智创信息技术有限公司版权所有 网站备案/许可证号：湘B2-20190120 | 工信部备案号：湘ICP备17000430号-2 | 公安备案号：湘公网安备43010402001071号 客服 商务合作 小程序 服务号 回到顶部 折叠 微信扫码登录 微信扫码登录 双重福利，扫码领取！ 关注公众号『三个皮匠报告』，免费领取报告 1.100+最新优质行业报告 2.每周免费精选报告分享 手机快捷登录 账号登录 记住密码 忘记密码？ 验证即登录，未注册将自动创建账号 获取验证码 收不到验证码注意事项 登录代表您已阅读并同意《三个皮匠服务条款》","source":"web","publishedAt":"2025-01-07T18:09:48+08:00"},{"id":"bocha-1","title":"费跃-构建企业级 RAG 系统的创新实践.pdf_三个皮匠报告","url":"https://www.sgpjbg.com/baogao/186916.html","snippet":"请登录后查看文档详情 全行业研究报告分享下载平台 0731-84720580 商务合作：really158d 友链申请 (QQ)：1737380874 关于我们 网站声明 免责声明 网站公告 我要入驻 更多 常见问题 用户协议 认证协议 版权申诉 关于我们 三个皮匠报告微信公众号 三个皮匠报告微信小程序 扫码咨询网站充值下载问题 友情链接： 营销自动化 亿欧智库 微播易 阿里妈妈 copyright@2008-2013 长沙景略智创信息技术有限公司版权所有 网站备案/许可证号：湘B2-20190120 | 工信部备案号：湘ICP备17000430号-2 | 公安备案号：湘公网安备43010402001071号 客服 商务合作 小程序 服务号 回到顶部 折叠 微信扫码登录 微信扫码登录 双重福利，扫码领取！ 关注公众号『三个皮匠报告』，免费领取报告 1.100+最新优质行业报告 2.每周免费精选报告分享 手机快捷登录 账号登录 记住密码 忘记密码？ 验证即登录，未注册将自动创建账号 获取验证码 收不到验证码注意事项 登录代表您已阅读并同意《三个皮匠服务条款》","source":"web","publishedAt":"2024-12-17T10:55:00+08:00"},{"id":"bocha-2","title":"Study996 个人主页","url":"https://devpress.csdn.net/user/Study996","snippet":"@Study996 Study996 关注 2025-10-25 15:10:19 加入 DevPress 抖音号： 简介 该用户还未填写简介 擅长的技术栈 未填写擅长的技术栈 可提供的服务 暂无可提供的服务 一文搞懂AI大语言模型工作原理，初中生都能看懂 本文系统介绍了神经网络与Transformer的工作原理及其训练过程。首先解释了神经元的基本概念和人工神经网络的结构，通过买肉案例形象说明神经元计算过程。然后详细阐述了神经网络的训练机制、循环神经网络(RNN)的特点及其局限性。重点解析了Transformer的核心模块——多头自注意力机制，通过圆桌会议的比喻生动说明了其工作原理。文章还完整介绍了大语言模型训练的四个关键阶段：预训练、监督微调、奖励 #机器学习#人工智能#AI +2【DeepSeek实战】高质量提示词的六大技巧 高质量提示词是解锁AI潜力的关键。本文介绍了六大提示词优化框架：1. STAR法则（情景-任务-行动-结果）使问题结构化；2. 5W2H分析法确保全面提问；3. CO-STAR框架（上下文-目标-风格-语气-受众-回复）精准控制输出；4. CRISPE框架激发跨学科创意；5. BROKE框架（背景-角色-目标-关键结果-优化）实现量化目标；6. 借助AI自身优化提示词。每个框架均配有具体应用场景示 #人工智能#AI#DeepSeek 普通LLM智能体与具身智能：从语言理解到自主行动 本文探讨了大语言模型(LLM)的演进及其两种主要应用形态：普通LLM智能体和具身智能(Agentic AI)。普通LLM智能体以被动响应方式运行，擅长语言处理和知识检索，如企业智能客服系统。而具身智能则具备主动规划、环境感知和自我学习能力，如小鹏汽车的智能驾驶系统。文章指出，具身智能的发展是迈向通用人工智能(AGI)的关键，它赋予了AI系统目标理解、自主决策和环境适应等更接近人类智能的特征。通过对 #人工智能 构建属于自己的创意/设计AI智能体？ 本文探讨了从通用AI模型向专业领域AI模型的发展趋势，特别聚焦设计领域。文章分析了隐性知识在设计中的重要性及其特征（隐含性、情境特定性等），提出了通过定性研究和AI交互记录法提取隐性知识的方法。重点阐述了如何将设计师行为转化为知识向量，并构建能够理解设计意图、生成连贯行动序列的智能体。文章还讨论了模型训练策略（LoRA、RLHF等）及当前面临的挑战，如知识提取完整性和智能体泛化能力。最后指出，通过 #人工智能#AI#生成式AI 大模型核心技术：微调、推理与优化详细指南，推荐收藏！ 本文系统介绍了大语言模型（LLM）的关键技术。首先阐述了LLM的基本概念和Transformer架构原理，包括编码器-解码器结构和自注意力机制。接着详细讲解了语言建模的发展历程，从n-gram到神经语言模型的演进。在模型训练方面，重点分析了预训练的计算挑战、分布式训练方法（DDP和FSDP）以及微调技术，特别是参数高效微调（PEFT）方法如LoRA、QLoRA和适配器等。此外，还探讨了提示工程策略 #人工智能#AI#机器学习 +1RAG（检索增强生成）架构与原理：告别LLM“幻觉”的秘密武器 RAG（检索增强生成）是一种优化大语言模型输出的技术，通过从外部知识库检索相关信息作为上下文输入，显著提高回答的准确性、可靠性和时效性。其工作流程分为检索（数据准备、嵌入、向量存储）和生成（上下文整合、LLM生成）两个阶段，能有效缓解LLM的幻觉问题。RAG具有提高准确性、增强可靠性、降低成本等优势，但也面临检索质量、分块策略等挑战。目前主流开源实现方案包括LangChain、LlamaIndex #数据库#运维#人工智能 +1大语言模型的详解与训练 本文系统介绍了大语言模型（LLM）的定义、特点、能力及训练方法。LLM与传统预训练模型的核心差异在于其庞大的参数量（十亿至千亿级）和海量训练数据，使其具备涌现能力、上下文学习、指令遵循和逐步推理等独特优势。文章详细阐述了LLM的三阶段训练流程：预训练（Pretrain）构建基础能力，监督微调（SFT）培养指令遵循能力，以及人类反馈强化学习（RLHF）实现价值观对齐。同时分析了训练过程中的关键技术挑 #语言模型#人工智能#自然语言处理 +1AI Agent到底是啥？一文速通 AI Agent作为新一代智能实体，凭借感知、规划、行动和记忆四大核心能力，正在从辅助工具进化为能独立完成复杂任务的数字员工。不同于传统AI系统，AI Agent通过整合大模型与外部工具，实现从思维到行动的闭环，在商业调研、电力运维、医疗诊断等领域展现出显著价值。尽管面临大模型不确定性、多Agent协作机制不成熟等挑战，但其在金融、制造等垂直行业的深度应用已初见成效。 #人工智能#机器学习#深度学习 不会用提示词的人，正在被AI时代淘汰：万字长文揭秘与机器对话的终极法则 摘要：本文探讨了AI时代下提示词（Prompt）的重要性与应用技巧。通过广告新人被AI取代的案例，揭示掌握提示词已成为职场新技能。文章提出五维构建法：1）角色定位，为AI设定专业身份；2）场景搭建，提供详细背景信息；3）任务分解，结构化表达需求；4）限制条件，设定内容边界；5）反馈迭代，持续优化输出。通过电商命名、旅游规划等实例，展示了精准提示词如何提升AI输出质量。最后指出，当前AI仍处于超级模 #人工智能#AI 科普专栏｜大语言模型：理解与生成语言的人工智能 大语言模型(LLM)正成为AI领域的核心技术，通过海量数据训练掌握语言模式。其训练采用;预训练+微调;方法，结合强化学习优化回答质量。LLM已广泛应用于智能客服、文本生成、机器翻译等领域，但仍面临理解能力有限、数据偏差等挑战。随着技术进步，LLM将向更智能、多模态方向发展，成为人类创新的智能伙伴。 #人工智能#深度学习#机器学习 共 41 条 1 2 3 4 5 请选择 无匹配数据 10 条/页20 条/页30 条/页40 条/页 加载中","source":"web","publishedAt":"2025-10-25T15:10:19+08:00"},{"id":"bocha-3","title":"一篇文章带你看懂企业级 RAG 系统:从图解到实战,打造智能问答中台_rag中台-CSDN博客","url":"https://wmcoder.blog.csdn.net/article/details/148788236","snippet":"一篇文章带你看懂企业级 RAG 系统：从图解到实战，打造智能问答中台 最新推荐文章于 2025-12-16 08:47:31 发布 原创 最新推荐文章于 2025-12-16 08:47:31 发布 · 308 阅读 · 0 · 0 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #RAG #人工智能 RAG 技术详解 专栏收录该内容 7 篇文章 ¥9.90 ¥99.00 订阅专栏 超级会员免费看 ✅ 实用、可落地、面向工程实践的全链路手册。本文以一张经典架构图为核心，全面解析企业级 RAG（Retrieval-Augmented Generation）系统的设计思路与技术环节，适合开发者、数据工程师、AI 架构师深入学习与实践。 🧠 一、什么是 RAG？为什么值得企业落地？ RAG（Retrieval-Augmented Generation）是一种将大语言模型（LLM）与外部知识检索机制融合的生成式 AI 技术框架。相比直接让大模型“记住所有知识”，RAG 模式将知识管理和生成逻辑解耦： ⚙️ LLM 负责语言理解和生成 🔍 外部知识库负责提供权威、可控、实时的业务内容 ✅ 企业选择 RAG 的四大理由： 知识可控：检索的是企业知识库，不容易“幻觉” 高效维护：知识库独立更新，无需频繁微调模型 多模态融合：支持文本、图像、语音等数据接入 任务多样：问答、搜索、摘要、分析、流程决策全覆盖 🧭 二、一张图读懂企业级 RAG 架构全景 架构图展示了从“原始数据采集”到“最终问答输出”的完整流程，结合模块化组件设计，满足高可用、高可扩展、可评估的企业需求。 了解本专栏 订阅专栏 解锁全文 超级会员免费看 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 未名编程 关注 关注 0 点赞 踩 0 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 打赏 打赏 打赏 举报 举报 专栏目录 订阅专栏 构建企业私有RAG系统全流程：从 PDF 到智能问答的落地实践 努力分享一些人工智能、计算机视觉、影像等相关的知识干货！ 04-01 1307 - ✅ 企业文档 → 可搜索向量的标准处理链路 - ✅ 私有知识库的快速搭建方式（Chroma / FAISS） - ✅ RAG 问答系统从输入 → 召回 → 生成 → 输出的完整闭环 - ✅ 多轮问答 / 结构化返回 / 部署上线建议 📌 实战派不是看个 demo 就算结束，而是能“封起来，用得起，上得线”。 参与评论 您还未登录，请先 登录 后发表或查看评论 RAG应用开发与优化：构建企业级LLM应用的实践指南 weixin_31459297的博客 04-11 486 本文详细介绍了如何基于大模型的RAG（Retrieval-Augmented Generation）应用进行开发与优化，特别是针对企业级LLM（Large Language Model）应用的构建。内容包括检索与响应质量评估的方法、生成评估数据集的步骤、运行评估检索过程的程序、评估响应质量的过程、以及如何基于自定义标准进行评估。此外，还探讨了在企业级应用中常见的优化策略，包括选择合适的知识块大小等。 AI大模型企业应用实战：Prompt让LLM理解知识 2401_84204413的博客 06-25 2950 • 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力： 大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。• 基于大模型和企业数据AI应用开发，实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能， 学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握；第五阶段： 大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型； 企业级 RAG 天花板：从朴素原型到 Agentic 王者，七层架构全解析 最新发布 小程故事多的博客 12-16 1076 摘要： 当前AI落地面临“连接悖论”：大模型缺乏实时企业数据支持，导致生产环境表现不佳。检索增强生成（RAG）技术通过动态检索外部知识库解决此问题，但多数实践停留在简单原型阶段。RAG的演进分为四个阶段：1）朴素RAG（线性三阶段流程，存在检索质量差、上下文浪费等问题）；2）高级RAG（通过查询扩展、重排序等优化检索精度）；3）模块化RAG（引入动态工作流，支持条件分支和循环）；4）Agentic RAG（RAG降级为工具，由智能体协调多工具完成复杂任务）。企业级RAG需构建七层架构，包括意图理解、数据处理 多模态知识图谱：重构大模型RAG效能新边界 2401_85343303的博客 04-19 2427 当前企业级RAG（Retrieval-Augmented Generation）系统在非结构化数据处理中面临四大核心问题：数据孤岛效应：异构数据源（文档/表格/图像/视频）独立存储，缺乏跨模态语义关联，导致知识检索呈现碎片化。例如合同文本中的设备型号无法关联操作手册中的技术参数，质检报告中的缺陷描述无法匹配生产线监控视频。语义鸿沟：传统向量检索依赖局部关键词匹配，难以捕捉跨文档的隐含逻辑关系（如报告中的图表引用意图）。 企业级大模型的护城河：RAG + 微调 新缸中之脑 01-31 1729 围绕LLM的炒作是前所未有的，但这是有道理的，生成式 AI 有潜力改变我们所知道的社会。在很多方面，LLM将使数据工程师变得更有价值——这令人兴奋！不过，向老板展示数据发现工具或文本到 SQL 生成器的炫酷演示是一回事，而将其与公司的专有数据（甚至更重要的客户数据）一起使用则是另一回事。很多时候，公司急于构建人工智能应用程序，却对其实验的财务和组织影响缺乏远见。这不是他们的错——高管和董事会应该为围绕这项（以及大多数）新技术的“快点走”心态承担责任。（还记得 NFT 吗？ 【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题？ 新亮笔记 02-06 1019 本文为译文，原文链接：https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system今天，我们将卷起袖子，深入研究构建企业级 RAG 系统的复杂世界。我们将揭示一些常见的挑战和误区，以及如何克服它们。我们还将分享一些最佳实践和技巧，让您的 RAG 系统更加强大、灵活和可靠。但这个博客不仅仅... 企业级RAG系统从入门到精通 12-10 具体包括如下话题： RAG系列 问答数据构建 - 使用RAG技术构建企业级文档问答系统之QA抽取 Baseline搭建 - 使用RAG技术构建企业级文档问答系统之基础流程 检索优化 - 检索优化(1)Embedding微调 - 检索优化(2)Multi ... 从0到1：用Gemini和PGVector构建你的企业级RAG智能问答系统 07-18 基于检索增强生成（RAG）架构的高校智能问答系统，旨在为高校提供智能化的文档问答服务。系统支持多种文档格式上传，通过向量化技术实现语义检索，结合Google Gemini Pro大语言模型生成准确的答案。 核心特性 智能... 【企业级RAG系统】基于混合检索与安全机制的高级架构设计：金融医疗制造法律领域智能问答应用 10-04 文章重点介绍了混合检索策略、分块优化、评估体系构建以及数据安全机制，并通过完整的Python代码示例展示了一个企业级RAG系统的实现，包括向量检索（Pinecone）、关键词检索（Elasticsearch）、结果融合、重排序、... 【实战分享】构建企业级RAG（Retrieval-Augmented Generation）知识库的全面实践_企业级rag知识库 Z4400840的博客 06-27 883 本文系统介绍了基于RAG技术的企业知识库构建方法。首先分析了知识库在企业知识管理中的价值，包括解决信息孤岛、提升检索效率等痛点。接着详细解析了RAG技术原理，涵盖文本预处理、文档切片策略、向量化处理等关键技术环节。重点阐述了Milvus向量数据库的选择依据及系统架构设计，并给出Python实现方案。最后提出7项优化方向：智能PDF切分、检索完整性提升、提示词优化、Text-to-SQL实现、多轮对话、智能报表和多智能体交互。附录包含6大核心模块的代码实现要点。该方案为企业构建智能知识管理系统提供了完整的技术 企业级RAG实施指南，企业知识库落地一定不要错过，长文建议收藏 Android23333的博客 05-19 1121 RAG系统配置最佳实践与企业选型指南，企业知识库落地避坑宝典 企业级RAG系统配置与框架选型：从需求到实施 RAG框架在企业中的深度应用与选型策略 企业如何成功实施Cherry Studio、AnythingLLM和RAGFlow？ 一份指南明白企业级RAG实施指南 ，想要成功实施RAG 不要错过 一文读懂：大模型RAG（检索增强生成） star_nwe的博客 07-24 1565 AI大模型作为人工智能领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住AI大模型的风口，掌握AI大模型的知识和技能将变得越来越重要。学习AI大模型是一个系统的过程，需要从基础开始，逐步深入到更高级的技术。这里给大家精心整理了一份全面的AI大模型学习资源，包括：AI大模型全套学习路线图（从入门到实战）、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频，免费分享！ 精通 RAG：如何构建企业 RAG 系统 lyy2017175913的博客 12-26 1111 构建一个强大且可扩展的企业级 RAG 系统显然需要仔细协调互连的组件。从用户身份验证到输入护栏、查询重写、编码、文档摄取和检索组件（例如向量数据库和生成器），每个步骤都在塑造系统性能方面发挥着关键作用。在不断发展的 RAG 系统领域，我们希望这份实用指南能帮助开发人员和领导者获得可操作的见解！ Agentic RAG 的 7 种企业级架构 渔夫.AI 01-23 1316 你好，我是渔夫。今天，分享一篇长达 35 页的最新Agentic RAG 综述。论文想解决的核心问题是，当今天大型语言模型（LLMs）在处理动态、实时查询时依赖静态训练数据导致的过时、不准确输出、幻觉等问题。它从最基本原则和 RAG 范式的演变开始，介绍 Agentic RAG 的 7 种架构。还重点介绍在 5 种应用场景的效果，如医疗保健、金融和教育等行业中的关键应用，且非常详细。 微调 vs RAG：知识库方案选型实战指南 charles666666的博客 07-29 719 当企业需要为业务注入专业知识时，面临模型微调与RAG（检索增强生成）的关键选择。本文从八大场景出发，提供决策指南：微调适合风格定制、资源受限环境和实时响应需求，能塑造品牌声音并保证速度；RAG则更擅长处理动态数据、防止信息幻觉，具有更好的可解释性和成本优势。文章通过对比表格和决策流程图，分析了两种技术在更新频率、成本、响应速度等维度的差异，并指出数据质量对微调、检索算法对RAG的关键影响。最后建议考虑混合方案，平衡专业深度与知识时效性，为技术决策者提供实用参考。 MVP 聚技站｜Multi RAG：企业级 RAG 的重要架构 寒冰屋的专栏 09-08 414 Multi RAG：企业级 RAG 的重要架构 企业知识库搭建指南：基于RAG与LLM搭建智能知识库 大模型学习路线 05-08 1032 基于RAG与LLM的知识库作为目前最有潜力的企业端大模型应用之一，从技术角度可以看到，建设方案已经完备；从业务角度，最终的应用效果和业务价值还需要观察，并通过业务侧的反馈不断地促进建设方案的进一步优化，比如增加对多模态知识的处理能力等。让我们共同期待这类应用普及那一天的到来。最先掌握AI的人，将会比较晚掌握AI的人有竞争优势这句话，放在计算机、互联网、移动互联网的开局时期，都是一样的道理。倘若大家对大模型抱有兴趣，那么这套大模型学习资料肯定会对你大有助益。如果你是零基础小白，快速入门大模型是可行的。 ✅ 实用、可落地、面向工程实践的全链路手册。本文以一张经典架构图为核心，全面解析企业级 RAG（Retrieval-Augmented Generation）系统的设计思路与技术环节，适合开发者、数据工程师、AI 架构师深入学习与实践。 🧠 一、什么是 RAG？为什么值得企业落地？ RAG（Retrieval-Augmented Generation）是一种将大语言模型（LLM）与外部知识检索机制融合的生成式 AI 技术框架。相比直接让大模型“记住所有知识”，RAG 模式将知识管理和生成逻辑解耦： ⚙️ LLM 负责语言理解和生成 🔍 外部知识库负责提供权威、可控、实时的业务内容 ✅ 企业选择 RAG 的四大理由： 知识可控：检索的是企业知识库，不容易“幻觉” 高效维护：知识库独立更新，无需频繁微调模型 多模态融合：支持文本、图像、语音等数据接入 任务多样：问答、搜索、摘要、分析、流程决策全覆盖 🧭 二、一张图读懂企业级 RAG 架构全景 架构图展示了从“原始数据采集”到“最终问答输出”的完整流程，结合模块化组件设计，满足高可用、高可扩展、可评估的企业需求。","source":"web","publishedAt":"2025-06-20T11:32:14+08:00"},{"id":"bocha-4","title":"oracle rac架构图_51CTO博客","url":"https://blog.51cto.com/topic/oracle-racjiagoutu.html","snippet":"Oracle RAC 运行于集群之上,为 Oracle 数据库提供了最高级别的可用性、可伸缩性和低成本计算能力。如果集群内的一个节点发生故障, Oracle 将可以继续在其余的节点上运行。 Oracl","source":"web","publishedAt":"2023-08-15T11:15:03+08:00"},{"id":"bocha-5","title":"ERP系统存储备份系统设计方案-20230504010441.pdf-原创力文档","url":"https://max.book118.com/html/2023/0504/8001074125005063.shtm","snippet":"ERP 系统存储备份系统设计 •存储系统釆用SAN 结构,集中存储。 •磁盘阵列内部应无单点故障; •任何磁盘控制器故障,均应保证阵列内数据的完整可用性; •详细介绍存储系统结构和内部模块连接图; •","source":"web","publishedAt":"2023-05-05T03:00:09+08:00"},{"id":"bocha-6","title":"大模型应用开发 | RAG在实际落地场景中的优化（三）RAG落地案例分享","url":"https://m.blog.csdn.net/Androiddddd/article/details/146206916","snippet":"大模型应用开发 | RAG在实际落地场景中的优化（三）RAG落地案例分享 最新推荐文章于 2025-11-29 03:18:39 发布 原创 最新推荐文章于 2025-11-29 03:18:39 发布 · 1.5k 阅读 · 29 · 29 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #人工智能 #自然语言处理 #AI大模型 #大模型 #LLM #ai #RAG 三、RAG落地案例分享 3.1数据基础设施领域的RAG 3.1.1运维智能体背景 在数据基础设施领域，有很多运维SRE，每天会接收到大量的告警，因此很多时间来需要响应应急事件，进而进行故障诊断，然后故障复盘，进而进行经验沉淀。另外一部分时间又需要响应用户咨询，需要他们用他们的知识以及三方平台工具使用经验进行答疑。 因此我们希望通过打造一个数据基础设施的通用智能体来解决告警诊断，答疑的这些问题。 3.1.2严谨专业的RAG 传统的 RAG + Agent 技术可以解决通用的，确定性没那么高的，单步任务场景。但是面对数据基础设施领域的专业场景，整个检索过程必须是确定，专业和真实的，并且是需要一步一步推理的。 右边是一个通过NativeRAG的一个泛泛而谈的总结，可能对于一个普通的用户，对专业的领域知识没那么了解时，可能是有用的信息，但是这部分对于数据基础设施领域的专业人士，就没有什么意义了。因此我们比较了通用的智能体和数据基础设施智能体在RAG上面的区别： 通用的智能体：传统的RAG对知识的严谨和专业性要求没那么高，适用于客服，旅游，平台答疑机器人这样的一些业务场景。 数据基础设施智能体：RAG流程是严谨和专业的，需要专属的RAG工作流程，上下文包括(DB告警->根因定位->应急止血->故障恢复)，并且需要对专家沉淀的问答和应急经验，进行结构化的抽取，建立层次关系。因此我们选择知识图谱来作为数据承载。 3.1.3 知识处理 基于数据基础设施的确定性和特殊性，我们选择通过结合知识图谱来作为诊断应急经验的知识承载。我们通过SRE沉淀下来的应急排查事件知识经验 结合应急复盘流程，建立了DB应急事件驱动的知识图谱，我们以DB抖动为例，影响DB抖动的几个事件，包括慢SQL问题，容量问题，我们在各个应急事件间建立了层级关系。 最后通过我们通过规范化应急事件规则，一步一步地建立了多源的知识 -> 知识结构化抽取 ->应急关系抽取 -> 专家审核 -> 知识存储的一套标准化的知识加工体系。 3.1.4 知识检索 在智能体检索阶段，我们使用GraphRAG作为静态知识检索的承载，因此识别到DB抖动异常后，找到了与DB抖动异常节点相关的节点作为我们分析依据，由于在知识抽取阶段每一个节点还保留了每个事件的一些元数据信息，包括事件名，事件描述，相关工具，工具参数等等， 因此我们可以通过执行工具的执行生命周期链路来获取返回结果拿到动态数据来作为应急诊断的排查依据。通过这种动静结合的混合召回的方式比纯朴素的RAG召回，保障了数据基础设施智能体执行的确定性，专业性和严谨性。 3.1.5 AWEL + Agent 最后通过社区AWEL+AGENT技术，通过AGENT编排的范式，打造了从意图专家-> 应急诊断专家 -> 诊断根因分析专家。 每个Agent的职能都是不一样的，意图专家负责识别解析用户的意图和识别告警信息诊断专家需要通过GraphRAG 定位到需要分析的根因节点，以及获取具体的根因信息。分析专家需要结合各个根因节点的数据 + 历史分析复盘报告生成诊断分析报告。 3.2金融财报分析领域的RAG 基于DB-GPT的财报分析助手 ：https://www.yuque.com/eosphoros/dbgpt-docs/cmogrzbtmqf057oe 四、总结 建议围绕各自领域构建属于自己的领域资产库包括，知识资产，工具资产以及知识图谱资产 领域资产:领域资产包括了领域知识库，领域API，工具脚本，领域知识图谱。 资产处理，整个资产数据链路涉及了领域资产加工，领域资产检索和领域资产评估。 非结构化 -> 结构化：有条理地归类，正确地组织知识信息。 提取更加丰富的语义信息。 资产检索： 希望是有层级，优先级的检索而并非单一的检索 后置过滤很重要，最好能通过业务语义一些规则进行过滤。 如何系统学习掌握AI大模型？ AI大模型作为人工智能领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住AI大模型的风口，掌握AI大模型的知识和技能将变得越来越重要。 学习AI大模型是一个系统的过程，需要从基础开始，逐步深入到更高级的技术。 这里给大家精心整理了一份全面的AI大模型学习资源，包括：AI大模型全套学习路线图（从入门到实战）、精品AI大模型学习书籍手册、视频教程、实战学习、面试题等，资料免费分享！ 1. 成长路线图&学习规划 要学习一门新的技术，作为新手一定要先学习成长路线图，方向不对，努力白费。 这里，我们为新手和想要进一步提升的专业人士准备了一份详细的学习成长路线图和规划。可以说是最科学最系统的学习成长路线。 2. 大模型经典PDF书籍 书籍和学习文档资料是学习大模型过程中必不可少的，我们精选了一系列深入探讨大模型技术的书籍和学习文档，它们由领域内的顶尖专家撰写，内容全面、深入、详尽，为你学习大模型提供坚实的理论基础。（书籍含电子版PDF） 3. 大模型视频教程 对于很多自学或者没有基础的同学来说，书籍这些纯文字类的学习教材会觉得比较晦涩难以理解，因此，我们提供了丰富的大模型视频教程，以动态、形象的方式展示技术概念，帮助你更快、更轻松地掌握核心知识。 4. 2024行业报告 行业分析主要包括对不同行业的现状、趋势、问题、机会等进行系统地调研和评估，以了解哪些行业更适合引入大模型的技术和应用，以及在哪些方面可以发挥大模型的优势。 5. 大模型项目实战 学以致用 ，当你的理论知识积累到一定程度，就需要通过项目实战，在实际操作中检验和巩固你所学到的知识，同时为你找工作和职业发展打下坚实的基础。 6. 大模型面试题 面试不仅是技术的较量，更需要充分的准备。 在你已经掌握了大模型技术之后，就需要开始准备面试，我们将提供精心整理的大模型面试题库，涵盖当前面试中可能遇到的各种技术问题，让你在面试中游刃有余。 全套的AI大模型学习资源已经整理打包，有需要的小伙伴可以微信扫描下方CSDN官方认证二维码，免费领取【保证100%免费】 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 大模型本地部署_ 关注 关注 29 点赞 踩 29 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 举报 举报 精选资源 AI大模型RAG项目实战课 10-29 该课程强调了RAG技术在大模型落地中的重要性，并列举了多种应用场景，如客服自动化、文档撰写、图像生成以及数据处理分析等。RAG技术通过结合非参数化的语料库数据库和参数化模型，改善了纯参数化模型的局限性。 ... 参与评论 您还未登录，请先 登录 后发表或查看评论 【企业级大模型开发】企业级大模型开发：MCP+RAG实战案例详解 08-18 内容概要：本文详细介绍了MCP（模型控制生产）与RAG（检索增强生成）结合在企业级大模型开发中的应用，特别是通过一个金融知识问答系统的实战案例展示其技术优势。MCP强调模型治理、性能监控和迭代优化，确保AI系统... 一文搞懂大模型RAG应用（附实践案例） 热门推荐 AIPHIL的博客 11-21 2万+ 大模型（Large Language Model，LLM）的浪潮已经席卷了几乎各行业，但当涉及到专业场景或行业细分领域时，通用大模型就会面临专业知识不足的问题。相对于成本昂贵的“Post Train”或“SFT”，基于RAG的技术方案往往成为一种更优选择。本文从RAG架构入手，详细介绍相关技术细节，并附上一份实践案例。检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。知识的局限性。 RAGFlow+DeepSeek-R1:14b落地案例分享（足够详细）：机加工行业设备维保场景 2401_84204413的博客 02-22 1597 2.1。 Ragas评估框架：如何提升金融数据分析的检索质量与准确性 最新发布 gitblog_00689的博客 11-29 882 在金融投资研究领域，快速准确地从海量数据中检索相关信息至关重要。Ragas作为一个专业的RAG（检索增强生成）评估框架，能够帮助量化金融数据分析系统的检索质量，确保投资决策基于准确、可靠的信息。📈 ## 为什么金融数据分析需要RAG评估？ 金融数据分析涉及大量的非结构化数据，包括公司财报、行业分析资料、新闻资讯等。传统的检索系统往往面临以下挑战： - **信息过载**：每天产生海量金融数据 72 个 RAG 实战场景大公开！从医疗到金融，总有一个戳中你的需求（附开源方案） m0_48891301的博客 06-11 2032 在大模型时代，RAG（检索增强生成）就像一把万能钥匙，正在解锁 AI 应用的无限可能。2024 年，从 GraphRAG 的知识图谱创新到多模态 RAG 的视觉突破，从医疗场景的精准诊断到企业级知识库的高效构建，RAG 技术正以「七十二变」的姿态渗透到各个领域。本文精心整理72 个真实场景的 RAG 落地案例，涵盖技术原理、开源项目与实战价值，建议收藏！ 一文彻底搞懂大模型RAG应用（附实战案例） 2401_84208172的博客 12-11 2050 大模型（Large Language Model，LLM）的浪潮已经席卷了几乎各行业，但当涉及到专业场景或行业细分领域时，通用大模型就会面临专业知识不足的问题。相对于成本昂贵的“Post Train”或“SFT”，基于RAG的技术方案往往成为一种更优选择。本文从RAG架构入手，详细介绍相关技术细节，并附上一份实践案例。检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。知识的局限性。 一文读懂RAG和LLM微调，教你结合业务场景落地LLM应用 aolan123的博客 10-23 2088 1. 需要外部知识吗？对于以前摘要的风格进行摘要的任务，主要数据源将是以前的摘要本身。如果这些摘要包含在静态数据集中，就不太需要持续外部数据检索。但是，如果有一个频繁更新的摘要动态数据库，目标是不断与最新条目对齐的话，RAG可能在这个场景更好发挥作用。2. 需要模型适配吗？这个用例的核心是适应专业领域或特定的写作风格。微调特别擅长捕捉风格细微差异、语调变化和特定领域的词汇，因此对于这个维度来说，微调也是是一个必要的选择。3. 必须是最小化幻觉吗？在大多数LLM应用中，都会存在响应幻觉的问题。 精选资源 2025大模型RAG+图计算实战案例合集.pdf 05-07 接着，文档深入探讨了腾讯在大模型应用中使用的三种技术：SFT（Supervised Fine-Tuning）、RAG和Agent。SFT基于大语言模型进行微调，结合业务专属数据固化特定领域的业务知识。RAG技术通过结合外部知识库和检索技术... 精选资源 2024 Agent+RAG：基于大模型的融合应用探索（八大案例，共146页）.pdf 02-12 内容概要：该文章汇集了八个典型的 Agent 和 RAG（检索增强生成）融合应用案例，详细展示了大模型技术在游戏娱乐、金融科技、语音助手、办公支持等多个领域中的应用。文章不仅探讨了 Agent 如何改变多领域中的交互与... 精选资源 2024 RAG核心技术与应用手册（八大案例，共181页）.pdf 02-12 文章从技术原理、性能优化策略，到实际应用场景如腾讯大模型业务落地、京东电商搜索优化、小红书的生成式检索等，展示了RAG在多个领域的重要作用及成效。文章还探讨了知识图谱和Agent技术在RAG系统中的应用以及语音... 企业级RAG实施指南，企业知识库落地一定不要错过，长文建议收藏 Android23333的博客 05-19 1121 RAG系统配置最佳实践与企业选型指南，企业知识库落地避坑宝典 企业级RAG系统配置与框架选型：从需求到实施 RAG框架在企业中的深度应用与选型策略 企业如何成功实施Cherry Studio、AnythingLLM和RAGFlow？ 一份指南明白企业级RAG实施指南 ，想要成功实施RAG 不要错过 揭秘！企业级RAG系统架构深度解析：从0到1构建智能知识引擎（附实战案例） weixin_40593051的博客 05-11 1349 本文聚焦企业级RAG系统架构，解析其从0到1的构建过程。RAG通过“检索-生成”模式，整合外部知识库与生成式AI，解决传统知识管理效率低、准确性差等问题，可提升知识检索准确率40%-60%，缩短问题解决时间50%以上。架构涵盖数据层（分布式存储、向量数据库）、处理层（检索、生成、融合模块）、应用层（多交互接口、监控），关键技术涉及Milvus、GPT-4o等工具。还介绍知识预处理、高效检索、提示工程等模块实现，以及性能优化、安全合规设计。实战案例显示，其能显著提升企业效率、降低成本。未来RAG将向多模态、 万字长文讲透 RAG 在实际落地场景中的优化 m0_56255097的博客 02-07 1172 2. RAG流程优化RAG流程的优化我们又分为了静态文档的RAG和动态数据获取的RAG，目前大部分涉及到的RAG只覆盖了非结构化的文档静态资产，但是实际业务很多场景的问答是通过工具获取动态数据 + 静态知识数据共同回答的场景，不仅需要检索到静态的知识，同时需要RAG检索到工具资产库里面工具信息并执行获取动态数据。构建企业领域工具资产库，将散落到各个平台的工具API，工具脚本进行整合，进而提供智能体端到端的使用能力。比如，除了静态知识库以外，我们可以通过导入工具库的方式进行工具的处理。 一文详解几种常见本地大模型个人知识库工具部署、微调及对比选型 youmaob的博客 07-24 7440 这里先盘点一下最近比较火爆的几个工具，将从知识库侧和大模型侧分别介绍。这六种。AnythingLLM 是 Mintplex Labs Inc. 开发的一款可以与任何内容聊天的私人 ChatGPT，是高效、可定制、开源的企业级文档聊天机器人解决方案。它能够将任何文档、资源或内容片段转化为大语言模型（LLM）在聊天中可以利用的相关上下文。其采用MIT许可证的开源框架，支持快速在本地部署基于检索增强生成（RAG）的大模型应用。在不调用外部接口、不发送本地数据的情况下，确保用户数据的安全。 如何利用 instructor 提高 RAG 的准确性和召回率 weixin_43829633的博客 10-15 1740 RAG（Retrieval Augmented Generation）是一种检索增强生成技术，它利用大型语言模型来处理用户查询，RAG 技术的主要组成包括数据提取—embedding—创建索引—检索—排序（Rerank）—LLM 归纳生成，不过实际落地过程来看，将用户查询转换为嵌入向量直接检索，很多时候的结果在相关度方面没有那么理想，本篇分享一种对用户查询进行重写再去进行检索从而提高准确性和召回率的方案 RAG（大模型+知识库）落地与知识管理的春天-新的知识运营体系 weixin_59191169的博客 05-23 4384 大模型时代来了，可能你也知道GPT大模型是被海量知识训练出来的，但不知道你有没有问过，什么样的知识才能训练大模型？站在企业的角度，很多企业都有自己的知识库或者文档中心，很多人也都有自己积攒数年的资料库，那是不是用上大模型，就能轻松实现基于自己知识库的智能搜索/智能问答/智能推荐呢？（一）知识基础形态和知识质量在传统的搜索、问答、推荐等场景中，通常会返回一整篇的文档，我们还得在这一大篇资料中去找到自己想要的那一段具体内容。不少企业构建的知识库，其实就是文档库。 简单三个案例来分析RAG、微调如何选择？ AI大模型的博客 06-21 1601 我们重点来讨论几个案例，来看一下每个案例到底选择RAG，还是微调，或者是RAG+微调。：比如说我们想打造一个AI的投资理财规划师，比如我根据用户的风险偏好，还有一些用户的情况来给他一个合理的建议，比如说基于一些目前市场上的情况，那这种AI的规划师我们到底怎么打造？那这里我们需要考虑的是RAG还是微调呢？大家可以先思考一下。那为了回答这个问题，我们首先要剖析那这样的系统它到底需要具备什么样的能力？1、第一个很重要，就是可以处理实时的数据，或者叫；2、它也需要具备； 金融服务中的GraphRAG和标准RAG对比案例解析 2401_85375298的博客 10-25 1113 使用案例：灾后索赔管理。数据 ：历史索赔、客户资料、保单详细信息、灾害影响数据、地理数据、社交网络、天气模式。 三、RAG落地案例分享 3.1数据基础设施领域的RAG 3.1.1运维智能体背景 在数据基础设施领域，有很多运维SRE，每天会接收到大量的告警，因此很多时间来需要响应应急事件，进而进行故障诊断，然后故障复盘，进而进行经验沉淀。另外一部分时间又需要响应用户咨询，需要他们用他们的知识以及三方平台工具使用经验进行答疑。 因此我们希望通过打造一个数据基础设施的通用智能体来解决告警诊断，答疑的这些问题。 3.1.2严谨专业的RAG 传统的 RAG + Agent 技术可以解决通用的，确定性没那么高的，单步任务场景。但是面对数据基础设施领域的专业场景，整个检索过程必须是确定，专业和真实的，并且是需要一步一步推理的。 右边是一个通过NativeRAG的一个泛泛而谈的总结，可能对于一个普通的用户，对专业的领域知识没那么了解时，可能是有用的信息，但是这部分对于数据基础设施领域的专业人士，就没有什么意义了。因此我们比较了通用的智能体和数据基础设施智能体在RAG上面的区别： 通用的智能体：传统的RAG对知识的严谨和专业性要求没那么高，适用于客服，旅游，平台答疑机器人这样的一些业务场景。 数据基础设施智能体：RAG流程是严谨和专业的，需要专属的RAG工作流程，上下文包括(DB告警->根因定位->应急止血->故障恢复)，并且需要对专家沉淀的问答和应急经验，进行结构化的抽取，建立层次关系。因此我们选择知识图谱来作为数据承载。 3.1.3 知识处理 基于数据基础设施的确定性和特殊性，我们选择通过结合知识图谱来作为诊断应急经验的知识承载。我们通过SRE沉淀下来的应急排查事件知识经验 结合应急复盘流程，建立了DB应急事件驱动的知识图谱，我们以DB抖动为例，影响DB抖动的几个事件，包括慢SQL问题，容量问题，我们在各个应急事件间建立了层级关系。 最后通过我们通过规范化应急事件规则，一步一步地建立了多源的知识 -> 知识结构化抽取 ->应急关系抽取 -> 专家审核 -> 知识存储的一套标准化的知识加工体系。 3.1.4 知识检索 在智能体检索阶段，我们使用GraphRAG作为静态知识检索的承载，因此识别到DB抖动异常后，找到了与DB抖动异常节点相关的节点作为我们分析依据，由于在知识抽取阶段每一个节点还保留了每个事件的一些元数据信息，包括事件名，事件描述，相关工具，工具参数等等， 因此我们可以通过执行工具的执行生命周期链路来获取返回结果拿到动态数据来作为应急诊断的排查依据。通过这种动静结合的混合召回的方式比纯朴素的RAG召回，保障了数据基础设施智能体执行的确定性，专业性和严谨性。 3.1.5 AWEL + Agent 最后通过社区AWEL+AGENT技术，通过AGENT编排的范式，打造了从意图专家-> 应急诊断专家 -> 诊断根因分析专家。 每个Agent的职能都是不一样的，意图专家负责识别解析用户的意图和识别告警信息诊断专家需要通过GraphRAG 定位到需要分析的根因节点，以及获取具体的根因信息。分析专家需要结合各个根因节点的数据 + 历史分析复盘报告生成诊断分析报告。 3.2金融财报分析领域的RAG 基于DB-GPT的财报分析助手 ：https://www.yuque.com/eosphoros/dbgpt-docs/cmogrzbtmqf057oe 四、总结 建议围绕各自领域构建属于自己的领域资产库包括，知识资产，工具资产以及知识图谱资产 领域资产:领域资产包括了领域知识库，领域API，工具脚本，领域知识图谱。 资产处理，整个资产数据链路涉及了领域资产加工，领域资产检索和领域资产评估。 非结构化 -> 结构化：有条理地归类，正确地组织知识信息。 提取更加丰富的语义信息。 资产检索： 希望是有层级，优先级的检索而并非单一的检索 后置过滤很重要，最好能通过业务语义一些规则进行过滤。 如何系统学习掌握AI大模型？ AI大模型作为人工智能领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住AI大模型的风口，掌握AI大模型的知识和技能将变得越来越重要。 学习AI大模型是一个系统的过程，需要从基础开始，逐步深入到更高级的技术。 这里给大家精心整理了一份全面的AI大模型学习资源，包括：AI大模型全套学习路线图（从入门到实战）、精品AI大模型学习书籍手册、视频教程、实战学习、面试题等，资料免费分享！ 1. 成长路线图&学习规划 要学习一门新的技术，作为新手一定要先学习成长路线图，方向不对，努力白费。 这里，我们为新手和想要进一步提升的专业人士准备了一份详细的学习成长路线图和规划。可以说是最科学最系统的学习成长路线。 2. 大模型经典PDF书籍 书籍和学习文档资料是学习大模型过程中必不可少的，我们精选了一系列深入探讨大模型技术的书籍和学习文档，它们由领域内的顶尖专家撰写，内容全面、深入、详尽，为你学习大模型提供坚实的理论基础。（书籍含电子版PDF） 3. 大模型视频教程 对于很多自学或者没有基础的同学来说，书籍这些纯文字类的学习教材会觉得比较晦涩难以理解，因此，我们提供了丰富的大模型视频教程，以动态、形象的方式展示技术概念，帮助你更快、更轻松地掌握核心知识。 4. 2024行业报告 行业分析主要包括对不同行业的现状、趋势、问题、机会等进行系统地调研和评估，以了解哪些行业更适合引入大模型的技术和应用，以及在哪些方面可以发挥大模型的优势。 5. 大模型项目实战 学以致用 ，当你的理论知识积累到一定程度，就需要通过项目实战，在实际操作中检验和巩固你所学到的知识，同时为你找工作和职业发展打下坚实的基础。 6. 大模型面试题 面试不仅是技术的较量，更需要充分的准备。 在你已经掌握了大模型技术之后，就需要开始准备面试，我们将提供精心整理的大模型面试题库，涵盖当前面试中可能遇到的各种技术问题，让你在面试中游刃有余。 全套的AI大模型学习资源已经整理打包，有需要的小伙伴可以微信扫描下方CSDN官方认证二维码，免费领取【保证100%免费】","source":"web","publishedAt":"2025-03-13T01:48:52+08:00"},{"id":"bocha-7","title":"首个基于MCP 的 RAG 框架:UltraRAG 2.0用几十行代码实现高性能RAG, 拒绝冗长工程实现_腾讯新闻","url":"https://news.qq.com/rain/a/20250829A064SC00","snippet":"首个基于MCP 的 RAG 框架：UltraRAG 2.0用几十行代码实现高性能RAG， 拒绝冗长工程实现AI前线2025-08-29 16:23发布于北京检索增强生成系统（RAG）正从早期“检索 + 生成”的简单拼接，走向融合自适应知识组织、多轮推理、动态检索的复杂知识系统（典型代表如 DeepResearch、Search-o1）。但这种复杂度的提升，使开发者在方法复现、快速迭代新想法时，面临着高昂的工程实现成本。为了解决这一痛点，清华大学 THUNLP 实验室、东北大学 NEUIR 实验室、OpenBMB 与 AI9Stars 联合推出 UltraRAG 2.0 （UR-2.0）—— 首个基于 Model Context Protocol (MCP) 架构设计的 RAG 框架。这一设计让科研人员只需编写 YAML 文件，就可以直接声明串行、循环、条件分支等复杂逻辑，从而以极低的代码量快速实现多阶段推理系统。UltraRAG 2.0 亮点一览🧩 组件化封装：将 RAG 的核心组件封装为标准化的独立 MCP Server；🔌 灵活调用与扩展：提供函数级 Tool 接口，支持功能的灵活调用与扩展；🪄 轻量流程编排：借助 MCP Client，建立自上而下的简洁化链路搭建；与传统框架相比，UltraRAG 2.0 显著降低了复杂 RAG 系统的技术门槛与学习成本，让研究者能够将更多精力投入到实验设计与算法创新上，而不是陷入冗长的工程实现。相关链接🔗 Github 链接：https://github.com/OpenBMB/UltraRAG🔗 项目主页：https://openbmb.github.io/UltraRAG/🔗 教程文档：https://ultrarag.openbmb.cn/🔗 开源数据集：https://huggingface.co/datasets/UltraRAG/UltraRAG_Benchmark化繁为简仅 5% 代码实现低门槛复现「简」的价值，在实践中尤为直观。以 IRCoT（https://arxiv.org/abs/2212.10509）这一经典方法为例，它依赖基于模型生成的 CoT 进行多轮检索直至产出最终答案，整体流程相当复杂。IRCoT 流程图在官方实现中，仅 Pipeline 部分就需要近 900 行手写逻辑；即便使用标杆级 RAG 框架（如 FlashRAG），也仍需超过 110 行代码。相比之下，UltraRAG 2.0 只需约 50 行代码即可完成同等功能。更值得强调的是，其中约一半还是用于编排的 Yaml 伪代码，这大幅降低了开发门槛与实现成本。不同框架实现 IRCoT 的代码规模与结构差异以下展示了不同框架在相同功能实现上的差异。可以看到，FlashRAG 的实现仍然需要较长的控制逻辑，涉及显式的循环、条件判断与状态更新。而在 UltraRAG 2.0 中，这些逻辑仅需几行 Pipeline YAML 配置即可表达，分支与循环均以简洁的声明方式完成，避免了繁琐的手动编码。👇 FlashRAG 与 UltraRAG 的代码实现对比FlashRAGUltraRAG简而不凡数十行代码实现高性能 RAG 系统对 UltraRAG 2.0 而言，「简」并不意味着功能受限。借助 MCP 架构与灵活的 YAML 流程定义，UltraRAG 2.0 为科研人员提供了一个高性能、可扩展的实验平台。研究者可以在极短时间内搭建出类似 DeepResearch 的多阶段推理系统，支持 动态检索、条件判断、多轮交互 等高级能力。在示例中，我们将 Retriever、Generation、Router 等模块通过 YAML 串联，构建了一个同时具备循环与条件分支的推理流程，实现了 Plan 生成 → 知识整理 → 子问题生成 等关键步骤，而这一切仅需 不到 100 行代码。示例系统流程图在性能上，该系统在复杂多跳问题上，相较 Vanilla RAG 性能提升约 12%，充分验证了 UltraRAG 2.0 在快速构建复杂推理系统方面的潜力。示例系统性能表现以下展示的是我们构建的示例系统在小红书平台上针对两个真实用户提问所生成的案例。 🌰 Case 1：毕业秋招求职 🙋 问题输入📖 生成结果 🌰 Case 2：七夕送礼物 🙋 问题输入📖 生成结果UltraRAG 2.0 让复杂推理系统的构建真正做到 低代码、高性能、可落地。用户不仅能在科研任务中获得性能提升，也能够在行业应用中快速落地，例如智能客服、教育辅导、医疗问答等典型场景，输出更可靠的知识增强答案。MCP 架构与原生流程控制在不同的 RAG 系统中，检索、生成等核心能力在功能上具有高度相似性，但由于开发者实现策略各异，模块之间往往缺乏统一接口，难以跨项目复用。Model Context Protocol (MCP) 作为一种开放协议，规范了为大型语言模型（LLMs）提供上下文的标准方式，并采用 Client–Server 架构，使得遵循该协议开发的 Server 组件可以在不同系统间无缝复用。受此启发，UltraRAG 2.0 基于 MCP 架构，将 RAG 系统中的检索、生成、评测等核心功能抽象并封装为相互独立的 MCP Server，并通过标准化的函数级 Tool 接口实现调用。这一设计既保证了模块功能扩展的灵活性，又允许新模块以“热插拔”的方式接入，无需对全局代码进行侵入式修改。在科研场景中，这种架构让研究者能够以极低的代码量快速适配新的模型或算法，同时保持整体系统的稳定性与一致性。UltraRAG 2.0 框架示意图复杂 RAG 推理框架的开发具有显著挑战，而 UltraRAG 2.0 之所以能够在低代码条件下支持复杂系统的构建，核心在于其底层对多结构 Pipeline 流程控制的原生支持。无论是串行、循环还是条件分支，所有控制逻辑均可在 YAML 层完成定义与调度，覆盖复杂推理任务所需的多种流程表达方式。在实际运行中，推理流程的调度由内置 Client 执行，其逻辑完全由用户编写的外部 Pipeline YAML 脚本描述，从而实现与底层实现的解耦。开发者可以像使用编程语言关键字一样调用 loop、step 等指令，以声明式方式快速构建多阶段推理流程。通过将 MCP 架构 与 原生流程控制深度融合，UltraRAG 2.0 让复杂 RAG 系统的搭建像“编排流程”一样自然高效。此外，框架内置 17 个主流 benchmark 任务与多种高质量 baseline，配合统一的评测体系与知识库支持，进一步提升了系统开发的效率与实验的可复现性。👇 欢迎访问教程文档快速上手 UltraRAG 2.0！相关链接🔗 Github 链接：https://github.com/OpenBMB/UltraRAG🔗 项目主页：https://openbmb.github.io/UltraRAG/🔗 教程文档：https://ultrarag.openbmb.cn/🔗 开源数据集：https://huggingface.co/datasets/UltraRAG/UltraRAG_BenchmarkInfoQ 老友！请留步！极客邦 1 号客服上线工作啦！后续我将通过微信视频号，以视频的形式持续更新技术话题、未来发展趋势、创业经验、商业踩坑教训等精彩内容，和大家一同成长，开启知识交流之旅","source":"web","publishedAt":"2025-08-29T16:23:33+08:00"},{"id":"bocha-8","title":"使用 C# 与 RAG 技术构建智能知识库实践指南","url":"https://www.cnblogs.com/zt199510/p/18695013","snippet":"可乐_加冰 博客园 首页 新随笔 联系 订阅 管理 使用 C# 与 RAG 技术构建智能知识库实践指南 什么是 RAG？ RAG（Retrieval-Augmented Generation）是一种结合检索与生成的混合架构，通过从知识库中检索相关上下文，辅助生成模型产出更准确、更专业的回答。这种技术特别适合需要结合领域知识的问答场景。 技术栈说明 本实现基于以下技术： .NET 7：核心开发框架 Microsoft.KernelMemory：微软开源的语义内存管理库 OpenAI API：GPT-4 文本生成与 Embedding 模型 本地向量存储：使用简单的磁盘存储方案 实现原理 RAG 技术工作流 本系统的核心实现基于 检索-生成双阶段架构，具体流程如下： 文档预处理流水线 graph TD A[原始文档] --> B(文本分块) B --> C(嵌入生成) C --> D[向量存储] 分块策略：滑动窗口算法（128 tokens/块，20% 重叠） 嵌入模型：text-embedding-3-small 生成 1536 维向量 向量存储：余弦相似度检索，Top-K=3 问答生成阶段 graph LR E[用户提问] --> F(语义检索) F --> G{是否匹配?} G -->|是| H[上下文增强提示] G -->|否| I[空结果标记] H --> J[GPT-4 生成] 动态上下文拼接 // 代码实现中的检索配置 new SearchClientConfig { MaxAskPromptSize = 2048, // 控制上下文总长度 AnswerTokens = 2048 // 限制生成响应长度 } 架构设计 系统分层架构 graph TB subgraph 基础设施层 A[本地文件存储] --> B[SimpleFileStorage] C[向量数据库] --> D[SimpleVectorDb] end subgraph 服务层 E[OpenAI 服务代理] --> F[HTTP 请求重定向] G[记忆服务] --> H[KMService] end subgraph 应用层 I[文档导入] --> J[ImportDocumentAsync] K[知识检索] --> L[AskAsync] end B --> G D --> G F --> G G --> I G --> K 关键组件说明 存储抽象层 文件存储：使用 SimpleFileStorage 实现原始文档版本管理 向量存储：基于磁盘的轻量级向量数据库，支持扩展 Redis 等方案 元数据存储：文档 ID、更新时间、URL 等信息的结构化存储 语义处理引擎 // 双模型协同配置 .WithOpenAITextGeneration(...) // 生成模型 .WithOpenAITextEmbedding(...) // 嵌入模型 异步流水线处理：文档解析与向量化并行执行 智能缓存：重复文档哈希值匹配机制 代理中间件 // OpenAIHttpClientHandler 核心逻辑 protected override async Task SendAsync(...) { // 动态修改 API 终结点 request.RequestUri = new Uri($\"{proxy}/v1/chat/completions\"); // 非生产环境记录完整请求日志 if(!IsProduction) Log(requestBody); } 支持私有化模型部署 请求/响应双向监控 核心实现解析 1. 内存服务初始化（KMService.cs） public class KMService { public MemoryServerless CreateMemoryByApp() { var searchConfig = new SearchClientConfig { MaxAskPromptSize = 2048, // 最大提示长度 MaxMatchesCount = 3, // 最大匹配数量 AnswerTokens = 2048, // 回答长度限制 EmptyAnswer = \"KMS_SEARCH_NULL\" }; var memory = new KernelMemoryBuilder() .WithSearchClientConfig(searchConfig) .WithOpenAITextGeneration(new OpenAIConfig { APIKey = \"your-api-key\", TextModel = \"gpt-4\" }) .WithOpenAITextEmbedding(new OpenAIConfig { APIKey = \"your-api-key\", EmbeddingModel = \"text-embedding-3-small\" }) .WithSimpleFileStorage() // 本地文件存储 .WithSimpleVectorDb(); // 本地向量存储 return memory.Build<MemoryServerless>(); } } 关键配置说明： 支持自定义检索策略 可扩展的存储方案（支持切换为Redis/PostgreSQL） 双模型配置：生成模型 + 嵌入模型 2. 文档处理流程（Program.cs） // 文档导入 var doc = new Document(\"doc001\") .AddFile(\"溺水防范指南.pdf\") .AddTag(\"category\", \"safety\"); await memory.ImportDocumentAsync(doc, index: \"safety-knowledge\"); // 知识检索 var question = new Question(\"如何预防儿童溺水？\"); var answer = await memory.AskAsync(question); 处理流程： 文档解析与分块 生成文本嵌入 向量存储索引 相似性检索 上下文增强生成 3. 定制化 HTTP 处理（OpenAIHttpClientHandlerUtil.cs） 实现代理转发和请求日志： protected override async Task<HttpResponseMessage> SendAsync( HttpRequestMessage request, CancellationToken cancellationToken) { // 动态修改请求路径 if(request.RequestUri.Path == \"/v1/chat/completions\") { request.RequestUri = new Uri($\"{_proxyUrl}/v1/chat/completions\"); } // 记录调试日志 if(!IsProduction){ LogRequest(request); } return await base.SendAsync(request, cancellationToken); } 架构优势 模块化设计 存储层与业务逻辑解耦 支持快速切换向量数据库 配置驱动的基础设施 效率优化 本地缓存机制减少IO开销 异步流水线处理文档 智能分块策略（MaxToken控制） 可观测性 内置请求日志追踪 文档版本管理 检索结果可解释性 最佳实践建议 扩展方案 添加混合检索策略（关键词+向量） 实现缓存层（Redis/MemoryCache） 接入监控系统（Prometheus + Grafana） 应用场景示例 企业知识库问答系统 技术文档智能助手 法律条款检索分析 医疗知识辅助决策 通过结合 C# 的强类型特性与 RAG 的灵活检索能力，我们能够构建出既可靠又智能的知识处理系统。微软 KernelMemory 库的深度集成，使得实现复杂的语义处理流程变得异常简单。未来可探索与 ML.NET 的整合，打造完全基于 .NET 生态的智能解决方案。 本单元代码地址如下：https://github.com/zt199510/deepseeksk/tree/main/Test2 posted @ 2025-01-30 02:38 可乐_加冰 阅读(1145) 评论(0) 收藏 举报 刷新页面返回顶部 公告 博客园 © 2004-2026 浙公网安备 33010602011771号 浙ICP备2021040463号-3","source":"web","publishedAt":"2025-02-01T08:00:00+08:00"},{"id":"bocha-9","title":"【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题?-阿里云开发者社区","url":"https://developer.aliyun.com/article/1576857","snippet":"开发者社区 数据库 文章 正文 【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题？ 2024-08-03 704 版权 版权声明： 本文内容由阿里云实名注册用户自发贡献，版权归原作者所有，阿里云开发者社区不拥有其著作权，亦不承担相应法律责任。具体规则请查看《 阿里云开发者社区用户服务协议》和 《阿里云开发者社区知识产权保护指引》。如果您发现本社区中有涉嫌抄袭的内容，填写 侵权投诉表单进行举报，一经查实，本社区将立刻删除涉嫌侵权内容。 简介： 【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题？ linxinliang 目录 热门文章 最新文章","source":"web","publishedAt":"2024-08-03T23:14:42+08:00"},{"id":"bocha-0","title":"使用 LangChain、LangGraph 和 RAGAS 构建复杂的 RAG 系统,大模型入门到精通,收藏这篇就足够了!_人工智能_AI小白龙*-北京朝阳AI社区","url":"https://devpress.csdn.net/aibjcy/68f05451a6dc56200e93a12a.html","snippet":"引言\n想打造一个生产就绪的\nRAG(Retrieval-Augmented\nGeneration)系统?那可不是件简单的事儿!得一步步来,精心设计,反复迭代。咱们得先把数据收拾干净,然后试试不同的分块","source":"web","publishedAt":"2025-10-16T10:10:57+08:00"},{"id":"bocha-1","title":"大模型应用开发  RAG在实际落地场景中的优化(三)RAG落地案例分享_rag落地应用-CSDN博客","url":"https://blog.csdn.net/Androiddddd/article/details/146206916","snippet":"大模型应用开发 | RAG在实际落地场景中的优化（三）RAG落地案例分享 最新推荐文章于 2025-11-29 03:18:39 发布 原创 最新推荐文章于 2025-11-29 03:18:39 发布 · 1.5k 阅读 · 29 · 29 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #人工智能 #自然语言处理 #AI大模型 #大模型 #LLM #ai #RAG 三、RAG落地案例分享 3.1数据基础设施领域的RAG 3.1.1运维智能体背景 在数据基础设施领域，有很多运维SRE，每天会接收到大量的告警，因此很多时间来需要响应应急事件，进而进行故障诊断，然后故障复盘，进而进行经验沉淀。另外一部分时间又需要响应用户咨询，需要他们用他们的知识以及三方平台工具使用经验进行答疑。 因此我们希望通过打造一个数据基础设施的通用智能体来解决告警诊断，答疑的这些问题。 3.1.2严谨专业的RAG 传统的 RAG + Agent 技术可以解决通用的，确定性没那么高的，单步任务场景。但是面对数据基础设施领域的专业场景，整个检索过程必须是确定，专业和真实的，并且是需要一步一步推理的。 右边是一个通过NativeRAG的一个泛泛而谈的总结，可能对于一个普通的用户，对专业的领域知识没那么了解时，可能是有用的信息，但是这部分对于数据基础设施领域的专业人士，就没有什么意义了。因此我们比较了通用的智能体和数据基础设施智能体在RAG上面的区别： 通用的智能体：传统的RAG对知识的严谨和专业性要求没那么高，适用于客服，旅游，平台答疑机器人这样的一些业务场景。 数据基础设施智能体：RAG流程是严谨和专业的，需要专属的RAG工作流程，上下文包括(DB告警->根因定位->应急止血->故障恢复)，并且需要对专家沉淀的问答和应急经验，进行结构化的抽取，建立层次关系。因此我们选择知识图谱来作为数据承载。 3.1.3 知识处理 基于数据基础设施的确定性和特殊性，我们选择通过结合知识图谱来作为诊断应急经验的知识承载。我们通过SRE沉淀下来的应急排查事件知识经验 结合应急复盘流程，建立了DB应急事件驱动的知识图谱，我们以DB抖动为例，影响DB抖动的几个事件，包括慢SQL问题，容量问题，我们在各个应急事件间建立了层级关系。 最后通过我们通过规范化应急事件规则，一步一步地建立了多源的知识 -> 知识结构化抽取 ->应急关系抽取 -> 专家审核 -> 知识存储的一套标准化的知识加工体系。 3.1.4 知识检索 在智能体检索阶段，我们使用GraphRAG作为静态知识检索的承载，因此识别到DB抖动异常后，找到了与DB抖动异常节点相关的节点作为我们分析依据，由于在知识抽取阶段每一个节点还保留了每个事件的一些元数据信息，包括事件名，事件描述，相关工具，工具参数等等， 因此我们可以通过执行工具的执行生命周期链路来获取返回结果拿到动态数据来作为应急诊断的排查依据。通过这种动静结合的混合召回的方式比纯朴素的RAG召回，保障了数据基础设施智能体执行的确定性，专业性和严谨性。 3.1.5 AWEL + Agent 最后通过社区AWEL+AGENT技术，通过AGENT编排的范式，打造了从意图专家-> 应急诊断专家 -> 诊断根因分析专家。 每个Agent的职能都是不一样的，意图专家负责识别解析用户的意图和识别告警信息诊断专家需要通过GraphRAG 定位到需要分析的根因节点，以及获取具体的根因信息。分析专家需要结合各个根因节点的数据 + 历史分析复盘报告生成诊断分析报告。 3.2金融财报分析领域的RAG 基于DB-GPT的财报分析助手 ：https://www.yuque.com/eosphoros/dbgpt-docs/cmogrzbtmqf057oe 四、总结 建议围绕各自领域构建属于自己的领域资产库包括，知识资产，工具资产以及知识图谱资产 领域资产:领域资产包括了领域知识库，领域API，工具脚本，领域知识图谱。 资产处理，整个资产数据链路涉及了领域资产加工，领域资产检索和领域资产评估。 非结构化 -> 结构化：有条理地归类，正确地组织知识信息。 提取更加丰富的语义信息。 资产检索： 希望是有层级，优先级的检索而并非单一的检索 后置过滤很重要，最好能通过业务语义一些规则进行过滤。 如何系统学习掌握AI大模型？ AI大模型作为人工智能领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住AI大模型的风口，掌握AI大模型的知识和技能将变得越来越重要。 学习AI大模型是一个系统的过程，需要从基础开始，逐步深入到更高级的技术。 这里给大家精心整理了一份全面的AI大模型学习资源，包括：AI大模型全套学习路线图（从入门到实战）、精品AI大模型学习书籍手册、视频教程、实战学习、面试题等，资料免费分享！ 1. 成长路线图&学习规划 要学习一门新的技术，作为新手一定要先学习成长路线图，方向不对，努力白费。 这里，我们为新手和想要进一步提升的专业人士准备了一份详细的学习成长路线图和规划。可以说是最科学最系统的学习成长路线。 2. 大模型经典PDF书籍 书籍和学习文档资料是学习大模型过程中必不可少的，我们精选了一系列深入探讨大模型技术的书籍和学习文档，它们由领域内的顶尖专家撰写，内容全面、深入、详尽，为你学习大模型提供坚实的理论基础。（书籍含电子版PDF） 3. 大模型视频教程 对于很多自学或者没有基础的同学来说，书籍这些纯文字类的学习教材会觉得比较晦涩难以理解，因此，我们提供了丰富的大模型视频教程，以动态、形象的方式展示技术概念，帮助你更快、更轻松地掌握核心知识。 4. 2024行业报告 行业分析主要包括对不同行业的现状、趋势、问题、机会等进行系统地调研和评估，以了解哪些行业更适合引入大模型的技术和应用，以及在哪些方面可以发挥大模型的优势。 5. 大模型项目实战 学以致用 ，当你的理论知识积累到一定程度，就需要通过项目实战，在实际操作中检验和巩固你所学到的知识，同时为你找工作和职业发展打下坚实的基础。 6. 大模型面试题 面试不仅是技术的较量，更需要充分的准备。 在你已经掌握了大模型技术之后，就需要开始准备面试，我们将提供精心整理的大模型面试题库，涵盖当前面试中可能遇到的各种技术问题，让你在面试中游刃有余。 全套的AI大模型学习资源已经整理打包，有需要的小伙伴可以微信扫描下方CSDN官方认证二维码，免费领取【保证100%免费】 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 大模型本地部署_ 关注 关注 29 点赞 踩 29 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 举报 举报 精选资源 AI大模型RAG项目实战课 10-29 该课程强调了RAG技术在大模型落地中的重要性，并列举了多种应用场景，如客服自动化、文档撰写、图像生成以及数据处理分析等。RAG技术通过结合非参数化的语料库数据库和参数化模型，改善了纯参数化模型的局限性。 ... 参与评论 您还未登录，请先 登录 后发表或查看评论 【企业级大模型开发】企业级大模型开发：MCP+RAG实战案例详解 08-18 内容概要：本文详细介绍了MCP（模型控制生产）与RAG（检索增强生成）结合在企业级大模型开发中的应用，特别是通过一个金融知识问答系统的实战案例展示其技术优势。MCP强调模型治理、性能监控和迭代优化，确保AI系统... 一文搞懂大模型RAG应用（附实践案例） 热门推荐 AIPHIL的博客 11-21 2万+ 大模型（Large Language Model，LLM）的浪潮已经席卷了几乎各行业，但当涉及到专业场景或行业细分领域时，通用大模型就会面临专业知识不足的问题。相对于成本昂贵的“Post Train”或“SFT”，基于RAG的技术方案往往成为一种更优选择。本文从RAG架构入手，详细介绍相关技术细节，并附上一份实践案例。检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。知识的局限性。 RAGFlow+DeepSeek-R1:14b落地案例分享（足够详细）：机加工行业设备维保场景 2401_84204413的博客 02-22 1597 2.1。 Ragas评估框架：如何提升金融数据分析的检索质量与准确性 最新发布 gitblog_00689的博客 11-29 882 在金融投资研究领域，快速准确地从海量数据中检索相关信息至关重要。Ragas作为一个专业的RAG（检索增强生成）评估框架，能够帮助量化金融数据分析系统的检索质量，确保投资决策基于准确、可靠的信息。📈 ## 为什么金融数据分析需要RAG评估？ 金融数据分析涉及大量的非结构化数据，包括公司财报、行业分析资料、新闻资讯等。传统的检索系统往往面临以下挑战： - **信息过载**：每天产生海量金融数据 72 个 RAG 实战场景大公开！从医疗到金融，总有一个戳中你的需求（附开源方案） m0_48891301的博客 06-11 2032 在大模型时代，RAG（检索增强生成）就像一把万能钥匙，正在解锁 AI 应用的无限可能。2024 年，从 GraphRAG 的知识图谱创新到多模态 RAG 的视觉突破，从医疗场景的精准诊断到企业级知识库的高效构建，RAG 技术正以「七十二变」的姿态渗透到各个领域。本文精心整理72 个真实场景的 RAG 落地案例，涵盖技术原理、开源项目与实战价值，建议收藏！ 一文彻底搞懂大模型RAG应用（附实战案例） 2401_84208172的博客 12-11 2050 大模型（Large Language Model，LLM）的浪潮已经席卷了几乎各行业，但当涉及到专业场景或行业细分领域时，通用大模型就会面临专业知识不足的问题。相对于成本昂贵的“Post Train”或“SFT”，基于RAG的技术方案往往成为一种更优选择。本文从RAG架构入手，详细介绍相关技术细节，并附上一份实践案例。检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。知识的局限性。 一文读懂RAG和LLM微调，教你结合业务场景落地LLM应用 aolan123的博客 10-23 2088 1. 需要外部知识吗？对于以前摘要的风格进行摘要的任务，主要数据源将是以前的摘要本身。如果这些摘要包含在静态数据集中，就不太需要持续外部数据检索。但是，如果有一个频繁更新的摘要动态数据库，目标是不断与最新条目对齐的话，RAG可能在这个场景更好发挥作用。2. 需要模型适配吗？这个用例的核心是适应专业领域或特定的写作风格。微调特别擅长捕捉风格细微差异、语调变化和特定领域的词汇，因此对于这个维度来说，微调也是是一个必要的选择。3. 必须是最小化幻觉吗？在大多数LLM应用中，都会存在响应幻觉的问题。 精选资源 2025大模型RAG+图计算实战案例合集.pdf 05-07 接着，文档深入探讨了腾讯在大模型应用中使用的三种技术：SFT（Supervised Fine-Tuning）、RAG和Agent。SFT基于大语言模型进行微调，结合业务专属数据固化特定领域的业务知识。RAG技术通过结合外部知识库和检索技术... 精选资源 2024 Agent+RAG：基于大模型的融合应用探索（八大案例，共146页）.pdf 02-12 内容概要：该文章汇集了八个典型的 Agent 和 RAG（检索增强生成）融合应用案例，详细展示了大模型技术在游戏娱乐、金融科技、语音助手、办公支持等多个领域中的应用。文章不仅探讨了 Agent 如何改变多领域中的交互与... 精选资源 2024 RAG核心技术与应用手册（八大案例，共181页）.pdf 02-12 文章从技术原理、性能优化策略，到实际应用场景如腾讯大模型业务落地、京东电商搜索优化、小红书的生成式检索等，展示了RAG在多个领域的重要作用及成效。文章还探讨了知识图谱和Agent技术在RAG系统中的应用以及语音... 企业级RAG实施指南，企业知识库落地一定不要错过，长文建议收藏 Android23333的博客 05-19 1121 RAG系统配置最佳实践与企业选型指南，企业知识库落地避坑宝典 企业级RAG系统配置与框架选型：从需求到实施 RAG框架在企业中的深度应用与选型策略 企业如何成功实施Cherry Studio、AnythingLLM和RAGFlow？ 一份指南明白企业级RAG实施指南 ，想要成功实施RAG 不要错过 揭秘！企业级RAG系统架构深度解析：从0到1构建智能知识引擎（附实战案例） weixin_40593051的博客 05-11 1349 本文聚焦企业级RAG系统架构，解析其从0到1的构建过程。RAG通过“检索-生成”模式，整合外部知识库与生成式AI，解决传统知识管理效率低、准确性差等问题，可提升知识检索准确率40%-60%，缩短问题解决时间50%以上。架构涵盖数据层（分布式存储、向量数据库）、处理层（检索、生成、融合模块）、应用层（多交互接口、监控），关键技术涉及Milvus、GPT-4o等工具。还介绍知识预处理、高效检索、提示工程等模块实现，以及性能优化、安全合规设计。实战案例显示，其能显著提升企业效率、降低成本。未来RAG将向多模态、 万字长文讲透 RAG 在实际落地场景中的优化 m0_56255097的博客 02-07 1172 2. RAG流程优化RAG流程的优化我们又分为了静态文档的RAG和动态数据获取的RAG，目前大部分涉及到的RAG只覆盖了非结构化的文档静态资产，但是实际业务很多场景的问答是通过工具获取动态数据 + 静态知识数据共同回答的场景，不仅需要检索到静态的知识，同时需要RAG检索到工具资产库里面工具信息并执行获取动态数据。构建企业领域工具资产库，将散落到各个平台的工具API，工具脚本进行整合，进而提供智能体端到端的使用能力。比如，除了静态知识库以外，我们可以通过导入工具库的方式进行工具的处理。 一文详解几种常见本地大模型个人知识库工具部署、微调及对比选型 youmaob的博客 07-24 7440 这里先盘点一下最近比较火爆的几个工具，将从知识库侧和大模型侧分别介绍。这六种。AnythingLLM 是 Mintplex Labs Inc. 开发的一款可以与任何内容聊天的私人 ChatGPT，是高效、可定制、开源的企业级文档聊天机器人解决方案。它能够将任何文档、资源或内容片段转化为大语言模型（LLM）在聊天中可以利用的相关上下文。其采用MIT许可证的开源框架，支持快速在本地部署基于检索增强生成（RAG）的大模型应用。在不调用外部接口、不发送本地数据的情况下，确保用户数据的安全。 如何利用 instructor 提高 RAG 的准确性和召回率 weixin_43829633的博客 10-15 1740 RAG（Retrieval Augmented Generation）是一种检索增强生成技术，它利用大型语言模型来处理用户查询，RAG 技术的主要组成包括数据提取—embedding—创建索引—检索—排序（Rerank）—LLM 归纳生成，不过实际落地过程来看，将用户查询转换为嵌入向量直接检索，很多时候的结果在相关度方面没有那么理想，本篇分享一种对用户查询进行重写再去进行检索从而提高准确性和召回率的方案 RAG（大模型+知识库）落地与知识管理的春天-新的知识运营体系 weixin_59191169的博客 05-23 4384 大模型时代来了，可能你也知道GPT大模型是被海量知识训练出来的，但不知道你有没有问过，什么样的知识才能训练大模型？站在企业的角度，很多企业都有自己的知识库或者文档中心，很多人也都有自己积攒数年的资料库，那是不是用上大模型，就能轻松实现基于自己知识库的智能搜索/智能问答/智能推荐呢？（一）知识基础形态和知识质量在传统的搜索、问答、推荐等场景中，通常会返回一整篇的文档，我们还得在这一大篇资料中去找到自己想要的那一段具体内容。不少企业构建的知识库，其实就是文档库。 简单三个案例来分析RAG、微调如何选择？ AI大模型的博客 06-21 1601 我们重点来讨论几个案例，来看一下每个案例到底选择RAG，还是微调，或者是RAG+微调。：比如说我们想打造一个AI的投资理财规划师，比如我根据用户的风险偏好，还有一些用户的情况来给他一个合理的建议，比如说基于一些目前市场上的情况，那这种AI的规划师我们到底怎么打造？那这里我们需要考虑的是RAG还是微调呢？大家可以先思考一下。那为了回答这个问题，我们首先要剖析那这样的系统它到底需要具备什么样的能力？1、第一个很重要，就是可以处理实时的数据，或者叫；2、它也需要具备； 金融服务中的GraphRAG和标准RAG对比案例解析 2401_85375298的博客 10-25 1113 使用案例：灾后索赔管理。数据 ：历史索赔、客户资料、保单详细信息、灾害影响数据、地理数据、社交网络、天气模式。 三、RAG落地案例分享 3.1数据基础设施领域的RAG 3.1.1运维智能体背景 在数据基础设施领域，有很多运维SRE，每天会接收到大量的告警，因此很多时间来需要响应应急事件，进而进行故障诊断，然后故障复盘，进而进行经验沉淀。另外一部分时间又需要响应用户咨询，需要他们用他们的知识以及三方平台工具使用经验进行答疑。 因此我们希望通过打造一个数据基础设施的通用智能体来解决告警诊断，答疑的这些问题。 3.1.2严谨专业的RAG 传统的 RAG + Agent 技术可以解决通用的，确定性没那么高的，单步任务场景。但是面对数据基础设施领域的专业场景，整个检索过程必须是确定，专业和真实的，并且是需要一步一步推理的。 右边是一个通过NativeRAG的一个泛泛而谈的总结，可能对于一个普通的用户，对专业的领域知识没那么了解时，可能是有用的信息，但是这部分对于数据基础设施领域的专业人士，就没有什么意义了。因此我们比较了通用的智能体和数据基础设施智能体在RAG上面的区别： 通用的智能体：传统的RAG对知识的严谨和专业性要求没那么高，适用于客服，旅游，平台答疑机器人这样的一些业务场景。 数据基础设施智能体：RAG流程是严谨和专业的，需要专属的RAG工作流程，上下文包括(DB告警->根因定位->应急止血->故障恢复)，并且需要对专家沉淀的问答和应急经验，进行结构化的抽取，建立层次关系。因此我们选择知识图谱来作为数据承载。 3.1.3 知识处理 基于数据基础设施的确定性和特殊性，我们选择通过结合知识图谱来作为诊断应急经验的知识承载。我们通过SRE沉淀下来的应急排查事件知识经验 结合应急复盘流程，建立了DB应急事件驱动的知识图谱，我们以DB抖动为例，影响DB抖动的几个事件，包括慢SQL问题，容量问题，我们在各个应急事件间建立了层级关系。 最后通过我们通过规范化应急事件规则，一步一步地建立了多源的知识 -> 知识结构化抽取 ->应急关系抽取 -> 专家审核 -> 知识存储的一套标准化的知识加工体系。 3.1.4 知识检索 在智能体检索阶段，我们使用GraphRAG作为静态知识检索的承载，因此识别到DB抖动异常后，找到了与DB抖动异常节点相关的节点作为我们分析依据，由于在知识抽取阶段每一个节点还保留了每个事件的一些元数据信息，包括事件名，事件描述，相关工具，工具参数等等， 因此我们可以通过执行工具的执行生命周期链路来获取返回结果拿到动态数据来作为应急诊断的排查依据。通过这种动静结合的混合召回的方式比纯朴素的RAG召回，保障了数据基础设施智能体执行的确定性，专业性和严谨性。 3.1.5 AWEL + Agent 最后通过社区AWEL+AGENT技术，通过AGENT编排的范式，打造了从意图专家-> 应急诊断专家 -> 诊断根因分析专家。 每个Agent的职能都是不一样的，意图专家负责识别解析用户的意图和识别告警信息诊断专家需要通过GraphRAG 定位到需要分析的根因节点，以及获取具体的根因信息。分析专家需要结合各个根因节点的数据 + 历史分析复盘报告生成诊断分析报告。 3.2金融财报分析领域的RAG 基于DB-GPT的财报分析助手 ：https://www.yuque.com/eosphoros/dbgpt-docs/cmogrzbtmqf057oe 四、总结 建议围绕各自领域构建属于自己的领域资产库包括，知识资产，工具资产以及知识图谱资产 领域资产:领域资产包括了领域知识库，领域API，工具脚本，领域知识图谱。 资产处理，整个资产数据链路涉及了领域资产加工，领域资产检索和领域资产评估。 非结构化 -> 结构化：有条理地归类，正确地组织知识信息。 提取更加丰富的语义信息。 资产检索： 希望是有层级，优先级的检索而并非单一的检索 后置过滤很重要，最好能通过业务语义一些规则进行过滤。 如何系统学习掌握AI大模型？ AI大模型作为人工智能领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住AI大模型的风口，掌握AI大模型的知识和技能将变得越来越重要。 学习AI大模型是一个系统的过程，需要从基础开始，逐步深入到更高级的技术。 这里给大家精心整理了一份全面的AI大模型学习资源，包括：AI大模型全套学习路线图（从入门到实战）、精品AI大模型学习书籍手册、视频教程、实战学习、面试题等，资料免费分享！ 1. 成长路线图&学习规划 要学习一门新的技术，作为新手一定要先学习成长路线图，方向不对，努力白费。 这里，我们为新手和想要进一步提升的专业人士准备了一份详细的学习成长路线图和规划。可以说是最科学最系统的学习成长路线。 2. 大模型经典PDF书籍 书籍和学习文档资料是学习大模型过程中必不可少的，我们精选了一系列深入探讨大模型技术的书籍和学习文档，它们由领域内的顶尖专家撰写，内容全面、深入、详尽，为你学习大模型提供坚实的理论基础。（书籍含电子版PDF） 3. 大模型视频教程 对于很多自学或者没有基础的同学来说，书籍这些纯文字类的学习教材会觉得比较晦涩难以理解，因此，我们提供了丰富的大模型视频教程，以动态、形象的方式展示技术概念，帮助你更快、更轻松地掌握核心知识。 4. 2024行业报告 行业分析主要包括对不同行业的现状、趋势、问题、机会等进行系统地调研和评估，以了解哪些行业更适合引入大模型的技术和应用，以及在哪些方面可以发挥大模型的优势。 5. 大模型项目实战 学以致用 ，当你的理论知识积累到一定程度，就需要通过项目实战，在实际操作中检验和巩固你所学到的知识，同时为你找工作和职业发展打下坚实的基础。 6. 大模型面试题 面试不仅是技术的较量，更需要充分的准备。 在你已经掌握了大模型技术之后，就需要开始准备面试，我们将提供精心整理的大模型面试题库，涵盖当前面试中可能遇到的各种技术问题，让你在面试中游刃有余。 全套的AI大模型学习资源已经整理打包，有需要的小伙伴可以微信扫描下方CSDN官方认证二维码，免费领取【保证100%免费】","source":"web","publishedAt":"2025-03-12T17:48:52+08:00"},{"id":"bocha-2","title":"世界上最简单最暴力的 RAG 系统-CSDN博客","url":"https://blog.csdn.net/xxue345678/article/details/141421805","snippet":"世界上最简单最暴力的 RAG 系统 最新推荐文章于 2025-12-12 09:58:45 发布 原创 最新推荐文章于 2025-12-12 09:58:45 发布 · 924 阅读 · 24 · 22 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #transformer #深度学习 #人工智能 #ai大模型 #大语言模型 #RAG #计算机技术 要搞一个 RAG 系统，hmmm,大模型，向量模型，向量存储这三个是标配，接着呢，为了让效果变得更好, 就要继续加啊加，再来个 rerank 模型，提高下精度，向量存储不够，再加个全文检索，做混合多路召回。瞬间把系统复杂度拉满，然后又要开始做资源隔离，做运维了，保证多系统协同工作，高可用 balabala… 光把系统搞复杂怎么行？应用也要搞复杂！效果还不高，引入llama_index，这玩意提供了上百种存储，召回策略，搞 oneapi, 这玩意封装了无数个大模型接口。 接着发现，开发应用太复杂了，要不再引入个 workflow ，减轻下开发的压力，把一些工作转移给业务？乌拉，成功转移，大家都有活干。 诶，发现和以前大数据时代，实时性好像不太好搞了，得搞个实时系统啊，那得再加kafka,全量批量更新向量数据库 继续balabala… 不就是一个获取一些 context，然后让大模型基于这些context 回答用户的一些question ，得到一个答案，就这么简单的一件事，为啥被这些搞软件，搞AI的同学搞成现在这么复杂呀？ 于是，我决定，我要搞一个没有向量数据库，没有向量模型，没有检索存储，没有向量数据库，没有llama_index,没有各种封装， 对，只依赖一个大模型就可以用的RAG系统！ 一条语句安装系统： pip install auto-coder 启动模型代理： easy_byzerllm deploy deepseek-chat --token _api_key 启动兼容 OpenAI 的服务： auto-coder doc serve \\ --model deepseek_chat \\ --doc_dir /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy/byzer-sql 通过 doc_dir 指定你的文档目录。这 在 NextChat 里配置下 API 地址 http://127.0.0.1:8000/v1。 这就是一个带有你私有知识库的 deepseek 了： 我还可以方便的集成到我的业务系统去用： 这就完了？当然不是！我们来讲讲原理。 前面的RAG系统为啥搞那么复杂？因为想当年大模型太弱！速度慢，窗口端（还记得4k的岁月么），一篇文档都放不下！ 速度慢，成本高，只能让大模型继续退化成rerank模型，embeding模型，去做数据初级过滤。 现在，大模型已经获得大发展。128K已经是标配，你看人家Kimi动不动几十篇文章放context里去，切啥片？ 现在7B 快如闪电，deepseek 成本低如狗，我们直接暴力并发让大模型看所有的内容来做，只输出一个token(yes/no)，加上他们家的kv磁盘缓存,不要太贵好么。 所以 auto-coder 新模式，直接使用大模型来做数据过滤（高并发，高输入，低输出），过滤的数据直接以文档为粒度放入到上下文给到大模型做回,效果巨好，远超“片段”模式。不服来PK。 原理讲完了，这就完了？当然不是，这个想法，我刚想清楚，用了十分钟就是实现了，全自动AI生成的代码， prompt 都是AI自己写的： from typing import Any, Dict, List, Optional, Tuple, Generator from autocoder.common import AutoCoderArgs from concurrent.futures import ThreadPoolExecutor, as_completed from byzerllm import ByzerLLM from loguru import logger import json import os import byzerllm class LongContextRAG: def __init__(self, llm: ByzerLLM, args: AutoCoderArgs, path: str) -> None: self.llm = llm self.args = args self.path = path @byzerllm.prompt() def _check_relevance(self, query: str, document: str) -> str: \"\"\" 请判断以下文档是否能够回答给出的问题。 只需回答\"是\"或\"否\"。 问题：{{ query }} 文档： {{ document }} 回答： \"\"\" @byzerllm.prompt() def _answer_question( self, query: str, relevant_docs: List[str] ) -> Generator[str, None, None]: \"\"\" 使用以下文档来回答问题。如果文档中没有相关信息，请说\"我没有足够的信息来回答这个问题\"。 文档： {% for doc in relevant_docs %} {{ doc }} {% endfor %} 问题：{{ query }} 回答： \"\"\" def _retrieve_documents(self) -> List[str]: documents = [] for root, dirs, files in os.walk(self.path): for file in files: if file.endswith(\".md\"): file_path = os.path.join(root, file) with open(file_path, \"r\", encoding=\"utf-8\") as f: documents.append(f.read()) return documents def stream_chat_oai( self, conversations, model: Optional[str] = None, role_mapping=None, llm_config: Dict[str, Any] = {}, ): query = conversations[-1][\"content\"] documents = self._retrieve_documents() with ThreadPoolExecutor( max_workers=self.args.index_filter_workers or 5 ) as executor: future_to_doc = { executor.submit( self._check_relevance.with_llm(self.llm).run, query, doc ): doc for doc in documents } relevant_docs = [] for future in as_completed(future_to_doc): try: doc = future_to_doc[future] if \"是\" in future.result().strip().lower(): relevant_docs.append(doc) except Exception as exc: logger.error(f\"Document processing generated an exception: {exc}\") if not relevant_docs: return [\"没有找到相关的文档来回答这个问题。\"], [] else: chunks = self._answer_question.with_llm(self.llm).run(query, relevant_docs) return chunks, [] 然后再让AI写个 jupyter notebook 让我测试下： 然后我就发布了！可以到这里看源码：https://github.com/allwefantasy/auto-coder/blob/master/src/autocoder/rag/long_context_rag.py 最后，这个代码是在 auto-coder里，但是是用 auto-coder.chat 开发的。对每次，auto-coder.chat 自己开发自己！ 我是一个坚定的大模型暴力论者，能简单用大模型就解决的，就用大模型解决！ 如何学习大模型 AI ？ 由于新岗位的生产效率，要优于被取代岗位的生产效率，所以实际上整个社会的生产效率是提升的。 但是具体到个人，只能说是： “最先掌握AI的人，将会比较晚掌握AI的人有竞争优势”。 这句话，放在计算机、互联网、移动互联网的开局时期，都是一样的道理。 我在一线互联网企业工作十余年里，指导过不少同行后辈。帮助很多人得到了学习和成长。 我意识到有很多经验和知识值得分享给大家，也可以通过我们的能力和经验解答大家在人工智能学习中的很多困惑，所以在工作繁忙的情况下还是坚持各种整理和分享。但苦于知识传播途径有限，很多互联网行业朋友无法获得正确的资料得到学习提升，故此将并将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 第一阶段（10天）：初阶应用 该阶段让大家对大模型 AI有一个最前沿的认识，对大模型 AI 的理解超过 95% 的人，可以在相关讨论时发表高级、不跟风、又接地气的见解，别人只会和 AI 聊天，而你能调教 AI，并能用代码将大模型和业务衔接。 大模型 AI 能干什么？大模型是怎样获得「智能」的？用好 AI 的核心心法大模型应用业务架构大模型应用技术架构代码示例：向 GPT-3.5 灌入新知识提示工程的意义和核心思想Prompt 典型构成指令调优方法论思维链和思维树Prompt 攻击和防范… 第二阶段（30天）：高阶应用 该阶段我们正式进入大模型 AI 进阶实战学习，学会构造私有知识库，扩展 AI 的能力。快速开发一个完整的基于 agent 对话机器人。掌握功能最强的大模型开发框架，抓住最新的技术进展，适合 Python 和 JavaScript 程序员。 为什么要做 RAG搭建一个简单的 ChatPDF检索的基础概念什么是向量表示（Embeddings）向量数据库与向量检索基于向量检索的 RAG搭建 RAG 系统的扩展知识混合检索与 RAG-Fusion 简介向量模型本地部署… 第三阶段（30天）：模型训练 恭喜你，如果学到这里，你基本可以找到一份大模型 AI相关的工作，自己也能训练 GPT 了！通过微调，训练自己的垂直大模型，能独立训练开源多模态大模型，掌握更多技术方案。 到此为止，大概2个月的时间。你已经成为了一名“AI小子”。那么你还想往下探索吗？ 为什么要做 RAG什么是模型什么是模型训练求解器 & 损失函数简介小实验2：手写一个简单的神经网络并训练它什么是训练/预训练/微调/轻量化微调Transformer结构简介轻量化微调实验数据集的构建… 第四阶段（20天）：商业闭环 对全球大模型从性能、吞吐量、成本等方面有一定的认知，可以在云端和本地等多种环境下部署大模型，找到适合自己的项目/创业方向，做一名被 AI 武装的产品经理。 硬件选型带你了解全球大模型使用国产大模型服务搭建 OpenAI 代理热身：基于阿里云 PAI 部署 Stable Diffusion在本地计算机运行大模型大模型的私有化部署基于 vLLM 部署大模型案例：如何优雅地在阿里云私有部署开源大模型部署一套开源 LLM 项目内容安全互联网信息服务算法备案… 学习是一个过程，只要学习就会有挑战。天道酬勤，你越努力，就会成为越优秀的自己。 如果你能在15天内完成所有的任务，那你堪称天才。然而，如果你能完成 60-70% 的内容，你就已经开始具备成为一名大模型 AI 的正确特征了。 这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【保证100%免费】 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 雪碧没气阿 关注 关注 24 点赞 踩 22 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 举报 举报 RAG私域问答场景升级版方案(第二期方案)[2]：工业级别构建私域问答（业务问题、性能问题、安全成本问题等详细解决方案） 丨汀、的博客 08-07 1800 RAG私域问答场景升级版方案(第二期方案)[2]：工业级别构建私域问答（业务问题、性能问题、安全成本问题等详细解决方案） 参与评论 您还未登录，请先 登录 后发表或查看评论 RAG系统向量数据库选型与Prompt Engineering鲁棒性测试实践 weixin_44872675的博客 06-10 1028 RAG与Prompt Engineering是推动AI应用落地的两大支柱，分别考验工程师对底层架构与系统安全的把控。测试工程师应深入理解向量数据库的能力边界，系统性设计Prompt测试用例，为AI系统的稳定、安全运行保驾护航！你们的测试不仅是最后一道防线，更是AI产品可持续创新的基石。欢迎留言交流你在RAG与Prompt Engineering测试实践中的心得与难题！ RAG 入门指南：从零开始构建一个 RAG 系统 mama19971023的博客 08-05 2476 在开始之前，我还是打算再次简要的介绍一下 RAG。在 Meta 的官方 Blog 上有这样一段话：这段话主要讲述了一个新的模型架构，也就是RAG (检索增强生成)的重要性和优势。可以概括为以下几点：1. 构建一个能够进行研究和上下文分析的模型虽然更具挑战性，但对未来的技术进步非常关键；2. 通过在知识密集的下游任务上微调，RAG 可以实现最先进的结果，比现有的最大的预训练序列到序列语言模型还要好；3. 与传统的预训练模型不同，RAG 的内部知识可以轻松地动态更改或补充。 RAG系统：用大模型赋能实时信息检索 xiangxueerfei的博客 12-14 996 检索增强型生成（RAG）系统正在重塑我们处理AI驱动信息的方式。作为架构师，我们需要理解这些系统的基本原理，从而有效地发挥它们的潜力。什么是RAG？总体而言，RAG系统通过将大型语言模型（LLM）与外部知识源集成，增强了其能力。这种集成允许模型动态地引入相关信息，使其能够生成不仅连贯而且事实准确、上下文相关的回应。RAG系统的主要组成部分包括：·检索器（Retriever）： 该组件从外部知识库中获取相关数据。·生成器（Generator）： LLM将检索到的信息综合成类似人类的回应。 什么是 RAG? 如何为自己的企业快速搭建一套 RAG 系统 最新发布 Y525698136的博客 12-12 781 今天就用大白话讲清什么是 RAG，再手把手教你快速搭建企业级 RAG 系统，新手也能轻松上手～ RAG 系统完全指南：从基础到高级实践 qingkahui24689的博客 01-09 1285 RAG 的核心思想是将信息检索与生成模型相结合。在传统的 LLM 应用中,模型仅依赖训练时学到的知识来回答问题,这导致了知识更新困难、回答可能过时或不准确等问题。而 RAG 系统通过在生成回答前主动检索相关信息,将实时、准确的知识作为上下文提供给模型,从而显著提升了回答的质量和可靠性。RAG 技术最显著的优势在于其知识的时效性和可追溯性。通过持续更新知识库,系统可以始终掌握最新信息,而无需重新训练模型。同时,每个回答都能追溯到具体的信息来源,这极大地提升了系统的可信度和实用性。 RAG系统全景：架构详解与落地实践指南 Everly_的博客 08-08 938 检索增强生成（RAG）已不仅仅是一项补充性技术，它更是将大模型从一个通用的“知识大脑”转变为能够解决特定领域问题的“专家系统”的核心桥梁。它通过“开卷考试”的模式，从根本上解决了LLM在知识时效性、领域深度和事实准确性上的固有缺陷，为AI在企业中的规模化落地铺平了最关键的一段路。构建一个卓越的RAG系统，并非单一技术的突破，而是一场贯穿数据处理全流程的系统工程。加载、切分、索引、嵌入、召回等，其中的每一个环节都充满了性能、成本与效果之间的权衡，不存在一劳永逸的“银弹”，只有最适合特定场景的最佳实践。 RAG实战篇：构建一个最小可行性的Rag系统 xx_nm98的博客 09-24 2295 经过上述流程，我们搭建了一个非常简单的Naive RAG系统，这个系统解析了一篇博客文章，然后接收用户提问，并使用博客的内容做增强生成。这是一个非常简单的框架，也很易于理解。但是在实际应用中还有非常多需要优化的地方，包括Indexing（索引）、Query Translation（查询转换）、Routing（路由）、Query Construction（查询构建）、Retrival（检索）和Generation（生成），每个环节都有多种有效的优化方式。 RAG学习（三）—— 优化索引构建 m0_73944777的博客 08-21 1323 本文介绍了RAG（检索增强生成）系统中的索引构建技术。主要内容包括： 索引的基本概念、常见向量索引类型、索引构建流程以及索引的优化技术 AI知识点——RAG cui_hao_nan的博客 07-21 700 在 RAG 中，Rerank 是一个对初步检索返回的候选文档列表进行再次排序的过程。因为初步检索需要快速地在海量的文档中找出大致相关的文档，其需要考虑效率，所以查找出的文档不会非常准确，这步是粗排。在已经筛选的相关的文档中再进行精筛，找出匹配度更高的文档让其排在前面，选其中的 Top-K然后扔给大模型，提高答案的准确性，这就是 Rerank，也是精排。Rerank 需要怎么做?初步检索生成候选文档：使用速度较快的传统检索方法获得一组候选文档。 一文读懂：大模型RAG weixin_42029738的博客 06-11 1521 本文概述 RAG 的核心算法，并举例说明其中的一些方法。RAG融合是一个强大的功能，能够提高RAG应用的语义搜索效率。通过使用语言模型生成多个查询并对搜索结果进行重新排序，RAG融合可以呈现更丰富多样的内容，并提供了一个额外的层次，用于调整应用。此外，RAG融合还可以实现自动纠正、节省成本以及增加内容多样性。但是，需要注意一些权衡，比如潜在的延迟问题、自动纠正的挑战以及成本影响。对于依赖常见概念但可能出现内部行话或重叠词汇的应用来说，RAG融合尤其有用。 RAG技术详解：构建高效、可信赖的知识检索系统 weixin_45312236的博客 05-29 5065 幻觉：在没有答案的情况下提供虚假信息。过时：当用户需要特定的当前响应时，提供过时或通用的信息。来源：从非权威来源创建响应。由于术语混淆，不同的培训来源使用相同的术语来谈论不同的事情，因此会产生不准确的响应。RAG 是解决其中一些挑战的一种方法。它会重定向 LLM，从权威的、预先确定的知识来源中检索相关信息。组织可以更好地控制生成的文本输出，并且用户可以深入了解 LLM 如何生成响应。 RAG系统（一）系统介绍与向量检索 h363659487的博客 05-21 873 介绍什么是RAG系统以及向量检索相关知识 RAG系统，实时信息检索的秘密武器！（非常详细）从零基础入门到精通，收藏这篇就够了 Javachichi的博客 05-29 935 RAG系统就像给AI安上了一双“慧眼”，让它能够实时检索信息，生成更准确、更靠谱的答案。它代表了AI架构的重大进步，是架构师们在现代数据环境中应对复杂性的利器。未来，随着技术的不断发展，RAG系统将会变得更高效、更安全，为各行各业带来更多创新应用。所以，别再犹豫了，赶紧拥抱RAG，一起迎接更智能、更实时的AI解决方案吧！大模型现在可是网络安全领域的热门话题，大家都想学！那么，怎么才能快速入门，成为大模型高手呢？别担心，这里有一份2025最新版的大模型学习路线图，能帮你系统地学习大模型，快速入门！ 中小企业级API大模型部署RAG客服系统 qq_46059087的博客 04-25 933 中小企业节约本地部署LLM成本并通过调用大模型API制作RAG问答系统的一些思路 【RAG】混合RAG系统，提升复杂推理任务表现 2301_81940605的博客 08-14 1364 检索增强生成（RAG）系统在处理复杂推理任务方面展现出显著的潜力。然而，现有的RAG系统在面对需要复杂推理、多领域知识集成及数值计算的任务时，仍存在性能瓶颈。为了进一步提升系统的表现，本文提出了一种混合RAG系统，通过整合多种优化方法，显著增强了系统的推理能力和处理复杂任务的能力。本文介绍的RAG系统设计并实现了一个包括网页处理、属性预测、数值计算、LLM知识提取、知识图谱及推理模块在内的综合架构。该系统能够有效地从多种来源提取信息，并通过高级推理模块结合这些信息，生成高质量的答案。 开源：基于DeepSeek打造RAG系统 CSDN_430422的博客 02-07 3488 RAG（Retrieval-Augmented Generation）即检索增强生成，是一种优化大型语言模型（LLM）输出的方法，旨在使LLM能够在生成响应之前引用训练数据之外的权威知识库。 【LLM大模型】RAG（检索增强生成）系统的问题及解决思路 DEVELOPERAA的博客 07-31 1316 随着大型语言模型（LLM）的出现，人们对更好搜索能力的需求催生了新的搜索方法。虽然基于关键字的传统搜索方法和推荐系统在某种程度上是有效的，但 LLM 的出现将搜索能力提升到了一个新的水平。 那可不可以先搭建跑起来，然后我们再根据结果来看是否需要预处理，包括该如何且精细到何种地步文档，可以么？先用最简单的方式跑起来一个本地RAG可以么 02-21 好的，用户现在问的是能不能先用最简单的方式搭建一个本地RAG跑起来，然后再根据结果调整预处理和文档处理的方式。这其实是一个很常见的需求，用户可能希望快速验证想法，而不是一开始就陷入复杂的配置和优化中。我... 要搞一个 RAG 系统，hmmm,大模型，向量模型，向量存储这三个是标配，接着呢，为了让效果变得更好, 就要继续加啊加，再来个 rerank 模型，提高下精度，向量存储不够，再加个全文检索，做混合多路召回。瞬间把系统复杂度拉满，然后又要开始做资源隔离，做运维了，保证多系统协同工作，高可用 balabala… 光把系统搞复杂怎么行？应用也要搞复杂！效果还不高，引入llama_index，这玩意提供了上百种存储，召回策略，搞 oneapi, 这玩意封装了无数个大模型接口。 接着发现，开发应用太复杂了，要不再引入个 workflow ，减轻下开发的压力，把一些工作转移给业务？乌拉，成功转移，大家都有活干。 诶，发现和以前大数据时代，实时性好像不太好搞了，得搞个实时系统啊，那得再加kafka,全量批量更新向量数据库 继续balabala… 不就是一个获取一些 context，然后让大模型基于这些context 回答用户的一些question ，得到一个答案，就这么简单的一件事，为啥被这些搞软件，搞AI的同学搞成现在这么复杂呀？ 于是，我决定，我要搞一个没有向量数据库，没有向量模型，没有检索存储，没有向量数据库，没有llama_index,没有各种封装， 对，只依赖一个大模型就可以用的RAG系统！ 一条语句安装系统： pip install auto-coder 启动模型代理： easy_byzerllm deploy deepseek-chat --token _api_key 启动兼容 OpenAI 的服务： auto-coder doc serve \\ --model deepseek_chat \\ --doc_dir /Users/allwefantasy/projects/llm_friendly_packages/github.com/allwefantasy/byzer-sql 通过 doc_dir 指定你的文档目录。这 在 NextChat 里配置下 API 地址 http://127.0.0.1:8000/v1。 这就是一个带有你私有知识库的 deepseek 了： 我还可以方便的集成到我的业务系统去用： 这就完了？当然不是！我们来讲讲原理。 前面的RAG系统为啥搞那么复杂？因为想当年大模型太弱！速度慢，窗口端（还记得4k的岁月么），一篇文档都放不下！ 速度慢，成本高，只能让大模型继续退化成rerank模型，embeding模型，去做数据初级过滤。 现在，大模型已经获得大发展。128K已经是标配，你看人家Kimi动不动几十篇文章放context里去，切啥片？ 现在7B 快如闪电，deepseek 成本低如狗，我们直接暴力并发让大模型看所有的内容来做，只输出一个token(yes/no)，加上他们家的kv磁盘缓存,不要太贵好么。 所以 auto-coder 新模式，直接使用大模型来做数据过滤（高并发，高输入，低输出），过滤的数据直接以文档为粒度放入到上下文给到大模型做回,效果巨好，远超“片段”模式。不服来PK。 原理讲完了，这就完了？当然不是，这个想法，我刚想清楚，用了十分钟就是实现了，全自动AI生成的代码， prompt 都是AI自己写的： from typing import Any, Dict, List, Optional, Tuple, Generator from autocoder.common import AutoCoderArgs from concurrent.futures import ThreadPoolExecutor, as_completed from byzerllm import ByzerLLM from loguru import logger import json import os import byzerllm class LongContextRAG: def __init__(self, llm: ByzerLLM, args: AutoCoderArgs, path: str) -> None: self.llm = llm self.args = args self.path = path @byzerllm.prompt() def _check_relevance(self, query: str, document: str) -> str: \"\"\" 请判断以下文档是否能够回答给出的问题。 只需回答\"是\"或\"否\"。 问题：{{ query }} 文档： {{ document }} 回答： \"\"\" @byzerllm.prompt() def _answer_question( self, query: str, relevant_docs: List[str] ) -> Generator[str, None, None]: \"\"\" 使用以下文档来回答问题。如果文档中没有相关信息，请说\"我没有足够的信息来回答这个问题\"。 文档： {% for doc in relevant_docs %} {{ doc }} {% endfor %} 问题：{{ query }} 回答： \"\"\" def _retrieve_documents(self) -> List[str]: documents = [] for root, dirs, files in os.walk(self.path): for file in files: if file.endswith(\".md\"): file_path = os.path.join(root, file) with open(file_path, \"r\", encoding=\"utf-8\") as f: documents.append(f.read()) return documents def stream_chat_oai( self, conversations, model: Optional[str] = None, role_mapping=None, llm_config: Dict[str, Any] = {}, ): query = conversations[-1][\"content\"] documents = self._retrieve_documents() with ThreadPoolExecutor( max_workers=self.args.index_filter_workers or 5 ) as executor: future_to_doc = { executor.submit( self._check_relevance.with_llm(self.llm).run, query, doc ): doc for doc in documents } relevant_docs = [] for future in as_completed(future_to_doc): try: doc = future_to_doc[future] if \"是\" in future.result().strip().lower(): relevant_docs.append(doc) except Exception as exc: logger.error(f\"Document processing generated an exception: {exc}\") if not relevant_docs: return [\"没有找到相关的文档来回答这个问题。\"], [] else: chunks = self._answer_question.with_llm(self.llm).run(query, relevant_docs) return chunks, [] 然后再让AI写个 jupyter notebook 让我测试下： 然后我就发布了！可以到这里看源码：https://github.com/allwefantasy/auto-coder/blob/master/src/autocoder/rag/long_context_rag.py 最后，这个代码是在 auto-coder里，但是是用 auto-coder.chat 开发的。对每次，auto-coder.chat 自己开发自己！ 我是一个坚定的大模型暴力论者，能简单用大模型就解决的，就用大模型解决！ 如何学习大模型 AI ？ 由于新岗位的生产效率，要优于被取代岗位的生产效率，所以实际上整个社会的生产效率是提升的。 但是具体到个人，只能说是： “最先掌握AI的人，将会比较晚掌握AI的人有竞争优势”。 这句话，放在计算机、互联网、移动互联网的开局时期，都是一样的道理。 我在一线互联网企业工作十余年里，指导过不少同行后辈。帮助很多人得到了学习和成长。 我意识到有很多经验和知识值得分享给大家，也可以通过我们的能力和经验解答大家在人工智能学习中的很多困惑，所以在工作繁忙的情况下还是坚持各种整理和分享。但苦于知识传播途径有限，很多互联网行业朋友无法获得正确的资料得到学习提升，故此将并将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 第一阶段（10天）：初阶应用 该阶段让大家对大模型 AI有一个最前沿的认识，对大模型 AI 的理解超过 95% 的人，可以在相关讨论时发表高级、不跟风、又接地气的见解，别人只会和 AI 聊天，而你能调教 AI，并能用代码将大模型和业务衔接。 大模型 AI 能干什么？大模型是怎样获得「智能」的？用好 AI 的核心心法大模型应用业务架构大模型应用技术架构代码示例：向 GPT-3.5 灌入新知识提示工程的意义和核心思想Prompt 典型构成指令调优方法论思维链和思维树Prompt 攻击和防范… 第二阶段（30天）：高阶应用 该阶段我们正式进入大模型 AI 进阶实战学习，学会构造私有知识库，扩展 AI 的能力。快速开发一个完整的基于 agent 对话机器人。掌握功能最强的大模型开发框架，抓住最新的技术进展，适合 Python 和 JavaScript 程序员。 为什么要做 RAG搭建一个简单的 ChatPDF检索的基础概念什么是向量表示（Embeddings）向量数据库与向量检索基于向量检索的 RAG搭建 RAG 系统的扩展知识混合检索与 RAG-Fusion 简介向量模型本地部署… 第三阶段（30天）：模型训练 恭喜你，如果学到这里，你基本可以找到一份大模型 AI相关的工作，自己也能训练 GPT 了！通过微调，训练自己的垂直大模型，能独立训练开源多模态大模型，掌握更多技术方案。 到此为止，大概2个月的时间。你已经成为了一名“AI小子”。那么你还想往下探索吗？ 为什么要做 RAG什么是模型什么是模型训练求解器 & 损失函数简介小实验2：手写一个简单的神经网络并训练它什么是训练/预训练/微调/轻量化微调Transformer结构简介轻量化微调实验数据集的构建… 第四阶段（20天）：商业闭环 对全球大模型从性能、吞吐量、成本等方面有一定的认知，可以在云端和本地等多种环境下部署大模型，找到适合自己的项目/创业方向，做一名被 AI 武装的产品经理。 硬件选型带你了解全球大模型使用国产大模型服务搭建 OpenAI 代理热身：基于阿里云 PAI 部署 Stable Diffusion在本地计算机运行大模型大模型的私有化部署基于 vLLM 部署大模型案例：如何优雅地在阿里云私有部署开源大模型部署一套开源 LLM 项目内容安全互联网信息服务算法备案… 学习是一个过程，只要学习就会有挑战。天道酬勤，你越努力，就会成为越优秀的自己。 如果你能在15天内完成所有的任务，那你堪称天才。然而，如果你能完成 60-70% 的内容，你就已经开始具备成为一名大模型 AI 的正确特征了。 这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【保证100%免费】","source":"web","publishedAt":"2024-09-16T16:34:53+08:00"},{"id":"bocha-3","title":"RAG实战篇：构建一个最小可行性的Rag系统","url":"https://new.qq.com/rain/a/20240927A05IBM00","snippet":"RAG实战篇：构建一个最小可行性的Rag系统人人都是产品经理2024-09-27 15:12发布于广东人人都是产品经理的官方账号在人工智能的世界里，RAG（Retrieval-Augmented Generation）技术正成为提升AI理解和生成能力的关键。本文将带你进入RAG系统的实战篇，从构建一个最小可行性的RAG系统开始，详细介绍如何将这一技术应用于实际场景中。在《AI大模型实战篇》系列文章中，风叔通过八篇文章，从最经典的ReAct模式开始，沿着规划路线介绍了REWOO、Plan&Execute和LLM Compiler，沿着反思路线介绍了Basic Reflection、Self Discover和Reflexion，并以最强大的设计模式LATS作为收尾。但是，所有的这些设计模式，都只是在告诉AI Agent应该如何规划和思考，且只能依赖于大模型既有的知识储备。而实际应用中，我们往往更希望AI Agent结合我们给定的知识和信息，在更专业的垂直领域内进行规划和思考。比如我们希望Agent帮我们做论文分析、书籍总结，或者在企业级场景中，让AI Agent写营销计划、内部知识问答、智能客服等等非常多的场景，只靠上面几种Agent设计模式是远远不够的，我们必须给大模型外挂知识库，并且通过工作流进一步约束和规范Agent的思考方向和行为模式。解决这个问题的最佳方式是利用RAG技术，接下来我们正式开启《RAG实战篇》系列。对于RAG还不太熟悉的朋友，可以先参考下面两篇文章：一、RAG系统实现方案概览我们将基于下图所示的框架，来构建一个完整的RAG系统。1. Indexing（索引）Indexing是任何RAG系统的第一步，在实际应用场景中，文档尺寸可能非常大，因此需要将长篇文档分割成多个文本块，以便更高效地处理和检索信息。Indexing环节主要面临三个难题：首先，内容表述不完整，内容块的语义信息受分割方式影响，致使在较长的语境中，重要信息被丢失或被掩盖。其次，块相似性搜索不准确，随着数据量增多，检索中的噪声增大，导致频繁与错误数据匹配，使得检索系统脆弱且不可靠。最后，参考轨迹不明晰，检索到的内容块可能来自任何文档，没有引用痕迹，可能出现来自多个不同文档的块，尽管语义相似，但包含的却是完全不同主题的内容。在这个框架中，我们将在索引环节实现Chunk optimization（块优化）、Multi-representation indexing、Specialized Embeddings（特殊嵌入）和Hierachical Indexing（多级索引）这四种优化方案。2. Query TranslationQuery Translation主要处理用户的输入。在初始的RAG系统中，往往直接使用原始query进行检索，可能会存在三个问题：第一，原始query的措辞不当，尤其是涉及到很多专业词汇时，query可能存在概念使用错误的问题；第二，往往知识库内的数据无法直接回答，需要组合知识才能找到答案；第三，当query涉及比较多的细节时，由于检索效率有限，大模型往往无法进行高质量的回答。在这个框架中，我们将在这个环节实现Multi-query（多查询）、Rag-Fusion、Decomposition（查询分解）、Stepback和HYDE这五种优化方案3. Routing（路由）路由的作用，是为每个Query选择最合适的处理管道，以及依据来自模型的输入或补充的元数据，来确定将启用哪些模块。比如在索引环节引入多重索引技术后，就需要使用多级路由机制，根据Query引导至最合适的父级索引。在路由环节，我们将实现Logical routing（基于逻辑的路由）和Sematic Routing（基于语义的路由）两种方案。4. Query Construction（查询构建）查询构建主要是为了将自然语言的Query，转化为某种特定机器或软件能理解的语言。因为随着大模型在各行各业的渗透，除文本数据外，诸如表格和图形数据等越来越多的结构化数据正被融入 RAG 系统。比如在一些ChatBI的场景下，就需要将用户的Query内容，转化为SQL语句，进行数据库查询，这就是Text-to-SQL。再比如工业设计场景下，可能需要将用户的Query转化为设计指令，或者设备控制指令，这就是Text-to-Cypher。在查询构建环节，我们将实现Text-to-SQL、Text-to-Cypher和Self-Query（让大模型自行构建Query）三种优化方案。5. Retrieval（检索）在检索的时候，用户的问题会被输入到嵌入模型中进行向量化处理，然后系统会在向量数据库中搜索与该问题向量语义上相似的知识文本或历史对话记录并返回。在朴素RAG中，系统会将所有检索到的块直接输入到 LLM生成回答，导致出现中间内容丢失、噪声占比过高、上下文长度限制等问题。在检索环节，我们将实现Reranking（重排序）、Refinement（压缩）、Corrective Rag（纠正性Rag）等方案。6. Generation（生成）在生成环节，可能会出现以下问题：第一，当系统忽略了以特定格式（例如表格或列表）提取信息的指令时，输出可能会出现格式错误；第二，输出错误或者输出不完整，比如对于一些比较类问题的处理往往不尽人意，以及可能出现的幻觉问题；第三，可能会输出一些不太符合人类/社会偏好，政治不正确的回答在生成环节，我们将重点介绍Self-Rag方案。要覆盖所有上面提到的优化环节，需要较长的内容篇幅，因此风叔会分成几篇文章来写。接下来，我们先从整体上，看看一个最小化的RAG系统是如何实现的。二、构建最小化的Naive Rag系统RAG发展初期，其核心框架由索引、检索和生成构成，这种范式被称作Naive RAG。Naive Rag的原理非常简单，包括以下三个步骤：索引：这一过程通常在离线状态下进行，将原始文档或数据进行清洗并分块，然后将分块后的知识通过embedding模型生成语义向量，并创建索引。检索：对用户输入的Query问题，使用相同的embedding模型，计算Query嵌入和文档块嵌入之间的向量相似度，然后选择相似度最高的前N个文档块作为当前问题的增强上下文信息。生成：将原始Query和相关文档合并为新的提示，然后由大型语言模型基于提供的信息回答问题。如果有历史对话信息，也可以合并到提示中，用于进行多轮对话。下面，风叔通过实际的源码，详细介绍如何构建一个最小化的Naive Rag系统。","source":"web","publishedAt":"2024-09-27T23:12:00+08:00"},{"id":"bocha-4","title":"RAG实战篇：构建一个最小可行性的Rag系统","url":"https://view.inews.qq.com/k/20240927A05IBM00?openApp=false&web_channel=wap","snippet":"RAG实战篇：构建一个最小可行性的Rag系统人人都是产品经理2024-09-27 15:12发布于广东人人都是产品经理的官方账号在人工智能的世界里，RAG（Retrieval-Augmented Generation）技术正成为提升AI理解和生成能力的关键。本文将带你进入RAG系统的实战篇，从构建一个最小可行性的RAG系统开始，详细介绍如何将这一技术应用于实际场景中。在《AI大模型实战篇》系列文章中，风叔通过八篇文章，从最经典的ReAct模式开始，沿着规划路线介绍了REWOO、Plan&Execute和LLM Compiler，沿着反思路线介绍了Basic Reflection、Self Discover和Reflexion，并以最强大的设计模式LATS作为收尾。但是，所有的这些设计模式，都只是在告诉AI Agent应该如何规划和思考，且只能依赖于大模型既有的知识储备。而实际应用中，我们往往更希望AI Agent结合我们给定的知识和信息，在更专业的垂直领域内进行规划和思考。比如我们希望Agent帮我们做论文分析、书籍总结，或者在企业级场景中，让AI Agent写营销计划、内部知识问答、智能客服等等非常多的场景，只靠上面几种Agent设计模式是远远不够的，我们必须给大模型外挂知识库，并且通过工作流进一步约束和规范Agent的思考方向和行为模式。解决这个问题的最佳方式是利用RAG技术，接下来我们正式开启《RAG实战篇》系列。对于RAG还不太熟悉的朋友，可以先参考下面两篇文章：一、RAG系统实现方案概览我们将基于下图所示的框架，来构建一个完整的RAG系统。1. Indexing（索引）Indexing是任何RAG系统的第一步，在实际应用场景中，文档尺寸可能非常大，因此需要将长篇文档分割成多个文本块，以便更高效地处理和检索信息。Indexing环节主要面临三个难题：首先，内容表述不完整，内容块的语义信息受分割方式影响，致使在较长的语境中，重要信息被丢失或被掩盖。其次，块相似性搜索不准确，随着数据量增多，检索中的噪声增大，导致频繁与错误数据匹配，使得检索系统脆弱且不可靠。最后，参考轨迹不明晰，检索到的内容块可能来自任何文档，没有引用痕迹，可能出现来自多个不同文档的块，尽管语义相似，但包含的却是完全不同主题的内容。在这个框架中，我们将在索引环节实现Chunk optimization（块优化）、Multi-representation indexing、Specialized Embeddings（特殊嵌入）和Hierachical Indexing（多级索引）这四种优化方案。2. Query TranslationQuery Translation主要处理用户的输入。在初始的RAG系统中，往往直接使用原始query进行检索，可能会存在三个问题：第一，原始query的措辞不当，尤其是涉及到很多专业词汇时，query可能存在概念使用错误的问题；第二，往往知识库内的数据无法直接回答，需要组合知识才能找到答案；第三，当query涉及比较多的细节时，由于检索效率有限，大模型往往无法进行高质量的回答。在这个框架中，我们将在这个环节实现Multi-query（多查询）、Rag-Fusion、Decomposition（查询分解）、Stepback和HYDE这五种优化方案3. Routing（路由）路由的作用，是为每个Query选择最合适的处理管道，以及依据来自模型的输入或补充的元数据，来确定将启用哪些模块。比如在索引环节引入多重索引技术后，就需要使用多级路由机制，根据Query引导至最合适的父级索引。在路由环节，我们将实现Logical routing（基于逻辑的路由）和Sematic Routing（基于语义的路由）两种方案。4. Query Construction（查询构建）查询构建主要是为了将自然语言的Query，转化为某种特定机器或软件能理解的语言。因为随着大模型在各行各业的渗透，除文本数据外，诸如表格和图形数据等越来越多的结构化数据正被融入 RAG 系统。比如在一些ChatBI的场景下，就需要将用户的Query内容，转化为SQL语句，进行数据库查询，这就是Text-to-SQL。再比如工业设计场景下，可能需要将用户的Query转化为设计指令，或者设备控制指令，这就是Text-to-Cypher。在查询构建环节，我们将实现Text-to-SQL、Text-to-Cypher和Self-Query（让大模型自行构建Query）三种优化方案。5. Retrieval（检索）在检索的时候，用户的问题会被输入到嵌入模型中进行向量化处理，然后系统会在向量数据库中搜索与该问题向量语义上相似的知识文本或历史对话记录并返回。在朴素RAG中，系统会将所有检索到的块直接输入到 LLM生成回答，导致出现中间内容丢失、噪声占比过高、上下文长度限制等问题。在检索环节，我们将实现Reranking（重排序）、Refinement（压缩）、Corrective Rag（纠正性Rag）等方案。6. Generation（生成）在生成环节，可能会出现以下问题：第一，当系统忽略了以特定格式（例如表格或列表）提取信息的指令时，输出可能会出现格式错误；第二，输出错误或者输出不完整，比如对于一些比较类问题的处理往往不尽人意，以及可能出现的幻觉问题；第三，可能会输出一些不太符合人类/社会偏好，政治不正确的回答在生成环节，我们将重点介绍Self-Rag方案。要覆盖所有上面提到的优化环节，需要较长的内容篇幅，因此风叔会分成几篇文章来写。接下来，我们先从整体上，看看一个最小化的RAG系统是如何实现的。二、构建最小化的Naive Rag系统RAG发展初期，其核心框架由索引、检索和生成构成，这种范式被称作Naive RAG。Naive Rag的原理非常简单，包括以下三个步骤：索引：这一过程通常在离线状态下进行，将原始文档或数据进行清洗并分块，然后将分块后的知识通过embedding模型生成语义向量，并创建索引。检索：对用户输入的Query问题，使用相同的embedding模型，计算Query嵌入和文档块嵌入之间的向量相似度，然后选择相似度最高的前N个文档块作为当前问题的增强上下文信息。生成：将原始Query和相关文档合并为新的提示，然后由大型语言模型基于提供的信息回答问题。如果有历史对话信息，也可以合并到提示中，用于进行多轮对话。下面，风叔通过实际的源码，详细介绍如何构建一个最小化的Naive Rag系统。","source":"web","publishedAt":"2024-09-27T23:12:00+08:00"},{"id":"bocha-5","title":"RAG实战篇:构建一个最小可行性的Rag系统 | 人人都是产品经理","url":"https://www.woshipm.com/ai/6121166.html","snippet":"在人工智能的世界里,RAG(Retrieval-Augmented\nGeneration)技术正成为提升AI理解和生成能力的关键。本文将带你进入RAG系统的实战篇,从构建一个最小可行性的RAG系统开始","source":"web","publishedAt":"2024-09-27T00:00:00+08:00"},{"id":"bocha-6","title":"RAG实战篇:构建一个最小可行性的Rag系统 - 今日头条","url":"https://www.toutiao.com/article/7419216889378767395","snippet":"2024-09-27 15:10 · 人人都是产品经理 在人工智能的世界里,RAG(Retrieval-Augmented Generation)技术正成为提升AI理解和生成能力的关键。本","source":"web","publishedAt":"2024-09-27T15:10:00+08:00"},{"id":"bocha-7","title":"构建一个完全本地的语音激活的实用RAG系统_import_torch_numpy","url":"https://www.sohu.com/a/863013108_185201","snippet":"本文将探讨如何构建一个RAG系统并使其完全由语音激活。RAG（检索增强生成）是一种将外部知识用于额外上下文以馈入到大语言模型（LLM），从而提高模型准确性和相关性的技术。这是一种比不断微调模型可靠得多的方法，可以改善生成式AI的结果。传统上，RAG系统依赖用户文本查询来搜索矢量数据库。然后将检索到的相关文档用作生成式AI的上下文输入，生成式AI负责生成文本格式的结果。然而，我们可以进一步扩展RAG系统，以便能够接受和生成语音形式的输出。本文将探讨如何构建一个RAG系统并使其完全由语音激活。构建一个完全由语音激活的RAG系统我在本文中假设读者对LLM和RAG系统已有一定的了解，因此不会进一步解释它们。要构建具有完整语音功能的RAG系统，我们将围绕三个关键组件来构建它：语音接收器和转录知识库音频文件响应生成总的来说，项目工作流程如下图所示：如果你已准备好，不妨开始准备这个项目成功所需要的一切。首先，我们不会在这个项目中使用Notebook IDE，因为我们希望RAG系统像生产系统一样工作。因此，应该准备一个标准的编程语言IDE，比如Visual Studio Code（VS Code）。接下来，我们还想为项目创建一个虚拟环境。你可以使用任何方法，比如Python或Conda。复制python -m venv rag-env-audio准备好虚拟环境后，我们安装本教程所需的所有库。复制pip install openai-whisper chromadb sentence-transformers sounddevice numpy scipy PyPDF2 transformers torch langchain-core langchain-community如果你可以访问GPU，也可以下载PyTorch库的GPU版本。复制pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118一切准备就绪后，我们将开始构建一个语音激活的RAG系统。要注意的是，包含所有代码和数据集的项目存储库位于该存储库中：https://github.com/CornelliusYW/RAG-To-Know/tree/main/RAG-Project/RAG-Voice-Activated。我们首先使用以下代码导入所有必要的库和环境变量。复制import osimport whisperimport chromadbfrom sentence_transformers import SentenceTransformerimport sounddevice as sdimport numpy as npfrom scipy.io.wavfile import writefrom sklearn.metrics.pairwise import cosine_similarityfrom transformers import AutoModelForCausalLM, AutoTokenizerfrom langchain_text_splitters import RecursiveCharacterTextSplitter import torchAUDIO_FILE = \"user_input.wav\"RESPONSE_AUDIO_FILE = \"response.wav\" PDF_FILE = \"Insurance_Handbook_20103.pdf\" SAMPLE_RATE = 16000WAKE_WORD = \"Hi\" SIMILARITY_THRESHOLD = 0.4 MAX_ATTEMPTS = 5将对各自代码中使用的所有变量进行解释。现在，暂且保持原样。在导入所有必要的库之后，我们将为RAG系统设置所有必要的函数。我将逐个分析，这样你就能理解我们的项目中发生了什么。第一步是创建一项功能来记录输入语音，并将语音转录成文本数据。我们将使用声音设备库用于记录语音，使用OpenAI Whisper用于音频转录。复制# For recording audio input.def record_audio(filename, duration=5, samplerate=SAMPLE_RATE):print(\"Listening... Speak now!\")audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='float32')sd.wait() print(\"Recording finished.\")write(filename, samplerate, (audio * 32767).astype(np.int16))# Transcribe the Input audio into text def transcribe_audio(filename):print(\"Transcribing audio...\")model = whisper.load_model(\"base.en\")result = model.transcribe(filename)return result[\"text\"].strip().lower()上述函数将成为接受和返回作为文本数据的语音的基础。我们将在这个项目中多次使用它们，所以请牢记这一点。我们将为RAG系统创建一个入口功能，准备好接受音频的功能。在下一段代码中，我们在使用WAKE_WORD（唤醒词）访问系统之前创建一个语音激活函数。这个唤醒词可以是任何内容，你可以根据需要进行设置。上述语音激活背后的想法是，如果我们录制的转录语音与唤醒词匹配，RAG系统就会被激活。然而，如果转录需要完全匹配唤醒词，这将是不可行的，因为转录系统很有可能生成不同格式的文本结果。为此我们可以使转录输出实现标准化。然而我想使用嵌入相似度，这样即使唤醒词的组成略有不同，系统仍然会被激活。复制# Detecting Wake Word to activate the RAG Systemdef detect_wake_word(max_attempts=MAX_ATTEMPTS):print(\"Waiting for wake word...\")text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')wake_word_embedding = text_embedding_model.encode(WAKE_WORD).reshape(1, -1)attempts = 0while attempts = SIMILARITY_THRESHOLD:print(f\"Wake word detected: {WAKE_WORD}\")return Trueattempts += 1print(f\"Attempt {attempts}/{max_attempts}. Please try again.\")print(\"Wake word not detected. Exiting.\")return False通过结合WAKE_WORD和SIMILARITY_THRESHOLD变量，我们将最终获得语音激活功能。接下来，不妨使用PDF文件构建知识库。为此，我们将准备一个函数，用于从该文件中提取文本并将其分割成块。复制def load_and_chunk_pdf(pdf_file):from PyPDF2 import PdfReaderprint(\"Loading and chunking PDF...\")reader = PdfReader(pdf_file)all_text = \"\"for page in reader.pages:text = page.extract_text()if text:all_text += text + \"\\n\"# Split the text into chunkstext_splitter = RecursiveCharacterTextSplitter(chunk_size=250, # Size of each chunkchunk_overlap=50, # Overlap between chunks to maintain contextseparators=[\"\\n\\n\", \"\\n\", \" \", \"\"] chunks = text_splitter.split_text(all_text)return chunks你可以将块大小替换成你想要的。没有使用确切的数字，所以用它们进行试验，看看哪个是最好的参数。然后来自上述函数的块被传递到矢量数据库中。我们将使用ChromaDB矢量数据库和SenteceTransformer来访问嵌入模型。复制def setup_chromadb(chunks):print(\"Setting up ChromaDB...\")client = chromadb.PersistentClient(path=\"chroma_db\")text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')# Delete existing collection (if needed)try:client.delete_collection(name=\"knowledge_base\")print(\"Deleted existing collection: knowledge_base\")except Exception as e:print(f\"Collection does not exist or could not be deleted: {e}\")collection = client.create_collection(name=\"knowledge_base\")for i, chunk in enumerate(chunks):embedding = text_embedding_model.encode(chunk).tolist()collection.add(ids=[f\"chunk_{i}\"],embeddings=[embedding],metadatas=[{\"source\": \"pdf\", \"chunk_id\": i}],documents=[chunk]print(\"Text chunks and embeddings stored in ChromaDB.\")return collectionAdditionally, we will prepare the function for retrieval with the text query to ChromaDB as wellldef query_chromadb(collection, query, top_k=3):\"\"\"Query ChromaDB for relevant chunks.\"\"\"text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')query_embedding = text_embedding_model.encode(query).tolist()results = collection.query(query_embeddings=[query_embedding],n_results=top_krelevant_chunks = [chunk for sublist in results[\"documents\"] for chunk in sublist]return relevant_chunks然后，我们需要准备生成功能来完成RAG系统。在本例中，我将使用托管在HuggingFace中的Qwen -1.5-0.5B-Chat模型。你可以根据需要调整提示和生成模型。复制def generate_response(query, context_chunks):device = \"cuda\" if torch.cuda.is_available() else \"cpu\"model_name = \"Qwen/Qwen1.5-0.5B-Chat\"model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=\"auto\",device_map=\"auto\"tokenizer = AutoTokenizer.from_pretrained(model_name)# Format the prompt with the query and contextcontext = \"\\n\".join(context_chunks)messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": f\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}]text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=Truemodel_inputs = tokenizer([text],return_tensors=\"pt\",padding=True,truncation=True).to(device)# Generate the responsegenerated_ids = model.generate(model_inputs.input_ids,attention_mask=model_inputs.attention_mask,max_new_tokens=512,pad_token_id=tokenizer.eos_token_idgenerated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]return response最后，令人兴奋的地方在于使用文本到语音模型将生成的响应转换成音频文件。就本例而言，我们将使用托管在HuggingFace中的Suno Bark模型。在生成音频之后，我们将播放音频响应以完成整条管道。复制def text_to_speech(text, output_file):from transformers import AutoProcessor, BarkModelprint(\"Generating speech...\")processor = AutoProcessor.from_pretrained(\"suno/bark-small\")model = BarkModel.from_pretrained(\"suno/bark-small\")inputs = processor(text, return_tensors=\"pt\")audio_array = model.generate(**inputs)audio = audio_array.cpu().numpy().squeeze()# Save the audio to a filewrite(output_file, 22050, (audio * 32767).astype(np.int16))print(f\"Audio response saved to {output_file}\")return audiodef play_audio(audio, samplerate=22050):print(\"Playing response...\")sd.play(audio, samplerate=samplerate)sd.wait()这就是完成完全由语音激活的RAG管道需要的所有功能。不妨把它们结合在一起，形成连贯有序的结构。复制def main():# Step 1: Load and chunk the PDFchunks = load_and_chunk_pdf(PDF_FILE)# Step 2: Set up ChromaDBcollection = setup_chromadb(chunks)# Step 3: Detect wake word with embedding similarityif not detect_wake_word():return # Exit if wake word is not detected# Step 4: Record and transcribe user inputrecord_audio(AUDIO_FILE, duration=5) user_input = transcribe_audio(AUDIO_FILE)print(f\"User Input: {user_input}\")# Step 5: Query ChromaDB for relevant chunksrelevant_chunks = query_chromadb(collection, user_input)print(f\"Relevant Chunks: {relevant_chunks}\")# Step 6: Generate response using a Hugging Face modelresponse = generate_response(user_input, relevant_chunks)print(f\"Generated Response: {response}\")# Step 7: Convert response to speech, save it, and play itaudio = text_to_speech(response, RESPONSE_AUDIO_FILE)play_audio(audio)# Clean upos.remove(AUDIO_FILE) # Delete the temporary audio fileif __name__ == \"__main__\":main()我已将整个代码保存在一个名为app.py的脚本中，我们可以使用以下代码激活系统。复制python app.py自己尝试一下，你将获得可用于审阅的响应音频文件。这就是构建带有语音激活的本地RAG系统所需的全部内容。你可以为系统构建一个应用程序并将其部署到生产环境中，进一步完善项目。结论构建具有语音激活的RAG系统涉及一系列先进的技术以及协同工作如同一个模型的多个模型。本项目利用检索和生成函数来构建RAG系统，通过几个步骤嵌入音频功能，增添另一层。我们打下基础后，就可以根据需求进一步完善项目。返回搜狐，查看更多","source":"web","publishedAt":"2025-02-24T14:34:00+08:00"},{"id":"bocha-8","title":"构建一个完全本地的语音激活的实用RAG系统","url":"https://m.sohu.com/a/863013108_185201/?pvid=000115_3w_a","snippet":"本文将探讨如何构建一个RAG系统并使其完全由语音激活。 RAG（检索增强生成）是一种将外部知识用于额外上下文以馈入到大语言模型（LLM），从而提高模型准确性和相关性的技术。这是一种比不断微调模型可靠得多的方法，可以改善生成式AI的结果。 传统上，RAG系统依赖用户文本查询来搜索矢量数据库。然后将检索到的相关文档用作生成式AI的上下文输入，生成式AI负责生成文本格式的结果。然而，我们可以进一步扩展RAG系统，以便能够接受和生成语音形式的输出。 本文将探讨如何构建一个RAG系统并使其完全由语音激活。 构建一个完全由语音激活的RAG系统 我在本文中假设读者对LLM和RAG系统已有一定的了解，因此不会进一步解释它们。 要构建具有完整语音功能的RAG系统，我们将围绕三个关键组件来构建它： 语音接收器和转录 知识库 音频文件响应生成 总的来说，项目工作流程如下图所示： 展开剩余 95 % 如果你已准备好，不妨开始准备这个项目成功所需要的一切。 首先，我们不会在这个项目中使用Notebook IDE，因为我们希望RAG系统像生产系统一样工作。因此，应该准备一个标准的编程语言IDE，比如Visual Studio Code（VS Code）。 接下来，我们还想为项目创建一个虚拟环境。你可以使用任何方法，比如Python或Conda。 复制 python -m venv rag-env-audio 准备好虚拟环境后，我们安装本教程所需的所有库。 复制 pip install openai-whisper chromadb sentence-transformers sounddevice numpy scipy PyPDF2 transformers torch langchain-core langchain-community 如果你可以访问GPU，也可以下载PyTorch库的GPU版本。 复制 pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118 一切准备就绪后，我们将开始构建一个语音激活的RAG系统。要注意的是，包含所有代码和数据集的项目存储库位于该存储库中：https://github.com/CornelliusYW/RAG-To-Know/tree/main/RAG-Project/RAG-Voice-Activated。 我们首先使用以下代码导入所有必要的库和环境变量。 复制 import os import whisper import chromadb from sentence_transformers import SentenceTransformer import sounddevice as sd import numpy as np from scipy.io.wavfile import write from sklearn.metrics.pairwise import cosine_similarity from transformers import AutoModelForCausalLM, AutoTokenizer from langchain_text_splitters import RecursiveCharacterTextSplitter import torch AUDIO_FILE = \"user_input.wav\" RESPONSE_AUDIO_FILE = \"response.wav\" PDF_FILE = \"Insurance_Handbook_20103.pdf\" SAMPLE_RATE = 16000 WAKE_WORD = \"Hi\" SIMILARITY_THRESHOLD = 0.4 MAX_ATTEMPTS = 5 将对各自代码中使用的所有变量进行解释。现在，暂且保持原样。 在导入所有必要的库之后，我们将为RAG系统设置所有必要的函数。我将逐个分析，这样你就能理解我们的项目中发生了什么。 第一步是创建一项功能来记录输入语音，并将语音转录成文本数据。我们将使用声音设备库用于记录语音，使用OpenAI Whisper用于音频转录。 复制 # For recording audio input. def record_audio(filename, duration=5, samplerate=SAMPLE_RATE): print(\"Listening... Speak now!\") audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='float32') sd.wait() print(\"Recording finished.\") write(filename, samplerate, (audio * 32767).astype(np.int16)) # Transcribe the Input audio into text def transcribe_audio(filename): print(\"Transcribing audio...\") model = whisper.load_model(\"base.en\") result = model.transcribe(filename) return result[\"text\"].strip().lower() 上述函数将成为接受和返回作为文本数据的语音的基础。我们将在这个项目中多次使用它们，所以请牢记这一点。 我们将为RAG系统创建一个入口功能，准备好接受音频的功能。在下一段代码中，我们在使用WAKE_WORD（唤醒词）访问系统之前创建一个语音激活函数。这个唤醒词可以是任何内容，你可以根据需要进行设置。 上述语音激活背后的想法是，如果我们录制的转录语音与唤醒词匹配，RAG系统就会被激活。然而，如果转录需要完全匹配唤醒词，这将是不可行的，因为转录系统很有可能生成不同格式的文本结果。为此我们可以使转录输出实现标准化。然而我想使用嵌入相似度，这样即使唤醒词的组成略有不同，系统仍然会被激活。 复制 # Detecting Wake Word to activate the RAG System def detect_wake_word(max_attempts=MAX_ATTEMPTS): print(\"Waiting for wake word...\") text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2') wake_word_embedding = text_embedding_model.encode(WAKE_WORD).reshape(1, -1) attempts = 0 while attempts = SIMILARITY_THRESHOLD: print(f\"Wake word detected: {WAKE_WORD}\") return True attempts += 1 print(f\"Attempt {attempts}/{max_attempts}. Please try again.\") print(\"Wake word not detected. Exiting.\") return False 通过结合WAKE_WORD和SIMILARITY_THRESHOLD变量，我们将最终获得语音激活功能。 接下来，不妨使用PDF文件构建知识库。为此，我们将准备一个函数，用于从该文件中提取文本并将其分割成块。 复制 def load_and_chunk_pdf(pdf_file): from PyPDF2 import PdfReader print(\"Loading and chunking PDF...\") reader = PdfReader(pdf_file) all_text = \"\" for page in reader.pages: text = page.extract_text() if text: all_text += text + \"\\n\" # Split the text into chunks text_splitter = RecursiveCharacterTextSplitter( chunk_size=250, # Size of each chunk chunk_overlap=50, # Overlap between chunks to maintain context separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] chunks = text_splitter.split_text(all_text) return chunks 你可以将块大小替换成你想要的。没有使用确切的数字，所以用它们进行试验，看看哪个是最好的参数。 然后来自上述函数的块被传递到矢量数据库中。我们将使用ChromaDB矢量数据库和SenteceTransformer来访问嵌入模型。 复制 def setup_chromadb(chunks): print(\"Setting up ChromaDB...\") client = chromadb.PersistentClient(path=\"chroma_db\") text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2') # Delete existing collection (if needed) try: client.delete_collection(name=\"knowledge_base\") print(\"Deleted existing collection: knowledge_base\") except Exception as e: print(f\"Collection does not exist or could not be deleted: {e}\") collection = client.create_collection(name=\"knowledge_base\") for i, chunk in enumerate(chunks): embedding = text_embedding_model.encode(chunk).tolist() collection.add( ids=[f\"chunk_{i}\"], embeddings=[embedding], metadatas=[{\"source\": \"pdf\", \"chunk_id\": i}], documents=[chunk] print(\"Text chunks and embeddings stored in ChromaDB.\") return collection Additionally, we will prepare the function for retrieval with the text query to ChromaDB as welll def query_chromadb(collection, query, top_k=3): \"\"\"Query ChromaDB for relevant chunks.\"\"\" text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2') query_embedding = text_embedding_model.encode(query).tolist() results = collection.query( query_embeddings=[query_embedding], n_results=top_k relevant_chunks = [chunk for sublist in results[\"documents\"] for chunk in sublist] return relevant_chunks 然后，我们需要准备生成功能来完成RAG系统。在本例中，我将使用托管在HuggingFace中的Qwen -1.5-0.5B-Chat模型。你可以根据需要调整提示和生成模型。 复制 def generate_response(query, context_chunks): device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model_name = \"Qwen/Qwen1.5-0.5B-Chat\" model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" tokenizer = AutoTokenizer.from_pretrained(model_name) # Format the prompt with the query and context context = \"\\n\".join(context_chunks) messages = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": f\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True model_inputs = tokenizer( [text], return_tensors=\"pt\", padding=True, truncation=True ).to(device) # Generate the response generated_ids = model.generate( model_inputs.input_ids, attention_mask=model_inputs.attention_mask, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] return response 最后，令人兴奋的地方在于使用文本到语音模型将生成的响应转换成音频文件。就本例而言，我们将使用托管在HuggingFace中的Suno Bark模型。在生成音频之后，我们将播放音频响应以完成整条管道。 复制 def text_to_speech(text, output_file): from transformers import AutoProcessor, BarkModel print(\"Generating speech...\") processor = AutoProcessor.from_pretrained(\"suno/bark-small\") model = BarkModel.from_pretrained(\"suno/bark-small\") inputs = processor(text, return_tensors=\"pt\") audio_array = model.generate(**inputs) audio = audio_array.cpu().numpy().squeeze() # Save the audio to a file write(output_file, 22050, (audio * 32767).astype(np.int16)) print(f\"Audio response saved to {output_file}\") return audio def play_audio(audio, samplerate=22050): print(\"Playing response...\") sd.play(audio, samplerate=samplerate) sd.wait() 这就是完成完全由语音激活的RAG管道需要的所有功能。不妨把它们结合在一起，形成连贯有序的结构。 复制 def main(): # Step 1: Load and chunk the PDF chunks = load_and_chunk_pdf(PDF_FILE) # Step 2: Set up ChromaDB collection = setup_chromadb(chunks) # Step 3: Detect wake word with embedding similarity if not detect_wake_word(): return # Exit if wake word is not detected # Step 4: Record and transcribe user input record_audio(AUDIO_FILE, duration=5) user_input = transcribe_audio(AUDIO_FILE) print(f\"User Input: {user_input}\") # Step 5: Query ChromaDB for relevant chunks relevant_chunks = query_chromadb(collection, user_input) print(f\"Relevant Chunks: {relevant_chunks}\") # Step 6: Generate response using a Hugging Face model response = generate_response(user_input, relevant_chunks) print(f\"Generated Response: {response}\") # Step 7: Convert response to speech, save it, and play it audio = text_to_speech(response, RESPONSE_AUDIO_FILE) play_audio(audio) # Clean up os.remove(AUDIO_FILE) # Delete the temporary audio file if __name__ == \"__main__\": main() 我已将整个代码保存在一个名为app.py的脚本中，我们可以使用以下代码激活系统。 复制 python app.py 自己尝试一下，你将获得可用于审阅的响应音频文件。 这就是构建带有语音激活的本地RAG系统所需的全部内容。你可以为系统构建一个应用程序并将其部署到生产环境中，进一步完善项目。 结论 构建具有语音激活的RAG系统涉及一系列先进的技术以及协同工作如同一个模型的多个模型。本项目利用检索和生成函数来构建RAG系统，通过几个步骤嵌入音频功能，增添另一层。我们打下基础后，就可以根据需求进一步完善项目。","source":"web","publishedAt":"2025-02-24T22:34:00+08:00"},{"id":"onebound-0","title":"【推荐系统】推荐系统领域最新研究进展","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=x5*2eWMAwAW6hdMZde8XRsMkybvCwEb1ep5lL9BDH0sXtAQOQYEmNabJH6kAI7s6LNKou9lI3R2wI0pIVz33CjZvDjQXcG2e9PMsdFX8lt*6**odGVdrEt9NF6R7vf5w&new=1","snippet":" on three real-world LBSN datasets, demonstrating the effectiveness of our model against state-of-the-art methods.11. Implementation of ...","source":"wechat"},{"id":"onebound-1","title":"论文周报 | 推荐系统领域最新研究进展","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=NJvHgs3fuwx9YMdu-zpz*V4maBmZNm5kFq2GCWpMLC65760SjClero4UqDCmRiUFgRDOoaLdsDe7ZTxoJrEk*hppRVShyJpuhoJFudzP1exLrbzSJ3AbT3amTgfATIlm&new=1","snippet":"real-world and semi-synthetic datasets, which demonstrates the ... system feeds a list of items to a user at a time in the result page, in ...","source":"wechat"},{"id":"onebound-2","title":"通过 Llama3 和 ChromaDB 实现精确的文档查询","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=RY*KWWK3XJfpHDa5pHcFtDGr-RueaWWrWuvRt*98O78sibTazoioCYSnW9s7i7SwvY2guujf5cCz5UraZQEiKq76NitCmkYkPHo-6*KPNB611aDtY9rWvSdDnwI8MDQ6&new=1","snippet":" system for enforcing the regulation, and for holding companies ... risk AI systems in real world conditions?＂test_rag(qa, query)输出-...","source":"wechat"},{"id":"onebound-3","title":"【评&middot;时】WAIC 2025腾讯云副总裁吴运声:把AI变成全新生产力","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=M2yaQ0AdtRJtAWr1uyrP8RG8fiC54EDAV771XsleAuoAX44itMpi6IRYRtOljb3x69wh8HWU9-ZVoCxthqN7xddDFqUrPN2sT8ENNBIEpiTjcg6MMmcJeMaoAXDujqbT&new=1","snippet":"In terms of RAG capabilities, the platform has been refined through numerous real-world enterprise scenarios and possesses solid, ...","source":"wechat"},{"id":"onebound-4","title":"爱可可 AI 前沿推介(4.16)","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=7whLjqiHyhuDTme2MnM-k0dpSNF4fRXggdMfnt2fR8zVm6OdSCay8y-QB34NSn1t3PWwrE83V8RQk0wuPgNPt7okLeaA8g5I2CDh8EayIBVgDUQLmZPmot6fe3nAEU5v&new=1","snippet":"real-world GenAI systems will likely continue to face challenges in ... Thanks to our implementation of RAG, our proposed system ...","source":"wechat"},{"id":"onebound-5","title":"《智能体设计模式:构建智能系统实战指南》学习笔记 &mdash;&mdash; introduction","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=B*0UwzGkqjiS2zhMPUElcGgb4sG1dqDJLiGBFINdsG0q8nO8KUhuy-3Mf-WCobnCrXvRxEqgtQ0dhm2Kr6Tko9tmjG3xqkTqT2Aw*7JedoI00hRY72t6p1XoJThPUbCP&new=1","snippet":"implementation using prominent agent development frameworks. ...  in real-world environments, and an overview of essential agentic ...","source":"wechat"},{"id":"onebound-6","title":"【会议前瞻】High-Performance Computer Architecture (HPCA) 参会预告","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=MLqm35-uuXIBVQO7R5tNzh*G680EHhSSql*Py2GU17zRW9nhns4d8aEMJ78H-8zIMHWI*Zimn2UcOlWNY0VXV95GmuWP4LWFQ9GReNuWsdLNr7ufmRnit1olOklyebqn&new=1","snippet":"average speedup on real-world applications compared to the state-... system scale and compute capabilities, i.e., increasing number of ...","source":"wechat"},{"id":"onebound-7","title":"2025-09-24 论文精选 | 推荐系统最新进展","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=TUeiOE3An5j7ASrEdYj-wc3iSGezZrjmCmv5ocoMKjHLNdOUK5Qs20QLpdS7*hcAQrSp14DpUVEpY-Ca4R3aO0gPJXt4p-kMF3afpX1u0CgLg605wwmY38F2xv3LIxIB&new=1","snippet":"Implementation code is available at https://github.com/Applied-...  across three real-world Foursquare datasets, outperforming both ...","source":"wechat"},{"id":"onebound-8","title":"MOFs@Top J:2019.12.08-2019.12.14","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=ZIfODQUOkzwU66HX0Pk3aI8lJVckJ2CZOKZf6BNx94fpu9Rpi*5pDJDdKN2aQxOIyPt7MfuwoEMEWjhpYph**CFKtpkycuyVfwx5DPigPcB64e3Qi31IebGNmvh1*ty*&new=1","snippet":"in the membrane system, the AA stacked COF exhibits higher ... preventing real-world implementation. Herein we report a ...","source":"wechat"},{"id":"onebound-9","title":"工学 | SCI期刊专刊截稿信息8条","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=l4LrQ8H9BXv0YkF6rei-HMS7tAJezgXRvfaau2B*xgJvWkKap0KSE-unVQjvAA9haARvK2gvs26UDtcIf*f8iB8zo-rJP9pFyT-4yRF4JKwQ1pSFf1UxLzw4Va0CIDa9&new=1","snippet":" that can be applied in real-world scenarios. Additionally, there will ... System SafetyRUL prediction and system reliability of complex ...","source":"wechat"},{"id":"bocha-0","title":"AI学习指南RAG篇(14)-RAG企业级应用案例-CSDN博客","url":"https://blog.csdn.net/zhaopeng_yu/article/details/145931623","snippet":"AI学习指南RAG篇(14)-RAG企业级应用案例 最新推荐文章于 2025-11-07 11:24:15 发布 原创 最新推荐文章于 2025-11-07 11:24:15 发布 · 1k 阅读 · 5 · 3 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #ai AI学习指南 专栏收录该内容 416 篇文章 ¥49.90 ¥99.00 订阅专栏 超级会员免费看 文章目录 一、引言 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 1.2 实现过程 1.3 示例代码 2. 知识管理平台 2.1 案例背景 2.2 实现过程 3. 企业级RAG系统建设 3.1 案例背景 3.2 实现过程 三、总结 一、引言 RAG（Retrieval-Augmented Generation，检索增强生成）技术在企业级应用中展现出巨大的潜力和价值。通过结合检索和生成，RAG系统能够提供更准确、更相关的回答，满足企业在智能客服、知识管理等多个场景中的需求。本文将分享一些企业级RAG应用案例，包括智能客服系统和知识管理平台等。 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 某初创公司希望通过自动化FAQ系统减轻客服团队的工作负担，提升客户服务体验。他们选择了LangChain框架来搭建基于RAG的聊天机器人，该机器人可以在回答用户问题时搜索相关文档，并生成答案[88]。 1.2 实现过程 加载并分割文档：使用DirectoryLoader加载FAQ文档，并使用CharacterTextSplitter进行分割。 创建向量数据库：使用OpenAIEmbeddings生成文档嵌入，并存储在Chroma向量数据库中。 初始化LLM和QA链：使用ChatOpenAI模型作为LLM，并初始化ConversationalRetrievalChain。 开始对话：用户提出问题，系统检索相关文档并生成回答。 了解本专栏 订阅专栏 解锁全文 超级会员免费看 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 俞兆鹏 关注 关注 5 点赞 踩 3 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 举报 举报 专栏目录 订阅专栏 【工业实战】从架构到优化：企业级RAG客服对话系统的构建之道 kakaZhui的博客 10-02 137 检索增强生成（Retrieval-Augmented Generation, RAG）已成为构建智能客服对话系统的核心技术。然而，将RAG从概念验证推向企业级应用，会遇到搜索范围不精、回答内容冗余、顶层结果准确率不高等一系列严峻挑战。本文旨在从工程设计与算法优化的双重视角，系统性地剖析构建一个高性能、高可用的RAG客服对话服务的完整方案。 参与评论 您还未登录，请先 登录 后发表或查看评论 【实战分享】构建企业级RAG（Retrieval-Augmented Generation）知识库的全面实践 后端研发工程师Marion的博客 12-22 5018 大模型指的是训练参数量极其庞大的深度学习模型，如GPT-3、GPT-4等。这些模型能够通过海量的数据学习，具备强大的语言理解和生成能力。在问答系统中，大模型能够理解用户提出的问题，并生成相关的回答。\"\"\"添加文档向量\"\"\"pass\"\"\"相似度检索\"\"\"pass\"\"\"获取页面分段\"\"\"passMilvus是一个开源的向量数据库，专为高效的向量存储和检索设计。Milvus支持多种索引方式（如IVF、HNSW等），并提供高效的查询和检索功能。索引构建：Milvus通过创建索引加速查询速度。 AI 产品经理学习路线图！从入门到实战，学习清单大曝光（附教学视频）直接抄作业 m0_63171455的博客 07-27 1757 想转型 AI 产品经理，却被 “算法、模型、项目” 搞得一头雾水？这套AI 产品经理学习清单，用 10 大模块搭建完整知识体系，从基础认知到项目实操全覆盖！ 今天为你拆解核心内容，帮你高效规划学习路径，记得保存好了～ 收藏必学：从零开始使用RAG技术构建企业级安全智能客服系统 最新发布 2401_85373691的博客 11-07 908 文章介绍了如何使用RAG（检索增强生成）技术，在本地搭建安全、智能的客服系统。通过将企业文档分块、向量化存储到数据库，结合大模型生成精准回答。文章提供了两种实现方式：使用Dify可视化工具快速搭建，或通过LangChain等库原生开发。这种方法既保证了数据安全，又能利用企业私有数据提供更精准的智能客服服务，特别适合对数据安全有要求的企业。 企业级RAG全解析：实现精准、安全、高效智能客服，收藏这一篇就够了！！ 2401_85327249的博客 03-28 803 随着金融行业数字化转型的加速，银行需要高效处理海量非结构化数据（如合同、政策文件、客户咨询记录等），同时确保服务的安全性、合规性与智能化。基于RAG技术构建的企业级系统，能够将传统检索与生成式AI结合，为银行提供精准、安全的智能服务。 以下结合银行业务场景，详解其核心流程与技术实现。 企业RAG落地优秀案例拆解 姑苏 06-20 272 【摘要】IBM Watson XAI举办的Enterprise RAG Challenge第二赛季冠军方案解析：该方案通过系统化流程构建企业级RAG系统，处理100份千页年报PDF。关键创新包括：1) 采用GPU加速的Docling解析器配合定制化表格序列化技术；2) 分页分块策略结合FAISS向量数据库；3) 独创LLM重排序机制（权重向量搜索30%+LLM评分70%）；4) 基于问题类型的四路prompt路由与结构化输出验证；5) 精细化的指令工程与CoT推理设计。系统在2分钟内完成100问作答，最终得 一些RAG技术的实际应用案例 alankuo的专栏 09-05 1839 3. 金融报告撰写：金融行业的分析师使用 RAG 技术，从历史财务报表、市场研究报告、宏观经济指标等数据源中，根据报告主题或关键词检索相关数据，再结合检索到的数据和分析结果生成报告内容，提升了报告的质量和制作效率，有助于分析师更快地完成任务。6. 媒体与新闻行业：新闻机构可以利用 RAG 技术，根据特定的主题或事件，从大量的新闻报道、社交媒体内容、历史档案中检索相关信息，帮助记者快速了解背景知识，撰写更全面、深入的报道，或者为新闻推荐系统提供个性化的内容推荐。 RAG产品的核心功能原型及构成模块 主攻大数据 人工智能 物联网 安全 低空经济等方向。mtsc 、gtest特邀分享嘉宾 04-28 912 通过上述设计，RAG产品可显著提升生成内容的准确性与可信度。实际开发中需重点关注。三大核心问题，并根据场景需求定制混合检索策略与领域微调方案。 【产品小白】产品视角的RAG 蟹老板的博客 03-28 460 深入理解检索增强生成（Retrieval-Augmented Generation，简称RAG）技术对于开发智能、高效的产品至关重要。​RAG技术将信息检索与生成式大语言模型（Large Language Models，LLMs）相结合，旨在提升模型在处理知识密集型任务时的准确性和可靠性。​。 大模型系列——解读RAG 我相信...... 02-04 3611 RAG 是2023年最流行的基于 LLM 的应用系统架构。有许多产品几乎完全建立在 RAG 之上，覆盖了结合网络搜索引擎和 LLM 的问答服务，到成千上万个数据聊天的应用程序。很多人将RAG和Agent 作为大模型应用的两种主流架构，但什么是RAG呢？RAG又涉及了哪些具体的技术呢？1. 什么是RAGRAG即检索增强生成，为 LLM 提供了从某些数据源检索到的信息，并基于此修正生成的答案。RAG ... 精选资源 企业级RAG系统从入门到精通案例 11-23 企业级RAG系统是一种结合了检索增强生成（Retrieval-Augmented Generation）技术的智能问答系统。该技术通过引入外部知识库（如文档集合），使得语言模型在回答问题时能够引用相关的文档内容，从而提高问答的准确性... 人工智能RAG+Agent+小模型协同架构：构建可靠高效经济的企业级AI应用系统设计 10-28 三者构成分层协作架构：小模型作为“守门员”处理简单请求与路由，RAG提供基于私有知识库的精准问答，Agent则完成复杂任务的自主规划与操作执行，共同打造面向企业级应用的智能化解决方案。; 适合人群：AI开发者、... 人工智能基于腾讯云智能体开发平台的RAG与多智能体协同技术：企业级大模型应用快速落地解决方案 09-18 内容概要：本文介绍了腾讯云智能体开发平台在AI Agent技术创新方面的核心能力与企业级智能体快速落地的实践路径。平台通过RAG、Workflow和Multi-Agent三大技术框架的持续升级，支持知识库问答、复杂流程编排与多智能... RAG 系统评测实践详细版：Coze 及相关产品评测对比，以及下一代 RAG 技术 m0_59235245的博客 10-09 1785 上面提到了 RAG 的定义是结合信息检索和大模型生成能力，为了全面评估 RAG 系统的性能，我们需要分别考察其在信息检索和答案生成两个子任务上的表现:评测信息检索能力： 借鉴传统 IR(Information Retrieval) 系统的评估方法和指标，如平均倒数排序 (MRR)、精确率 (Precision)、召回率 (Recall) 等。大模型生成能力： 主要依赖于语言模型本身的性能，看答案的流畅度、连贯性、准确性等方面，可以采用人工评分、BLEU 等自动化指标，以及问答准确率等任务相关指标。 RAG系列：一文让你由浅到深搞懂RAG实现 ytt0523_com的博客 05-01 2177 RAG（检索增强生成）是一种结合检索与生成的技术，通过实时检索外部知识库中的信息，动态增强大语言模型（LLM）的生成能力，是用于解决幻觉问题、知识时效问题、超长文本问题等各种大模型本身制约或不足的必要技术。RAG核心流程包括：对用户问题进行改写、扩写和重构，让用户问题更利于检索；从外部知识库（如企业文档、行业数据库）中筛选与用户问题相关的片段‌，并将检索结果与原始问题整合为增强提示词，输入给LLM；‌LLM基于增强后的提示词，生成精准、可靠的答案。RAG具有以下优点：实时性。 AI产品经理必须知道的技术 之 RAG aolan123的博客 06-25 1432 在对用户问题进行Embedding前，对问题进行补充完善。避免用户问题太过简单、或者有明显错误。也可以考虑将用户问题，进行主题关键词抽取，或者使用知识图谱等进行初步的信息识别。这么做的目的是，避免用户的问题，信息太多太杂，导致检索出来的相关文档，与用户提问意图关联不大。也就是对用户问题进行简化。在检索文档时，可以增加一些过滤条件，例如指定章节、关键词包含、日期筛选、相似度阈值等，以使检索出来的内容更准确。对检索结果，也可考虑将相似度，与文档自身的权重进行综合加权。使提供给大模型的内容资料是最优的。 读懂RAG这一篇就够了，万字详述RAG的5步流程和12个优化策略 热门推荐 2401_82452722的博客 01-30 2万+ ©作者|帅气的桌子来源|神州问学RAG概述ChatGPT、GLM等生成式人工智能在文本生成、文本到图像生成等任务中表现出令人印象深刻的性能。但它们也存在固有局限性，包括产生幻觉、缺乏对生成文本的可解释性、专业领域知识理解差，以及对最新知识的了解有限。为了克服这些限制，提高模型的能力，有两种主要途径：一种是微调（Fine Tune）来更新模型，另一种是让他们能够与外部世界互动，以不同的形式和方式获取知识。微调固然效果好，可以让模型真正的“学会”一些私域知识。但是微调也会带来几个问题：首先，由于生成模型依赖于内 RAG是什么，RAG综述，一文让你由浅到深搞懂RAG实现！ qq_46094651的博客 05-14 1454 RAG（检索增强生成）是一种结合检索与生成的技术，通过实时检索外部知识库中的信息，动态增强大语言模型（LLM）的生成能力，是用于解决幻觉问题、知识时效问题、超长文本问题等各种大模型本身制约或不足的必要技术。 【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题？ 新亮笔记 02-06 1019 本文为译文，原文链接：https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system今天，我们将卷起袖子，深入研究构建企业级 RAG 系统的复杂世界。我们将揭示一些常见的挑战和误区，以及如何克服它们。我们还将分享一些最佳实践和技巧，让您的 RAG 系统更加强大、灵活和可靠。但这个博客不仅仅... Elastic-Agentic RAG：打造企业级AI应用的未来 资源摘要信息:\"Elastic-Agentic RAG构建之路探讨了RAG（Retrieval-Augmented Generation）技术的局限性及Agentic RAG在企业级应用中的优势。RAG是一种将检索和生成模型相结合的技术，但存在一些局限，如多源数据融合... 文章目录 一、引言 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 1.2 实现过程 1.3 示例代码 2. 知识管理平台 2.1 案例背景 2.2 实现过程 3. 企业级RAG系统建设 3.1 案例背景 3.2 实现过程 三、总结 一、引言 RAG（Retrieval-Augmented Generation，检索增强生成）技术在企业级应用中展现出巨大的潜力和价值。通过结合检索和生成，RAG系统能够提供更准确、更相关的回答，满足企业在智能客服、知识管理等多个场景中的需求。本文将分享一些企业级RAG应用案例，包括智能客服系统和知识管理平台等。 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 某初创公司希望通过自动化FAQ系统减轻客服团队的工作负担，提升客户服务体验。他们选择了LangChain框架来搭建基于RAG的聊天机器人，该机器人可以在回答用户问题时搜索相关文档，并生成答案[88]。 1.2 实现过程 加载并分割文档：使用DirectoryLoader加载FAQ文档，并使用CharacterTextSplitter进行分割。 创建向量数据库：使用OpenAIEmbeddings生成文档嵌入，并存储在Chroma向量数据库中。 初始化LLM和QA链：使用ChatOpenAI模型作为LLM，并初始化ConversationalRetrievalChain。 开始对话：用户提出问题，系统检索相关文档并生成回答。","source":"web","publishedAt":"2025-03-15T06:30:00+08:00"},{"id":"bocha-1","title":"AI学习指南RAG篇(14)-RAG企业级应用案例","url":"https://m.blog.csdn.net/zhaopeng_yu/article/details/145931623","snippet":"AI学习指南RAG篇(14)-RAG企业级应用案例 最新推荐文章于 2025-11-07 11:24:15 发布 原创 最新推荐文章于 2025-11-07 11:24:15 发布 · 1k 阅读 · 5 · 3 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #ai AI学习指南 专栏收录该内容 416 篇文章 ¥49.90 ¥99.00 订阅专栏 超级会员免费看 文章目录 一、引言 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 1.2 实现过程 1.3 示例代码 2. 知识管理平台 2.1 案例背景 2.2 实现过程 3. 企业级RAG系统建设 3.1 案例背景 3.2 实现过程 三、总结 一、引言 RAG（Retrieval-Augmented Generation，检索增强生成）技术在企业级应用中展现出巨大的潜力和价值。通过结合检索和生成，RAG系统能够提供更准确、更相关的回答，满足企业在智能客服、知识管理等多个场景中的需求。本文将分享一些企业级RAG应用案例，包括智能客服系统和知识管理平台等。 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 某初创公司希望通过自动化FAQ系统减轻客服团队的工作负担，提升客户服务体验。他们选择了LangChain框架来搭建基于RAG的聊天机器人，该机器人可以在回答用户问题时搜索相关文档，并生成答案[88]。 1.2 实现过程 加载并分割文档：使用DirectoryLoader加载FAQ文档，并使用CharacterTextSplitter进行分割。 创建向量数据库：使用OpenAIEmbeddings生成文档嵌入，并存储在Chroma向量数据库中。 初始化LLM和QA链：使用ChatOpenAI模型作为LLM，并初始化ConversationalRetrievalChain。 开始对话：用户提出问题，系统检索相关文档并生成回答。 了解本专栏 订阅专栏 解锁全文 超级会员免费看 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 俞兆鹏 关注 关注 5 点赞 踩 3 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 举报 举报 专栏目录 订阅专栏 【工业实战】从架构到优化：企业级RAG客服对话系统的构建之道 kakaZhui的博客 10-02 137 检索增强生成（Retrieval-Augmented Generation, RAG）已成为构建智能客服对话系统的核心技术。然而，将RAG从概念验证推向企业级应用，会遇到搜索范围不精、回答内容冗余、顶层结果准确率不高等一系列严峻挑战。本文旨在从工程设计与算法优化的双重视角，系统性地剖析构建一个高性能、高可用的RAG客服对话服务的完整方案。 参与评论 您还未登录，请先 登录 后发表或查看评论 【实战分享】构建企业级RAG（Retrieval-Augmented Generation）知识库的全面实践 后端研发工程师Marion的博客 12-22 5018 大模型指的是训练参数量极其庞大的深度学习模型，如GPT-3、GPT-4等。这些模型能够通过海量的数据学习，具备强大的语言理解和生成能力。在问答系统中，大模型能够理解用户提出的问题，并生成相关的回答。\"\"\"添加文档向量\"\"\"pass\"\"\"相似度检索\"\"\"pass\"\"\"获取页面分段\"\"\"passMilvus是一个开源的向量数据库，专为高效的向量存储和检索设计。Milvus支持多种索引方式（如IVF、HNSW等），并提供高效的查询和检索功能。索引构建：Milvus通过创建索引加速查询速度。 AI 产品经理学习路线图！从入门到实战，学习清单大曝光（附教学视频）直接抄作业 m0_63171455的博客 07-27 1757 想转型 AI 产品经理，却被 “算法、模型、项目” 搞得一头雾水？这套AI 产品经理学习清单，用 10 大模块搭建完整知识体系，从基础认知到项目实操全覆盖！ 今天为你拆解核心内容，帮你高效规划学习路径，记得保存好了～ 收藏必学：从零开始使用RAG技术构建企业级安全智能客服系统 最新发布 2401_85373691的博客 11-07 908 文章介绍了如何使用RAG（检索增强生成）技术，在本地搭建安全、智能的客服系统。通过将企业文档分块、向量化存储到数据库，结合大模型生成精准回答。文章提供了两种实现方式：使用Dify可视化工具快速搭建，或通过LangChain等库原生开发。这种方法既保证了数据安全，又能利用企业私有数据提供更精准的智能客服服务，特别适合对数据安全有要求的企业。 企业级RAG全解析：实现精准、安全、高效智能客服，收藏这一篇就够了！！ 2401_85327249的博客 03-28 803 随着金融行业数字化转型的加速，银行需要高效处理海量非结构化数据（如合同、政策文件、客户咨询记录等），同时确保服务的安全性、合规性与智能化。基于RAG技术构建的企业级系统，能够将传统检索与生成式AI结合，为银行提供精准、安全的智能服务。 以下结合银行业务场景，详解其核心流程与技术实现。 企业RAG落地优秀案例拆解 姑苏 06-20 272 【摘要】IBM Watson XAI举办的Enterprise RAG Challenge第二赛季冠军方案解析：该方案通过系统化流程构建企业级RAG系统，处理100份千页年报PDF。关键创新包括：1) 采用GPU加速的Docling解析器配合定制化表格序列化技术；2) 分页分块策略结合FAISS向量数据库；3) 独创LLM重排序机制（权重向量搜索30%+LLM评分70%）；4) 基于问题类型的四路prompt路由与结构化输出验证；5) 精细化的指令工程与CoT推理设计。系统在2分钟内完成100问作答，最终得 一些RAG技术的实际应用案例 alankuo的专栏 09-05 1839 3. 金融报告撰写：金融行业的分析师使用 RAG 技术，从历史财务报表、市场研究报告、宏观经济指标等数据源中，根据报告主题或关键词检索相关数据，再结合检索到的数据和分析结果生成报告内容，提升了报告的质量和制作效率，有助于分析师更快地完成任务。6. 媒体与新闻行业：新闻机构可以利用 RAG 技术，根据特定的主题或事件，从大量的新闻报道、社交媒体内容、历史档案中检索相关信息，帮助记者快速了解背景知识，撰写更全面、深入的报道，或者为新闻推荐系统提供个性化的内容推荐。 RAG产品的核心功能原型及构成模块 主攻大数据 人工智能 物联网 安全 低空经济等方向。mtsc 、gtest特邀分享嘉宾 04-28 912 通过上述设计，RAG产品可显著提升生成内容的准确性与可信度。实际开发中需重点关注。三大核心问题，并根据场景需求定制混合检索策略与领域微调方案。 【产品小白】产品视角的RAG 蟹老板的博客 03-28 460 深入理解检索增强生成（Retrieval-Augmented Generation，简称RAG）技术对于开发智能、高效的产品至关重要。​RAG技术将信息检索与生成式大语言模型（Large Language Models，LLMs）相结合，旨在提升模型在处理知识密集型任务时的准确性和可靠性。​。 大模型系列——解读RAG 我相信...... 02-04 3611 RAG 是2023年最流行的基于 LLM 的应用系统架构。有许多产品几乎完全建立在 RAG 之上，覆盖了结合网络搜索引擎和 LLM 的问答服务，到成千上万个数据聊天的应用程序。很多人将RAG和Agent 作为大模型应用的两种主流架构，但什么是RAG呢？RAG又涉及了哪些具体的技术呢？1. 什么是RAGRAG即检索增强生成，为 LLM 提供了从某些数据源检索到的信息，并基于此修正生成的答案。RAG ... 精选资源 企业级RAG系统从入门到精通案例 11-23 企业级RAG系统是一种结合了检索增强生成（Retrieval-Augmented Generation）技术的智能问答系统。该技术通过引入外部知识库（如文档集合），使得语言模型在回答问题时能够引用相关的文档内容，从而提高问答的准确性... 人工智能RAG+Agent+小模型协同架构：构建可靠高效经济的企业级AI应用系统设计 10-28 三者构成分层协作架构：小模型作为“守门员”处理简单请求与路由，RAG提供基于私有知识库的精准问答，Agent则完成复杂任务的自主规划与操作执行，共同打造面向企业级应用的智能化解决方案。; 适合人群：AI开发者、... 人工智能基于腾讯云智能体开发平台的RAG与多智能体协同技术：企业级大模型应用快速落地解决方案 09-18 内容概要：本文介绍了腾讯云智能体开发平台在AI Agent技术创新方面的核心能力与企业级智能体快速落地的实践路径。平台通过RAG、Workflow和Multi-Agent三大技术框架的持续升级，支持知识库问答、复杂流程编排与多智能... RAG 系统评测实践详细版：Coze 及相关产品评测对比，以及下一代 RAG 技术 m0_59235245的博客 10-09 1785 上面提到了 RAG 的定义是结合信息检索和大模型生成能力，为了全面评估 RAG 系统的性能，我们需要分别考察其在信息检索和答案生成两个子任务上的表现:评测信息检索能力： 借鉴传统 IR(Information Retrieval) 系统的评估方法和指标，如平均倒数排序 (MRR)、精确率 (Precision)、召回率 (Recall) 等。大模型生成能力： 主要依赖于语言模型本身的性能，看答案的流畅度、连贯性、准确性等方面，可以采用人工评分、BLEU 等自动化指标，以及问答准确率等任务相关指标。 RAG系列：一文让你由浅到深搞懂RAG实现 ytt0523_com的博客 05-01 2177 RAG（检索增强生成）是一种结合检索与生成的技术，通过实时检索外部知识库中的信息，动态增强大语言模型（LLM）的生成能力，是用于解决幻觉问题、知识时效问题、超长文本问题等各种大模型本身制约或不足的必要技术。RAG核心流程包括：对用户问题进行改写、扩写和重构，让用户问题更利于检索；从外部知识库（如企业文档、行业数据库）中筛选与用户问题相关的片段‌，并将检索结果与原始问题整合为增强提示词，输入给LLM；‌LLM基于增强后的提示词，生成精准、可靠的答案。RAG具有以下优点：实时性。 AI产品经理必须知道的技术 之 RAG aolan123的博客 06-25 1432 在对用户问题进行Embedding前，对问题进行补充完善。避免用户问题太过简单、或者有明显错误。也可以考虑将用户问题，进行主题关键词抽取，或者使用知识图谱等进行初步的信息识别。这么做的目的是，避免用户的问题，信息太多太杂，导致检索出来的相关文档，与用户提问意图关联不大。也就是对用户问题进行简化。在检索文档时，可以增加一些过滤条件，例如指定章节、关键词包含、日期筛选、相似度阈值等，以使检索出来的内容更准确。对检索结果，也可考虑将相似度，与文档自身的权重进行综合加权。使提供给大模型的内容资料是最优的。 读懂RAG这一篇就够了，万字详述RAG的5步流程和12个优化策略 热门推荐 2401_82452722的博客 01-30 2万+ ©作者|帅气的桌子来源|神州问学RAG概述ChatGPT、GLM等生成式人工智能在文本生成、文本到图像生成等任务中表现出令人印象深刻的性能。但它们也存在固有局限性，包括产生幻觉、缺乏对生成文本的可解释性、专业领域知识理解差，以及对最新知识的了解有限。为了克服这些限制，提高模型的能力，有两种主要途径：一种是微调（Fine Tune）来更新模型，另一种是让他们能够与外部世界互动，以不同的形式和方式获取知识。微调固然效果好，可以让模型真正的“学会”一些私域知识。但是微调也会带来几个问题：首先，由于生成模型依赖于内 RAG是什么，RAG综述，一文让你由浅到深搞懂RAG实现！ qq_46094651的博客 05-14 1454 RAG（检索增强生成）是一种结合检索与生成的技术，通过实时检索外部知识库中的信息，动态增强大语言模型（LLM）的生成能力，是用于解决幻觉问题、知识时效问题、超长文本问题等各种大模型本身制约或不足的必要技术。 【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题？ 新亮笔记 02-06 1019 本文为译文，原文链接：https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system今天，我们将卷起袖子，深入研究构建企业级 RAG 系统的复杂世界。我们将揭示一些常见的挑战和误区，以及如何克服它们。我们还将分享一些最佳实践和技巧，让您的 RAG 系统更加强大、灵活和可靠。但这个博客不仅仅... 文章目录 一、引言 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 1.2 实现过程 1.3 示例代码 2. 知识管理平台 2.1 案例背景 2.2 实现过程 3. 企业级RAG系统建设 3.1 案例背景 3.2 实现过程 三、总结 一、引言 RAG（Retrieval-Augmented Generation，检索增强生成）技术在企业级应用中展现出巨大的潜力和价值。通过结合检索和生成，RAG系统能够提供更准确、更相关的回答，满足企业在智能客服、知识管理等多个场景中的需求。本文将分享一些企业级RAG应用案例，包括智能客服系统和知识管理平台等。 二、企业级RAG应用案例 1. 智能客服系统 1.1 案例背景 某初创公司希望通过自动化FAQ系统减轻客服团队的工作负担，提升客户服务体验。他们选择了LangChain框架来搭建基于RAG的聊天机器人，该机器人可以在回答用户问题时搜索相关文档，并生成答案[88]。 1.2 实现过程 加载并分割文档：使用DirectoryLoader加载FAQ文档，并使用CharacterTextSplitter进行分割。 创建向量数据库：使用OpenAIEmbeddings生成文档嵌入，并存储在Chroma向量数据库中。 初始化LLM和QA链：使用ChatOpenAI模型作为LLM，并初始化ConversationalRetrievalChain。 开始对话：用户提出问题，系统检索相关文档并生成回答。","source":"web","publishedAt":"2025-02-28T08:00:00+08:00"},{"id":"bocha-2","title":"RAG系统在企业中的应用:构建、优化与效益分析_rag 企业应用示例-CSDN博客","url":"https://blog.csdn.net/m0_57081622/article/details/138728596","snippet":"RAG系统在企业中的应用：构建、优化与效益分析 最新推荐文章于 2025-12-31 02:45:00 发布 原创 最新推荐文章于 2025-12-31 02:45:00 发布 · 1.2k 阅读 · 19 · 19 · CC 4.0 BY-SA版权 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 文章标签： #人工智能 #语言模型 #机器学习 #职场和发展 #ai 虽然互联网上充斥着有关简单 RAG 系统的文章，但构建一个稳健的企业级解决方案的过程却往往充满未知。大多数构建者甚至不知道他们在构建 RAG 系统时最重要的决策是什么…… 但这篇博客不仅仅是理论上的旅程，它也是一个帮助您采取行动的实用指南！从保障措施对于确保安全的重要性到查询重写对用户体验的影响，我们将提供可操作的见解和真实世界的示例。无论您是经验丰富的开发人员还是引领团队的技术领导者，请系好安全带，准备深入探索前沿企业级 RAG 的复杂世界！ 在探讨 RAG 架构之前，我想分享一项关于构建 RAG 系统时常见故障点的最新研究。研究人员分析了来自三个独特领域的案例研究，发现了七个常见的 RAG 故障点。 构建 RAG 系统的挑战 「案例研究：」 认知评审员 (Cognitive reviewer) 认知评审员是一个 RAG 系统，旨在帮助研究人员分析科学文献。研究人员可以定义一个研究问题或目标，然后上传一系列相关的研究论文。然后，系统会根据既定的目标对所有文档进行排序，供研究人员手动评审。此外，研究人员还可以直接向整个文档集提问。 人工智能导师 (AI Tutor) AI 导师是另一个 RAG 系统，它可以让学生就某个单元提问，并根据学习内容获取答案。学生可以通过访问来源列表来验证答案。AI 导师集成在迪肯大学的学习管理系统中，可以索引所有内容，包括 PDF、视频和文本文档。系统在切分视频之前使用 Whisper 深度学习模型转录视频。RAG 管道包含一个用于查询泛化的改写器，并且聊天界面利用过去对话为每个问题提供上下文。 生物医学问答 (Biomedical Q&A) 在生物医学问答案例研究中，使用 BioASQ 数据集创建了一个 RAG 系统，该数据集包含问题、文档链接和答案。该数据集由生物医学专家准备，包含领域特定的问题-答案对。问题的答案可以是是非题、文本摘要、事实性陈述或列表。 RAG 系统的 7 个故障点 通过这些案例研究，研究人员发现了构建 RAG 系统时经常出现的七个故障点： 「缺失内容 (FP1)」 ：用户提出的问题无法用现有文档回答。理想情况下，RAG 系统会回复类似 “抱歉，我不知道” 的消息。然而，对于缺乏明确答案的内容相关问题，系统可能会误导性地提供回复。 「错失顶尖文档 (FP2)」 ：问题的答案存在于文档中，但排名不够高，未被包含在返回给用户的结果中。虽然理论上所有文档都经过排名并用于后续步骤，但实际上只会返回排名前 K 的文档，K 值根据性能进行选择。 「不在上下文 - 整合策略限制 (FP3)」 ：包含答案的文档从数据库中检索出来，但未能整合到生成回复的上下文中。这种情况发生在返回大量文档时，会导致整合过程受阻，从而妨碍相关答案的检索。 「未提取 (FP4)」 ：答案存在于上下文中，但模型未能提取正确的信息。这种情况通常发生在上下文中存在大量噪声或冲突信息时。 「格式错误 (FP5)」 ：问题涉及提取特定格式的信息，例如表格或列表，但模型忽略了指令。 「特异性错误 (FP6)」 ：回复包含答案，但缺乏必要的特异性或过度特异，未能满足用户需求。这种情况发生在 RAG 系统设计者对给定问题预设了结果时，例如教师寻求教育内容。在这种情况下，除了答案之外还应提供特定的教育内容。特异性错误也出现在用户不确定如何措辞问题过于笼统时。 「不完整 (FP7)」 ：不完整的答案虽然准确，但缺少一些信息，即使这些信息存在于上下文中并且可以提取。例如，诸如“文档 A、B 和 C 中涵盖了哪些关键点？”这样的问题，不如将它们分开提问会更好。 下表总结了他们解决每个问题学到的经验教训。构建企业级 RAG 系统时，我们将牢记这些教训。 如何构建企业级 RAG 系统 现在我们已经了解了设计 RAG 系统时常遇到的常见问题，接下来我们将逐步讲解每个组件的设计需求和作用，以及构建这些组件的最佳实践。上面的 RAG 系统架构图提供了每个组件的使用位置和方式的上下文。 用户认证 这是整个系统的起点！在用户开始与聊天机器人互动之前，我们需要出于各种原因对用户进行身份验证。身份验证有助于确保安全性 和个性化，这对于企业系统来说是必不可少的。 「访问控制」：身份验证确保只有授权用户才能访问系统。它有助于控制谁可以与系统互动以及他们被允许执行哪些操作。 「数据安全」：保护敏感数据至关重要。用户身份验证可防止未经授权的个人访问机密信息，从而防止数据泄露和未经授权的数据操纵。 「用户隐私」：身份验证通过确保只有目标用户才能访问其个人信息和账户详细信息来帮助维护用户隐私。这对于建立用户信任至关重要。 「法律合规」：许多司法管辖区和行业都有法规和法律要求组织实施适当的用户身份验证来保护用户数据和隐私。遵守这些法规有助于避免法律问题和潜在惩罚。 「问责制」：身份验证通过将系统内的操作与特定用户账户关联来确保问责制。这对于审计和跟踪用户活动至关重要，有助于识别和解决任何安全事件或可疑行为。 「个性化和定制」：身份验证允许系统识别单个用户，从而实现个性化和定制用户体验。这可以包括定制内容、偏好和设置。 像 AWS Cognito 或 Firebase Authentication 之类的服务可以帮助您轻松地将用户注册和身份验证添加到移动和网络应用程序中。 输入护栏 防止有害或包含隐私信息的 用户输入 至关重要。最近的研究表明，劫持大型语言模型 (LLMs) 变得容易。这就是输入护栏发挥作用的地方。让我们来看看需要护栏的不同场景。 「匿名化」：输入护栏可以匿名化或编辑个人可识别信息 (PII)，例如姓名、地址或联系方式。这有助于保护隐私并防止恶意披露敏感信息的尝试。 「限制子字符串」：禁止某些子字符串或模式，这些子字符串或模式可能会被利用进行 SQL 注入、跨站点脚本 (XSS) 或其他注入攻击，从而防止安全漏洞或不需要的行为。 「限制主题」：为了限制讨论或输入与特定主题相关的内容，这些主题可能不当、冒犯或违反社区准则，因此过滤掉包含仇恨言论、歧视或色情内容的内容很重要。 「限制代码」：必须防止注入可执行代码，否则可能会破坏系统安全或导致代码注入攻击。 「限制语言」：验证文本输入是否使用正确的语言或脚本，以防止处理过程中出现潜在的误解或错误。 「检测提示注入」：减轻注入误导性或有害提示的尝试，这些提示可能以非预期方式操纵系统或影响大型语言模型的行为。 「限制令牌」：对用户输入强制执行最大令牌或字符限制有助于避免资源耗尽并防止拒绝服务 (DoS) 攻击。 「检测毒性」：实施毒性过滤器来识别和阻止包含有害或辱骂语言的输入。 为了保护您的 RAG 系统免受这些场景的影响，您可以利用 Meta 提供的 Llama Guard。您可以自己托管它，也可以使用 Sagemaker 等托管服务。但是，请不要指望它能完美地检测毒性内容。 查询重写器 一旦查询通过输入护栏，我们就会将其发送到查询重写器。有时候，用户查询可能含糊不清，或者需要上下文才能更好地理解用户的意图。查询重写是一种有助于解决此问题的技术。它涉及转换用户查询以提高清晰度、准确性和相关性。让我们来看看一些最常用的技术： 「基于历史记录重写」：这种方法中，系统利用用户的查询历史记录来理解对话的上下文并改进后续查询。例如，信用卡查询： 查询历史记录： 您有多少张信用卡？ 白金卡和金卡是否有年费？ 比较两者的功能。 基于用户查询历史记录，我们需要识别上下文的发展脉络，辨别用户查询之间的意图和关联，并生成与不断演变的上下文相符的查询。 重写后的查询：比较白金卡和金卡的功能。 「创建子查询」：由于检索问题，复杂查询可能难以回答。为了简化任务，查询会被分解成更具体的子查询。这有助于检索生成答案所需的正确上下文。LlamaIndex 将此称为子问题查询引擎。 例如，对于查询“比较白金卡和金卡的功能”，系统会为每个信用卡生成子查询，分别关注原始查询中提到的单个实体。 重写后的子查询： 白金信用卡的功能有哪些？ 金信用卡的功能有哪些？ 「创建相似查询」：为了提高检索相关文档的可能性，我们会根据用户输入生成类似的查询。这可以克服检索在语义或词汇匹配方面的限制。 如果用户询问信用卡的功能，系统会生成相关的查询。可以使用同义词、相关术语或领域知识来创建与用户意图相符的查询。 生成的相似查询： 我想知道白金信用卡。-> 告诉我白金信用卡的优点。 选择文本编码器需要考虑的因素 在选择文本编码器时，您需要决定使用私有编码器还是公共编码器。由于私有编码器易于使用，您可能会倾向于使用它们，但在这两种选择之间需要权衡一些具体的利弊。这是一个重要的决定，它将影响您系统的性能和延迟。 「查询成本」 确保语义搜索的流畅用户体验依赖于嵌入式 API 服务的高可用性。OpenAI 和类似的供应商提供可靠的 API，消除了托管管理的需要。然而，选择开源模型需要根据模型大小和延迟需求进行工程方面的投入。较小的模型（最多 1.1 亿参数）可以利用 CPU 实例托管，而较大的模型可能需要 GPU 服务来满足延迟要求。 「索引成本」 设置语义搜索涉及对文档进行索引，这会产生非平凡的成本。由于索引和查询共享相同的编码器，因此索引成本取决于所选择的编码器服务。为了方便服务重置或重新索引到替代向量数据库，建议单独存储嵌入向量。忽略此步骤将需要重新计算相同的嵌入向量。 「存储成本」 对于索引数百万个向量的应用程序，向量数据库的存储成本是一个重要因素。存储成本与维度线性扩展，OpenAI 在 1526 维度的嵌入向量产生最大的存储成本。要估计存储成本，请计算每个文档的平均单位（词组或句子）并进行外推。 「语言支持」 为了支持您的非英语语言，可以使用多语言编码器或将翻译系统与英语编码器结合使用。 「搜索延迟」 语义搜索的延迟与嵌入向量的维度成线性比例增长。为了尽量减少延迟，最好选择较低维度的嵌入向量。 「隐私」 像金融和医疗保健等敏感领域的严格数据隐私要求可能会使像 OpenAI 这样的服务变得不可行。 文档摄取 文档摄取系统管理着数据的处理和持久化。在索引过程中，每个文档都会被分成较小的块，然后使用嵌入模型转换为嵌入向量。然后将原始块和嵌入向量一起编入索引数据库。让我们看看文档摄取系统的组件。 「文档解析器」 文档解析器在主动从各种文档格式中提取结构化信息方面起着核心作用，尤其关注格式处理。这包括但不限于解析可能包含图像和表格的 PDF 文档。 「文档格式」 文档解析器必须能够熟练处理各种文档格式，例如 PDF、Word、Excel 等，以确保文档处理的可适应性。这涉及识别和管理嵌入内容，例如超链接、多媒体元素或注释，以提供文档的综合表示。 「表格识别」 识别和提取文档中的表格数据对于维护信息结构（尤其是在报告或研究论文中）至关重要。提取与表格相关的元数据，包括标题、行和列信息，可以增强对文档组织结构的理解。诸如表格转换器之类的模型可以用于此任务。 「图像识别」 光字符识别 (OCR) 应用于文档中的图像，以主动识别和提取文本，使其可以进行索引和后续检索。 「元数据提取」 元数据是指关于文档的附加信息，它不是文档主要内容的一部分。它包括作者、创建日期、文档类型、关键字等详细信息。元数据提供 valuable context 并帮助组织文档，并通过考虑元数据属性来提高搜索结果的相关性。可以使用 NLP/OCR 管道提取元数据，并将其作为特殊字段与文档一起索引。 「分块器」 您决定如何对长文本进行分词 (拆分) 可以决定嵌入向量质量和搜索系统的性能。如果块太小，则无法回答某些问题；如果块太长，则答案会包含生成的噪音。您可以利用摘要技术来减少噪音、文本大小、编码成本和存储成本。 分块是一个重要但经常被低估的主题。它可能需要类似于特征工程的领域专业知识。例如，针对 Python 代码库的拆块可能会使用 def/class 等前缀来完成。有关分块的更深入探讨，请阅读我们的博客文章。 索引器 顾名思义，索引器负责创建文档索引，该索引用作一种结构化数据结构（快速说三遍……）。索引器可以促进高效的搜索和检索操作。高效的索引对于快速准确地检索文档至关重要。它涉及将块或令牌映射到它们在文档集合中的对应位置。索引器在文档检索方面执行重要任务，包括创建索引以及添加、更新或删除文档。 索引器作为 RAG 系统的关键组件，面临着各种挑战和问题，这些问题会影响系统整体的效率和性能。 可扩展性问题 随着文档量的增长，维护高效和快速的索引变得具有挑战性。当系统难以处理越来越多的文档时，可能会出现可扩展性问题，从而导致更慢的索引和检索速度。 实时索引更新 在文档频繁添加、更新或删除的系统中，使索引保持实时更新可能具有挑战性。确保实时 API 和实时索引机制无缝运行而不影响系统性能是一项持续的挑战。 一致性和原子性 面对并发文档更新或修改时，实现一致性和原子性可能很复杂。确保即使在同时进行更改的情况下，索引更新也能维护数据完整性，这需要仔细的设计和实现。 优化存储空间 索引大量文档可能会导致大量存储需求。优化存储空间同时确保索引保持可访问和响应是一个持续的挑战，尤其是在存储成本成为关注问题的情况下。 安全和访问控制 实施适当的安全措施和访问控制以防止对索引进行未经授权的修改至关重要。确保只有授权用户或进程才能执行 CRUD 操作有助于保护文档存储库的完整性。 监控和维护 定期监控索引器的健康和性能至关重要。检测诸如索引失败、资源瓶颈或过时索引等问题需要健壮的监控和维护程序，以确保系统随着时间的推移顺利运行。 这些都是一些众所周知的软件工程难题，可以通过遵循良好的软件设计实践来解决。 数据存储 由于我们处理各种数据，因此我们需要针对每种数据采用专用的存储方式。对于每种存储类型及其特定用例，了解不同的注意事项至关重要。 嵌入向量 数据库类型：SQL/NoSQL 单独存储文档嵌入向量可以实现快速重新索引，而无需重新计算整个文档语料库的嵌入向量。此外，嵌入向量存储还可以充当备份，即使在系统故障或更新的情况下也能确保关键信息的保留。 文档 数据库类型：NoSQL 以原始格式存储文档对于持久化存储至关重要。这种原始格式作为各种处理阶段（例如索引、解析和检索）的基础。它还为未来的系统增强提供了灵活性，因为原始文档保持不变，可以根据需要进行重新处理。 聊天历史记录 数据库类型：NoSQL 存储聊天历史记录对于支持 RAG 系统的对话方面必不可少。聊天历史记录存储允许系统回忆用户之前的查询、回复和偏好，使其能够根据用户的独特上下文进行调整和定制未来的交互。这些历史数据是通过利用它们进行研究来改进机器学习系统的重要资源。 用户反馈 数据库类型：NoSQL/SQL 用户反馈通过 RAG 应用程序中的各种交互机制系统地收集。在大多数 LLM 系统中，用户可以使用顶/踩、星级评分和文本反馈提供反馈。这一系列用户见解作为一个宝贵的存储库，囊括了用户体验和感知，构成了持续系统增强的基础。 向量数据库 为语义搜索提供支持的向量数据库是 RAG 系统的关键检索组件。然而，选择合适的组件对于避免潜在问题至关重要。在选择过程中需要考虑几个 向量数据库因素。让我们来看看其中的一些。 召回率 vs. 延迟 在向量数据库中，优化召回率（相关结果的百分比）和延迟（返回结果的时间）需要进行权衡。Flat、HNSW（分层可导航小世界）、PQ（产品量化）、ANNOY 和 DiskANN 等不同索引在速度和召回率之间会做出不同的权衡。对您的数据和查询进行基准研究以做出明智的决策。 成本 具有托管解决方案的云原生数据库通常根据数据存储和查询量计费。对于拥有大量数据的组织来说，这种模式可以避免基础设施成本。主要考虑因素包括评估数据集增长、团队能力、数据敏感性以及了解托管云解决方案的成本影响。 自主托管 vs. 托管服务 另一方面，自托管为组织提供了对其基础设施的更大控制权，并且成本也可能更低。但是，它也伴随着管理和维护基础设施的责任，包括可扩展性、安全性和更新方面的考虑。 插入速度与查询速度 平衡插入速度和查询速度至关重要。寻找能够处理具有高插入速度要求的流式用例的供应商。但是，对于大多数组织而言，优先考虑查询速度更具相关性。评估高峰时段的向量插入速度查询延迟，以做出明智的决策。 内存 vs. 磁盘索引存储 在内存存储和磁盘存储之间进行选择涉及速度和成本的权衡。虽然内存存储提供高速性能，但某些用例需要存储大于内存的向量。内存映射文件等技术可以在不影响搜索速度的情况下扩展向量存储。DiskANN 中的 Vamana 等新索引承诺提供高效的内存外索引。 Full-Text search vs. Vector Hybrid search 来源：https://www.pinecone.io/learn/hybrid-search-intro/ 仅仅使用向量搜索可能不够适用于企业级应用程序。另一方面，混合搜索，即集成了密集和稀疏方法的搜索，需要额外的工作。实现密集向量索引、稀疏倒排索引和重新排序步骤是典型的。通过一个名为 alpha 的参数在 Pinecone、Weaviate 和 Elasticsearch 中调整密集和稀疏元素之间的平衡。 过滤 现实世界中的搜索查询通常涉及对元数据属性进行过滤。虽然预过滤搜索似乎是自然的，但可能会导致缺少相关结果。如果过滤后的搜索查询中被过滤的属性只占数据集的一小部分，后过滤搜索可能会出现问题。像 Weaviate 这样的自定义过滤搜索结合了预过滤和倒排索引分片以及 HNSW 索引分片的有效语义搜索。 提高检索效率的技术 最近的研究表明，大型语言模型(LLMs)很容易被无关的上下文所分散注意力，而且拥有大量上下文（检索到的 topK 文档）可能会因为LLMs的注意力模式而错过某些上下文。因此，利用相关和多样化的文档来提高检索是至关重要的。让我们看一些提高检索的已证实技术。 假设文档嵌入（HyDE） 我们可以使用 HyDE 技术来解决检索性能差的问题，特别是在处理短查询或不匹配查询时，这可能会使查找信息变得困难。HyDE 采用独特的方法，通过使用 GPT 等模型创建的假设性文档来解决这个问题。这些假设性文档捕获了重要的模式，但可能具有虚构或不正确的细节。然后，一个智能文本编码器将这个假设性文档转换成一个向量嵌入。这个嵌入有助于在集合中找到类似的真实文档，比查询的嵌入更好。 实验表明，HyDE 比其他高级方法效果更好，使其成为提升 RAG 系统性能的有用工具。 查询路由 当处理多个索引时，查询路由会带来优势，将查询定向到最相关的索引，以实现高效的检索。这种方法通过确保每个查询被定向到适当的索引来简化搜索过程，优化信息检索的准确性和速度。 在企业搜索的背景下，数据来自各种来源，如技术文档、产品文档、任务和代码仓库，查询路由成为一个强大的工具。例如，如果用户搜索与特定产品功能相关的信息，则可以将查询智能地路由到包含产品文档的索引，从而提高搜索结果的准确性。 重新排名器 当从编码器检索的结果不能提供最佳质量时，会使用重新排名器来增强文档排名。利用开源的仅编码器变压器（如BGE-large）进行跨编码器设置已成为常见做法。最近的仅解码器方法，如RankVicuna、RankGPT和RankZephyr，进一步提高了重新排名器的性能。 引入重新排名器有其好处，可以减少LLM在响应中的幻觉，并改善系统的跨域泛化。但是，它也存在缺点。复杂的重新排名器可能会增加延迟，因为计算开销大，影响实时应用程序。此外，部署高级重新排名器可能会消耗资源，需要仔细考虑性能提升与资源利用之间的平衡。 最大边际相关性（MMR） MMR是一种旨在增强响应中检索项的多样性、避免冗余的方法。与仅关注检索最相关项不同，MMR在相关性和多样性之间取得了平衡。它就像是在派对上向朋友介绍人。首先，根据朋友的喜好，确定最匹配的人。然后，寻找略有不同的人。这个过程一直持续，直到达到所需的介绍人数。MMR确保呈现出更多样化和相关的项目集，最大限度地减少了冗余。 自动截断 Weaviate 中的自动截断功能旨在通过检测具有相近分数的对象组来限制返回的搜索结果数量。它通过分析搜索结果的分数并识别这些值中的显著跳跃来工作，这可能表明从高度相关到不太相关的结果的过渡。 例如，考虑一个返回具有以下距离值的对象的搜索： [0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23]。 自动截断返回以下结果： autocut: 1: [0.1899, 0.1901, 0.191] autocut: 2: [0.1899, 0.1901, 0.191, 0.21, 0.215] autocut: 3: [0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23] 6377b36750c946ae2ba7c75ec47fca036324a1ec-2054x800.webp Source: https://youtu.be/TRjq7t2Ms5I?si=D0z5sHKW4SMqMgSG&t=742 递归检索，又称为由小到大的检索技术，将较小的块嵌入以进行检索，同时返回更大的父上下文给语言模型进行综合。较小的文本块有助于更准确地进行检索，而较大的块则为语言模型提供了更丰富的上下文信息。这个连续的过程通过最初集中于较小、信息更密集的单元来优化检索的准确性，然后将它们高效地链接到更广泛的上下文父块以进行综合。 句子窗口检索 检索过程获取一个单独的句子，并返回该特定句子周围的文本窗口。句子窗口检索确保检索到的信息不仅准确，而且在语境上相关，提供了主要句子周围的全面信息。 生成器 现在我们已经讨论了所有的检索组件，让我们来谈谈生成器。这需要仔细考虑和权衡，主要是在自托管推断部署和私有 API 服务之间进行。这本身是一个大话题，我会简要提及，以避免让您感到不知所措。 API 考虑因素 在评估 LLMs 的 API 服务器时，优先考虑确保无缝集成和强大性能的功能至关重要。一个设计良好的 API 应该作为流行的 LLMs 的简单启动器，同时还要解决关键考虑因素，如生产就绪性、安全性和幻觉检测。值得注意的是，HuggingFace 的 TGI 服务器体现了这些原则的一套全面功能。让我们了解一下在 LLM 服务器中需要的一些最受欢迎的功能。 性能 高效的 API 必须优先考虑性能，以满足不同用户需求。张量并行性是一种在多个 GPU 上实现更快推断的功能，增强了整体处理速度。此外，持续批处理传入请求确保了总吞吐量的增加，有助于实现更响应迅速和可扩展的系统。使用位和字节以及 GPT-Q 进行量化进一步优化了 API，在各种用例中提高了效率。利用优化的变压器代码确保了在最流行的架构上无缝推断。 生成质量增强器 为了提高生成质量，API 应该包含能够转换输出的功能。对数处理器包括温度缩放、top-p、top-k 和重复惩罚，允许用户根据自己的偏好自定义输出。此外，停止序列提供了对生成的控制，使用户可以管理和优化内容生成过程。对数概率对幻觉检测至关重要，它作为一种额外的精炼层，确保生成的输出与预期的上下文一致，避免误导性信息。 安全性 API 的安全性至关重要，特别是在处理 LLMs 和企业用例时。安全张量权重加载是一个重要功能，通过防止未经授权的模型参数篡改来有助于模型的安全部署。此外，包含水印技术增加了一层额外的安全性，使得追踪和追责在 LLMs 的使用中成为可能。 用户体验 在用户体验领域，标记流是一种关键功能，用于实现无缝交互。利用服务器发送事件（SSE）进行标记流增强了 API 的实时响应性，为用户提供了更流畅和更具交互性的体验。这确保了用户可以逐步接收生成的内容，提高了 LLM 的整体参与度和可用性。 自托管推断 自托管推断涉及将 LLMs 部署到由云服务提供商（如 AWS、GCP 或 Azure）提供的服务器上。服务器的选择，例如 TGI、Ray 或 FastAPI，是一个关键决定，直接影响系统的性能和成本。考虑因素包括计算效率、部署便利性和与所选 LLM 的兼容性。 衡量 LLM 推断性能至关重要，Anyscale 的 LLMPerf 排行榜等排行榜至关重要。它根据关键性能指标，包括首个令牌的到达时间（TTFT）、令牌间延迟（ITL）和成功率，对推断提供者进行排名。负载测试和正确性测试对评估托管模型的不同特性至关重要。 在新方法中，Predibase 的 LoRAX 以一种创新的方式高效地提供了精细调整的 LLMs。它解决了使用共享 GPU 资源服务多个精细调整模型的挑战。 私有 API 服务 像 OpenAI、Fireworks、Anyscale、Replicate、Mistral、Perplexity 和 Together 这样的公司提供的 LLM API 服务提供了替代部署方法。了解它们的功能、定价模型和 LLM 性能指标至关重要。例如，OpenAI 的基于令牌的定价模型，区分输入和输出令牌，可以极大地影响使用 API 的总成本。在比较私有 API 服务与自托管 LLMs 的成本时，必须考虑 GPU 成本、利用率和可扩展性等因素。对于一些情况来说，速率限制可能是一个限制因素。 改进 RAG 的提示技术 存在许多用于改进 RAG 输出的提示技术。在我们的 RAG 掌握系列的第二部分中，我们深入探讨了前 5 种最有效的方法。许多这些新技术超越了 CoT（思维链）的性能。您还可以将它们组合起来，以最小化幻觉。 输出保护栏 输出保护栏的功能与其输入对应物类似，但专门设计用于检测生成的输出中的问题。它侧重于识别幻觉、竞争对手提及以及可能导致品牌损害的问题，作为 RAG 评估的一部分。其目标是防止生成不准确或伦理上可疑的信息，这些信息可能与品牌的价值观不符。通过积极监控和分析输出，这个保护栏确保生成的内容保持事实准确、符合道德标准，并与品牌的准则一致。 以下是一个可能会损害企业品牌的回复示例，但会被适当的输出保护栏屏蔽： 用户反馈 一旦生成并提供输出，从用户那里获得积极或消极的反馈是非常有帮助的。用户反馈对于改进 RAG 系统的推动力量非常重要，这是一个持续的过程，而不是一次性的努力。这不仅包括定期执行自动化任务，如重新索引和实验重新运行，还包括系统性地整合用户见解以实现实质性的系统增强。 系统改进中最具影响力的杠杆在于积极解决底层数据中的问题。RAG 系统应包括一个用于处理用户反馈和推动持续改进的迭代工作流程。 用户互动和反馈收集 用户与 RAG 应用进行互动，并利用诸如👍/👎或星级评价等功能提供反馈。这一多样化的反馈机制集合起来作为用户对系统性能的体验和感知的宝贵库存。 问题识别和诊断检查 收集反馈后，团队可以进行全面的分析，以识别可能性能不佳的查询。这涉及检查检索到的资源并仔细审查，以确定性能不佳是否源自检索、生成或底层数据源。 数据改进策略 一旦识别出问题，特别是那些根源于数据本身的问题，团队就可以战略性地制定计划来提升数据质量。这可能涉及纠正不完整的信息或重组组织不佳的内容。 评估和测试协议 在实施数据改进后，系统必须经过严格的评估，以前性能不佳的查询。从这些评估中获得的见解可以系统地整合到测试套件中，确保根据真实世界的交互进行持续的审查和完善。 通过积极参与用户在这一全面反馈循环中，RAG 系统不仅解决了通过自动化过程识别出的问题，还利用了用户体验的丰富性。 可观测性 建立 RAG 系统并不仅仅是将系统投入生产。即使具有健壮的防护措施和用于微调的高质量数据，模型在投入生产后仍需要进行持续监控。生成式人工智能应用程序除了标准指标如延迟和成本之外，还需要特定的LLM可观测性来检测和纠正幻觉、域外查询和链路失败等问题。现在让我们来看看LLM可观测性的支柱。 提示分析和优化 使用实时生产数据识别与提示相关的问题，并通过强大的评估机制迭代，以识别和解决幻觉等问题。 LLM应用的可追溯性 从像Langchain和LlamaIndex这样的常见框架中捕获LLM的追踪数据，以调试提示和步骤。 信息检索增强 排除故障并评估RAG参数，以优化对LLM性能至关重要的检索过程。 警报 如果系统行为与预期不符，例如错误增加、高延迟和幻觉等，即可收到警报。 首先和最重要的是，实时监控对于观察应用程序在生产环境中的性能、行为和整体健康状况至关重要。要密切关注 SLA 符合情况，并设置警报，以及时解决任何偏差。通过分析使用模式和资源消耗来有效跟踪运行LLM应用程序所涉及的成本，以帮助您进行成本优化。 Galileo 的 LLM Studio 提供了专门设计的LLM可观测性，以在用户投诉之前主动发出警报并立即采取纠正措施。Galileo 的防护指标旨在监控您模型的质量和安全性，涵盖基础、不确定性、真实性、语调、毒性、PII 等方面。这些指标先前用于评估和实验，现在可以无缝集成到监控阶段。 此外，您还可以灵活注册自定义指标，以定制监控过程以满足您的具体需求。利用从监控数据中生成的见解和警报，了解需要关注的潜在问题、异常情况或改进领域。这种全面的方法确保您的LLM应用程序在现实场景中高效安全地运行。 缓存 对于大规模运营的公司来说，成本可能成为障碍。缓存是一种在这种情况下节省资金的好方法。缓存涉及将提示及其对应的响应存储在数据库中，以便以后检索使用。这种战略性缓存机制使大型语言模型应用程序能够通过以下三个显着优势来加快和节省响应成本： 「增强生产推理」：缓存有助于在生产过程中更快速、更经济地进行推理。通过利用缓存的响应，某些查询可以实现接近于零的延迟，从而简化用户体验。 「加速开发周期」：在开发阶段，缓存被证明是一项福音，因为它消除了为相同的提示重复调用 API 的需要。这可以带来更快速、更经济的开发周期。 「数据存储」：一个存储所有提示的综合数据库的存在简化了大型语言模型的微调过程。利用存储的提示-响应对可以简化基于累积数据的模型优化。 如果您想要认真实施缓存，您可以利用 GPTCache 来缓存完全匹配和相似匹配的响应。它提供了诸如缓存命中率、延迟和召回率等值得的指标，这些指标可以洞察缓存的性能，从而实现持续优化以确保最佳效率。 多租户 SaaS 软件通常有多个租户，需要平衡简单性和隐私性。对于 RAG 系统的多租户，目标是构建一个不仅能有效地查找信息，而且还能尊重每个用户数据限制的系统。用更简单的术语来说，系统会隔离每个用户的交互，确保系统只会查看和使用针对该用户的相关信息。 构建多租户的一种简单方法是使用元数据。当我们将文档添加到系统时，我们在元数据中包含特定的用户信息。这样，每个文档都与特定用户相关联。当用户搜索时，系统会使用此元数据进行过滤，只显示与该用户相关的文档。然后，它会进行智能搜索以找到该用户最重要的信息。这种方法可以防止不同用户之间的私人信息混淆，从而保持每个人的数据安全和私密。 了解如何使用 Llamaindex 实现多租户。 总结 构建一个强大且可扩展的企业级 RAG 系统显然需要仔细协调互连的组件。从用户身份验证到输入护栏、查询重写、编码、文档摄取和检索组件（例如向量数据库和生成器），每个步骤都在塑造系统性能方面发挥着关键作用。 在不断发展的 RAG 系统领域，我们希望这份实用指南能帮助开发人员和领导者获得可操作的见解！ 如何学习AI大模型？ 作为一名热心肠的互联网老兵，我决定把宝贵的AI知识分享给大家。 至于能学习到多少就看你的学习毅力和能力了 。我已将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【保证100%免费】 一、全套AGI大模型学习路线 AI大模型时代的学习之旅：从基础到前沿，掌握人工智能的核心技能！ 二、640套AI大模型报告合集 这套包含640份报告的合集，涵盖了AI大模型的理论研究、技术实现、行业应用等多个方面。无论您是科研人员、工程师，还是对AI大模型感兴趣的爱好者，这套报告合集都将为您提供宝贵的信息和启示。 三、AI大模型经典PDF籍 随着人工智能技术的飞速发展，AI大模型已经成为了当今科技领域的一大热点。这些大型预训练模型，如GPT-3、BERT、XLNet等，以其强大的语言理解和生成能力，正在改变我们对人工智能的认识。 那以下这些PDF籍就是非常不错的学习资源。 四、AI大模型商业化落地方案 作为普通人，入局大模型时代需要持续学习和实践，不断提高自己的技能和认知水平，同时也需要有责任感和伦理意识，为人工智能的健康发展贡献力量。 确定要放弃本次机会？ 福利倒计时 : : 立减 ¥ 普通VIP年卡可用 立即使用 AI大模型. 关注 关注 19 点赞 踩 19 收藏 觉得还不错? 一键收藏 知道了 0 评论 分享 复制链接 分享到 QQ 分享到新浪微博 扫一扫 举报 举报 RAG模型在制造业领域的应用：智能生产与质量控制 03-09 525 1. 背景介绍 1.1 制造业的挑战与机遇 随着全球经济的发展，制造业正面临着前所未有的挑战与机遇。一方面，客户需求日益多样化，产品生命周期不断缩短，制造企业需要提高生产效率，降低成本，提升产品质量以满足市场需求。另一方面，新兴技术如物联网、大数据、人工智能等为制造业带来了革命性的变革，使得智能制 参与评论 您还未登录，请先 登录 后发表或查看评论 AI数据技术02：RAG数据检索 gongdiwudu的专栏 10-04 7583 ​ 在人工智能的动态环境中，检索增强生成（RAG）已成为游戏规则的改变者，彻底改变了我们生成文本和与文本交互的方式。RAG 使用大型语言模型（LLM） 等工具将信息检索的强大功能与自然语言生成无缝结合，为内容创建提供了一种变革性的方法。 ​ 在企业知识管理中运用 RAG：打造高效智能知识库 hy098543的博客 03-18 808 同时，知识图谱也有助于生成器生成更具逻辑性和连贯性的知识内容，通过利用实体之间的关系，生成的回答能够更好地体现知识的内在联系。同时，通过知识图谱的应用，实现了不同知识领域之间的关联查询，例如在查询某一产品的生产工艺时，能够同时获取与之相关的原材料供应商信息、质量检测标准等，提高了知识应用的效率。例如，当员工查询关于某新产品营销策略的知识时，检索器可以从过往的营销策划文档、市场调研报告、竞品分析资料等数据源中，快速定位到与之相关的信息，为后续的知识生成提供基础。 企业级RAG知识库终极指南！实战方法、关键细节与平台选型，看这篇就够了！ Python_cocola的博客 11-06 1195 有的问不到答案，有的答非所问，有的跑得慢还烧钱。 其实往往不是模型不够强，而是你背后的 RAG 知识库没搭好。 企业级RAG系统构建指南：深度万字报告揭秘RAG实施最优策略 Everly_的博客 12-20 1206 自2022年底OpenAI发布ChatGPT以来，大模型受到市场广泛关注，各行各业积极探索大模型的应用。但从企业实践来看，将大模型无缝集成到企业工作流中存在较多挑战，包括大模型的幻觉、开发和维护大模型的高成本以及由于大模型知识库的局限性而导致的准确率不满足业务需求。在实践RAG的过程中，企业会发现RAG走通很容易，但实际落地生产的难度非常大。基于对企业RAG落地实践的调研与研究， RAG在企业应用：场景深入与进阶策略 kaka0722ww的博客 03-21 1476 随着大语言模型（LLM）在对话与生成任务上的快速普及，Retrieval-Augmented Generation（RAG）成为解决大模型“幻觉”等短板的重要方案。通过将外部检索到的真实文档嵌入对话上下文，RAG确保了输出的准确性与可控性，在企业内的知识库问答、技术支持、报告生成等应用中备受关注。 RAG：基于大模型的融合应用探索-146页.pdf 04-23 小米公司在语音助手场景中，通过优化Prompt构建和工具调用，实现了更高效的Agent运作。在办公领域，RAG技术通过检索增强生成，解决了大模型的幻觉和知识更新问题，提高了问答系统的准确性和时效性。Elasticsearch 8... 精品资料：大模型LLM+RAG：大模型前沿技术与应用构建指南-160页.pdf 04-09 大模型前沿技术与应用构建指南详细介绍了LLM（大语言模型）与RAG（检索增强生成）技术在各种业务场景中的应用和实践。文档从360广告推荐业务的背景及需求入手，阐述了语言模型在推荐系统中的应用，包括用户表征预... RAG实战 第六章：RAG 系统部署、监控与持续优化 学习，输出==》再学习再输出 06-25 517 将 RAG 应用从开发环境迁移到生产环境，并确保其长期稳定、高效、可靠地运行，是构建成功智能客服助手的最后也是最重要的一步。本章将引导读者完成 RAG 系统的部署，并详细讲解如何对其进行有效的监控、日志管理以及基于性能反馈进行持续优化的策略。 RAG在测试领域的应用：提升测试输出质量的关键技术 最新发布 测试者家园 12-31 1600 传统手段难以兼顾效率与质量：经验依赖人工审核，自动化生成缺乏语境理解，结果往往流于形式。这时，RAG（Retrieval-Augmented Generation）技术的引入，为提升测试输出质量提供了新的可能。 RAG系统架构演进全景图：从2024到2025的实战观察 2502_94431433的博客 11-28 90 这些数字背后是实实在在的业务影响。比如在荷兰语RAG系统测试中，Cohere embed-multilingual-v3成功识别了\"hiërarchie\"（等级制度）与\"ranken\"（级别）的语义关联，而专攻英语的模型完全失效。查询进来先被\"重写\"和\"扩展\"，检索结果还要经过\"重排序\"和\"摘要压缩\"。从技术演进的角度看，RAG正在从\"增强生成的工具\"转变为\"认知协作的平台\"。通过这套系统性的调试方法论，你能够从被动救火转向主动优化，真正掌握RAG系统调试的精髓，实现从\"踩坑\"到\"精通\"的完整进化路径。 关于RAG在企业生产过程中的应用方向探索 ainnle的专栏 05-04 1221 RAG在企业生产过程中的应用 企业级RAG应用的5大技术发展趋势，各位老板们准备好了吗？ 2401_85375151的博客 12-10 1061 基于大模型的RAG（Retrival-Augmented Generation，检索增强生成）已经成为生成式AI落地最重要的应用形式之一。随着相关理论与实践的不断展开与完善，RAG应用在企业领域也逐渐从原型阶段走向了生产，并呈现出了一些显著的技术发展趋势。 【实战分享】构建企业级RAG（Retrieval-Augmented Generation）知识库的全面实践 后端研发工程师Marion的博客 12-22 5018 大模型指的是训练参数量极其庞大的深度学习模型，如GPT-3、GPT-4等。这些模型能够通过海量的数据学习，具备强大的语言理解和生成能力。在问答系统中，大模型能够理解用户提出的问题，并生成相关的回答。\"\"\"添加文档向量\"\"\"pass\"\"\"相似度检索\"\"\"pass\"\"\"获取页面分段\"\"\"passMilvus是一个开源的向量数据库，专为高效的向量存储和检索设计。Milvus支持多种索引方式（如IVF、HNSW等），并提供高效的查询和检索功能。索引构建：Milvus通过创建索引加速查询速度。 RAG 入门指南：从零开始构建一个 RAG 系统 mama19971023的博客 08-05 2476 在开始之前，我还是打算再次简要的介绍一下 RAG。在 Meta 的官方 Blog 上有这样一段话：这段话主要讲述了一个新的模型架构，也就是RAG (检索增强生成)的重要性和优势。可以概括为以下几点：1. 构建一个能够进行研究和上下文分析的模型虽然更具挑战性，但对未来的技术进步非常关键；2. 通过在知识密集的下游任务上微调，RAG 可以实现最先进的结果，比现有的最大的预训练序列到序列语言模型还要好；3. 与传统的预训练模型不同，RAG 的内部知识可以轻松地动态更改或补充。 【建议收藏】企业级 RAG 产品的搭建需要重点考虑哪些问题？ 新亮笔记 02-06 1019 本文为译文，原文链接：https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system今天，我们将卷起袖子，深入研究构建企业级 RAG 系统的复杂世界。我们将揭示一些常见的挑战和误区，以及如何克服它们。我们还将分享一些最佳实践和技巧，让您的 RAG 系统更加强大、灵活和可靠。但这个博客不仅仅... 企业级大模型的护城河：RAG + 微调 新缸中之脑 01-31 1729 围绕LLM的炒作是前所未有的，但这是有道理的，生成式 AI 有潜力改变我们所知道的社会。在很多方面，LLM将使数据工程师变得更有价值——这令人兴奋！不过，向老板展示数据发现工具或文本到 SQL 生成器的炫酷演示是一回事，而将其与公司的专有数据（甚至更重要的客户数据）一起使用则是另一回事。很多时候，公司急于构建人工智能应用程序，却对其实验的财务和组织影响缺乏远见。这不是他们的错——高管和董事会应该为围绕这项（以及大多数）新技术的“快点走”心态承担责任。（还记得 NFT 吗？ RAG实战篇：构建一个最小可行性的Rag系统 xx_nm98的博客 09-24 2295 经过上述流程，我们搭建了一个非常简单的Naive RAG系统，这个系统解析了一篇博客文章，然后接收用户提问，并使用博客的内容做增强生成。这是一个非常简单的框架，也很易于理解。但是在实际应用中还有非常多需要优化的地方，包括Indexing（索引）、Query Translation（查询转换）、Routing（路由）、Query Construction（查询构建）、Retrival（检索）和Generation（生成），每个环节都有多种有效的优化方式。 RAG在医疗领域的应用：辅助诊断，提升医疗效率 04-30 1515 1. 背景介绍 1.1 医疗领域的挑战 医疗领域一直面临着诸多挑战,例如医疗资源分布不均、医生工作压力巨大、医疗成本不断上升等。随着人口老龄化和慢性病患病率的上升,这些挑战变得更加严峻。因此,提高医疗效率、降低医疗成本、提供更好的医疗服务成为当务之急。 虽然互联网上充斥着有关简单 RAG 系统的文章，但构建一个稳健的企业级解决方案的过程却往往充满未知。大多数构建者甚至不知道他们在构建 RAG 系统时最重要的决策是什么…… 但这篇博客不仅仅是理论上的旅程，它也是一个帮助您采取行动的实用指南！从保障措施对于确保安全的重要性到查询重写对用户体验的影响，我们将提供可操作的见解和真实世界的示例。无论您是经验丰富的开发人员还是引领团队的技术领导者，请系好安全带，准备深入探索前沿企业级 RAG 的复杂世界！ 在探讨 RAG 架构之前，我想分享一项关于构建 RAG 系统时常见故障点的最新研究。研究人员分析了来自三个独特领域的案例研究，发现了七个常见的 RAG 故障点。 构建 RAG 系统的挑战 「案例研究：」 认知评审员 (Cognitive reviewer) 认知评审员是一个 RAG 系统，旨在帮助研究人员分析科学文献。研究人员可以定义一个研究问题或目标，然后上传一系列相关的研究论文。然后，系统会根据既定的目标对所有文档进行排序，供研究人员手动评审。此外，研究人员还可以直接向整个文档集提问。 人工智能导师 (AI Tutor) AI 导师是另一个 RAG 系统，它可以让学生就某个单元提问，并根据学习内容获取答案。学生可以通过访问来源列表来验证答案。AI 导师集成在迪肯大学的学习管理系统中，可以索引所有内容，包括 PDF、视频和文本文档。系统在切分视频之前使用 Whisper 深度学习模型转录视频。RAG 管道包含一个用于查询泛化的改写器，并且聊天界面利用过去对话为每个问题提供上下文。 生物医学问答 (Biomedical Q&A) 在生物医学问答案例研究中，使用 BioASQ 数据集创建了一个 RAG 系统，该数据集包含问题、文档链接和答案。该数据集由生物医学专家准备，包含领域特定的问题-答案对。问题的答案可以是是非题、文本摘要、事实性陈述或列表。 RAG 系统的 7 个故障点 通过这些案例研究，研究人员发现了构建 RAG 系统时经常出现的七个故障点： 「缺失内容 (FP1)」 ：用户提出的问题无法用现有文档回答。理想情况下，RAG 系统会回复类似 “抱歉，我不知道” 的消息。然而，对于缺乏明确答案的内容相关问题，系统可能会误导性地提供回复。 「错失顶尖文档 (FP2)」 ：问题的答案存在于文档中，但排名不够高，未被包含在返回给用户的结果中。虽然理论上所有文档都经过排名并用于后续步骤，但实际上只会返回排名前 K 的文档，K 值根据性能进行选择。 「不在上下文 - 整合策略限制 (FP3)」 ：包含答案的文档从数据库中检索出来，但未能整合到生成回复的上下文中。这种情况发生在返回大量文档时，会导致整合过程受阻，从而妨碍相关答案的检索。 「未提取 (FP4)」 ：答案存在于上下文中，但模型未能提取正确的信息。这种情况通常发生在上下文中存在大量噪声或冲突信息时。 「格式错误 (FP5)」 ：问题涉及提取特定格式的信息，例如表格或列表，但模型忽略了指令。 「特异性错误 (FP6)」 ：回复包含答案，但缺乏必要的特异性或过度特异，未能满足用户需求。这种情况发生在 RAG 系统设计者对给定问题预设了结果时，例如教师寻求教育内容。在这种情况下，除了答案之外还应提供特定的教育内容。特异性错误也出现在用户不确定如何措辞问题过于笼统时。 「不完整 (FP7)」 ：不完整的答案虽然准确，但缺少一些信息，即使这些信息存在于上下文中并且可以提取。例如，诸如“文档 A、B 和 C 中涵盖了哪些关键点？”这样的问题，不如将它们分开提问会更好。 下表总结了他们解决每个问题学到的经验教训。构建企业级 RAG 系统时，我们将牢记这些教训。 如何构建企业级 RAG 系统 现在我们已经了解了设计 RAG 系统时常遇到的常见问题，接下来我们将逐步讲解每个组件的设计需求和作用，以及构建这些组件的最佳实践。上面的 RAG 系统架构图提供了每个组件的使用位置和方式的上下文。 用户认证 这是整个系统的起点！在用户开始与聊天机器人互动之前，我们需要出于各种原因对用户进行身份验证。身份验证有助于确保安全性 和个性化，这对于企业系统来说是必不可少的。 「访问控制」：身份验证确保只有授权用户才能访问系统。它有助于控制谁可以与系统互动以及他们被允许执行哪些操作。 「数据安全」：保护敏感数据至关重要。用户身份验证可防止未经授权的个人访问机密信息，从而防止数据泄露和未经授权的数据操纵。 「用户隐私」：身份验证通过确保只有目标用户才能访问其个人信息和账户详细信息来帮助维护用户隐私。这对于建立用户信任至关重要。 「法律合规」：许多司法管辖区和行业都有法规和法律要求组织实施适当的用户身份验证来保护用户数据和隐私。遵守这些法规有助于避免法律问题和潜在惩罚。 「问责制」：身份验证通过将系统内的操作与特定用户账户关联来确保问责制。这对于审计和跟踪用户活动至关重要，有助于识别和解决任何安全事件或可疑行为。 「个性化和定制」：身份验证允许系统识别单个用户，从而实现个性化和定制用户体验。这可以包括定制内容、偏好和设置。 像 AWS Cognito 或 Firebase Authentication 之类的服务可以帮助您轻松地将用户注册和身份验证添加到移动和网络应用程序中。 输入护栏 防止有害或包含隐私信息的 用户输入 至关重要。最近的研究表明，劫持大型语言模型 (LLMs) 变得容易。这就是输入护栏发挥作用的地方。让我们来看看需要护栏的不同场景。 「匿名化」：输入护栏可以匿名化或编辑个人可识别信息 (PII)，例如姓名、地址或联系方式。这有助于保护隐私并防止恶意披露敏感信息的尝试。 「限制子字符串」：禁止某些子字符串或模式，这些子字符串或模式可能会被利用进行 SQL 注入、跨站点脚本 (XSS) 或其他注入攻击，从而防止安全漏洞或不需要的行为。 「限制主题」：为了限制讨论或输入与特定主题相关的内容，这些主题可能不当、冒犯或违反社区准则，因此过滤掉包含仇恨言论、歧视或色情内容的内容很重要。 「限制代码」：必须防止注入可执行代码，否则可能会破坏系统安全或导致代码注入攻击。 「限制语言」：验证文本输入是否使用正确的语言或脚本，以防止处理过程中出现潜在的误解或错误。 「检测提示注入」：减轻注入误导性或有害提示的尝试，这些提示可能以非预期方式操纵系统或影响大型语言模型的行为。 「限制令牌」：对用户输入强制执行最大令牌或字符限制有助于避免资源耗尽并防止拒绝服务 (DoS) 攻击。 「检测毒性」：实施毒性过滤器来识别和阻止包含有害或辱骂语言的输入。 为了保护您的 RAG 系统免受这些场景的影响，您可以利用 Meta 提供的 Llama Guard。您可以自己托管它，也可以使用 Sagemaker 等托管服务。但是，请不要指望它能完美地检测毒性内容。 查询重写器 一旦查询通过输入护栏，我们就会将其发送到查询重写器。有时候，用户查询可能含糊不清，或者需要上下文才能更好地理解用户的意图。查询重写是一种有助于解决此问题的技术。它涉及转换用户查询以提高清晰度、准确性和相关性。让我们来看看一些最常用的技术： 「基于历史记录重写」：这种方法中，系统利用用户的查询历史记录来理解对话的上下文并改进后续查询。例如，信用卡查询： 查询历史记录： 您有多少张信用卡？ 白金卡和金卡是否有年费？ 比较两者的功能。 基于用户查询历史记录，我们需要识别上下文的发展脉络，辨别用户查询之间的意图和关联，并生成与不断演变的上下文相符的查询。 重写后的查询：比较白金卡和金卡的功能。 「创建子查询」：由于检索问题，复杂查询可能难以回答。为了简化任务，查询会被分解成更具体的子查询。这有助于检索生成答案所需的正确上下文。LlamaIndex 将此称为子问题查询引擎。 例如，对于查询“比较白金卡和金卡的功能”，系统会为每个信用卡生成子查询，分别关注原始查询中提到的单个实体。 重写后的子查询： 白金信用卡的功能有哪些？ 金信用卡的功能有哪些？ 「创建相似查询」：为了提高检索相关文档的可能性，我们会根据用户输入生成类似的查询。这可以克服检索在语义或词汇匹配方面的限制。 如果用户询问信用卡的功能，系统会生成相关的查询。可以使用同义词、相关术语或领域知识来创建与用户意图相符的查询。 生成的相似查询： 我想知道白金信用卡。-> 告诉我白金信用卡的优点。 选择文本编码器需要考虑的因素 在选择文本编码器时，您需要决定使用私有编码器还是公共编码器。由于私有编码器易于使用，您可能会倾向于使用它们，但在这两种选择之间需要权衡一些具体的利弊。这是一个重要的决定，它将影响您系统的性能和延迟。 「查询成本」 确保语义搜索的流畅用户体验依赖于嵌入式 API 服务的高可用性。OpenAI 和类似的供应商提供可靠的 API，消除了托管管理的需要。然而，选择开源模型需要根据模型大小和延迟需求进行工程方面的投入。较小的模型（最多 1.1 亿参数）可以利用 CPU 实例托管，而较大的模型可能需要 GPU 服务来满足延迟要求。 「索引成本」 设置语义搜索涉及对文档进行索引，这会产生非平凡的成本。由于索引和查询共享相同的编码器，因此索引成本取决于所选择的编码器服务。为了方便服务重置或重新索引到替代向量数据库，建议单独存储嵌入向量。忽略此步骤将需要重新计算相同的嵌入向量。 「存储成本」 对于索引数百万个向量的应用程序，向量数据库的存储成本是一个重要因素。存储成本与维度线性扩展，OpenAI 在 1526 维度的嵌入向量产生最大的存储成本。要估计存储成本，请计算每个文档的平均单位（词组或句子）并进行外推。 「语言支持」 为了支持您的非英语语言，可以使用多语言编码器或将翻译系统与英语编码器结合使用。 「搜索延迟」 语义搜索的延迟与嵌入向量的维度成线性比例增长。为了尽量减少延迟，最好选择较低维度的嵌入向量。 「隐私」 像金融和医疗保健等敏感领域的严格数据隐私要求可能会使像 OpenAI 这样的服务变得不可行。 文档摄取 文档摄取系统管理着数据的处理和持久化。在索引过程中，每个文档都会被分成较小的块，然后使用嵌入模型转换为嵌入向量。然后将原始块和嵌入向量一起编入索引数据库。让我们看看文档摄取系统的组件。 「文档解析器」 文档解析器在主动从各种文档格式中提取结构化信息方面起着核心作用，尤其关注格式处理。这包括但不限于解析可能包含图像和表格的 PDF 文档。 「文档格式」 文档解析器必须能够熟练处理各种文档格式，例如 PDF、Word、Excel 等，以确保文档处理的可适应性。这涉及识别和管理嵌入内容，例如超链接、多媒体元素或注释，以提供文档的综合表示。 「表格识别」 识别和提取文档中的表格数据对于维护信息结构（尤其是在报告或研究论文中）至关重要。提取与表格相关的元数据，包括标题、行和列信息，可以增强对文档组织结构的理解。诸如表格转换器之类的模型可以用于此任务。 「图像识别」 光字符识别 (OCR) 应用于文档中的图像，以主动识别和提取文本，使其可以进行索引和后续检索。 「元数据提取」 元数据是指关于文档的附加信息，它不是文档主要内容的一部分。它包括作者、创建日期、文档类型、关键字等详细信息。元数据提供 valuable context 并帮助组织文档，并通过考虑元数据属性来提高搜索结果的相关性。可以使用 NLP/OCR 管道提取元数据，并将其作为特殊字段与文档一起索引。 「分块器」 您决定如何对长文本进行分词 (拆分) 可以决定嵌入向量质量和搜索系统的性能。如果块太小，则无法回答某些问题；如果块太长，则答案会包含生成的噪音。您可以利用摘要技术来减少噪音、文本大小、编码成本和存储成本。 分块是一个重要但经常被低估的主题。它可能需要类似于特征工程的领域专业知识。例如，针对 Python 代码库的拆块可能会使用 def/class 等前缀来完成。有关分块的更深入探讨，请阅读我们的博客文章。 索引器 顾名思义，索引器负责创建文档索引，该索引用作一种结构化数据结构（快速说三遍……）。索引器可以促进高效的搜索和检索操作。高效的索引对于快速准确地检索文档至关重要。它涉及将块或令牌映射到它们在文档集合中的对应位置。索引器在文档检索方面执行重要任务，包括创建索引以及添加、更新或删除文档。 索引器作为 RAG 系统的关键组件，面临着各种挑战和问题，这些问题会影响系统整体的效率和性能。 可扩展性问题 随着文档量的增长，维护高效和快速的索引变得具有挑战性。当系统难以处理越来越多的文档时，可能会出现可扩展性问题，从而导致更慢的索引和检索速度。 实时索引更新 在文档频繁添加、更新或删除的系统中，使索引保持实时更新可能具有挑战性。确保实时 API 和实时索引机制无缝运行而不影响系统性能是一项持续的挑战。 一致性和原子性 面对并发文档更新或修改时，实现一致性和原子性可能很复杂。确保即使在同时进行更改的情况下，索引更新也能维护数据完整性，这需要仔细的设计和实现。 优化存储空间 索引大量文档可能会导致大量存储需求。优化存储空间同时确保索引保持可访问和响应是一个持续的挑战，尤其是在存储成本成为关注问题的情况下。 安全和访问控制 实施适当的安全措施和访问控制以防止对索引进行未经授权的修改至关重要。确保只有授权用户或进程才能执行 CRUD 操作有助于保护文档存储库的完整性。 监控和维护 定期监控索引器的健康和性能至关重要。检测诸如索引失败、资源瓶颈或过时索引等问题需要健壮的监控和维护程序，以确保系统随着时间的推移顺利运行。 这些都是一些众所周知的软件工程难题，可以通过遵循良好的软件设计实践来解决。 数据存储 由于我们处理各种数据，因此我们需要针对每种数据采用专用的存储方式。对于每种存储类型及其特定用例，了解不同的注意事项至关重要。 嵌入向量 数据库类型：SQL/NoSQL 单独存储文档嵌入向量可以实现快速重新索引，而无需重新计算整个文档语料库的嵌入向量。此外，嵌入向量存储还可以充当备份，即使在系统故障或更新的情况下也能确保关键信息的保留。 文档 数据库类型：NoSQL 以原始格式存储文档对于持久化存储至关重要。这种原始格式作为各种处理阶段（例如索引、解析和检索）的基础。它还为未来的系统增强提供了灵活性，因为原始文档保持不变，可以根据需要进行重新处理。 聊天历史记录 数据库类型：NoSQL 存储聊天历史记录对于支持 RAG 系统的对话方面必不可少。聊天历史记录存储允许系统回忆用户之前的查询、回复和偏好，使其能够根据用户的独特上下文进行调整和定制未来的交互。这些历史数据是通过利用它们进行研究来改进机器学习系统的重要资源。 用户反馈 数据库类型：NoSQL/SQL 用户反馈通过 RAG 应用程序中的各种交互机制系统地收集。在大多数 LLM 系统中，用户可以使用顶/踩、星级评分和文本反馈提供反馈。这一系列用户见解作为一个宝贵的存储库，囊括了用户体验和感知，构成了持续系统增强的基础。 向量数据库 为语义搜索提供支持的向量数据库是 RAG 系统的关键检索组件。然而，选择合适的组件对于避免潜在问题至关重要。在选择过程中需要考虑几个 向量数据库因素。让我们来看看其中的一些。 召回率 vs. 延迟 在向量数据库中，优化召回率（相关结果的百分比）和延迟（返回结果的时间）需要进行权衡。Flat、HNSW（分层可导航小世界）、PQ（产品量化）、ANNOY 和 DiskANN 等不同索引在速度和召回率之间会做出不同的权衡。对您的数据和查询进行基准研究以做出明智的决策。 成本 具有托管解决方案的云原生数据库通常根据数据存储和查询量计费。对于拥有大量数据的组织来说，这种模式可以避免基础设施成本。主要考虑因素包括评估数据集增长、团队能力、数据敏感性以及了解托管云解决方案的成本影响。 自主托管 vs. 托管服务 另一方面，自托管为组织提供了对其基础设施的更大控制权，并且成本也可能更低。但是，它也伴随着管理和维护基础设施的责任，包括可扩展性、安全性和更新方面的考虑。 插入速度与查询速度 平衡插入速度和查询速度至关重要。寻找能够处理具有高插入速度要求的流式用例的供应商。但是，对于大多数组织而言，优先考虑查询速度更具相关性。评估高峰时段的向量插入速度查询延迟，以做出明智的决策。 内存 vs. 磁盘索引存储 在内存存储和磁盘存储之间进行选择涉及速度和成本的权衡。虽然内存存储提供高速性能，但某些用例需要存储大于内存的向量。内存映射文件等技术可以在不影响搜索速度的情况下扩展向量存储。DiskANN 中的 Vamana 等新索引承诺提供高效的内存外索引。 Full-Text search vs. Vector Hybrid search 来源：https://www.pinecone.io/learn/hybrid-search-intro/ 仅仅使用向量搜索可能不够适用于企业级应用程序。另一方面，混合搜索，即集成了密集和稀疏方法的搜索，需要额外的工作。实现密集向量索引、稀疏倒排索引和重新排序步骤是典型的。通过一个名为 alpha 的参数在 Pinecone、Weaviate 和 Elasticsearch 中调整密集和稀疏元素之间的平衡。 过滤 现实世界中的搜索查询通常涉及对元数据属性进行过滤。虽然预过滤搜索似乎是自然的，但可能会导致缺少相关结果。如果过滤后的搜索查询中被过滤的属性只占数据集的一小部分，后过滤搜索可能会出现问题。像 Weaviate 这样的自定义过滤搜索结合了预过滤和倒排索引分片以及 HNSW 索引分片的有效语义搜索。 提高检索效率的技术 最近的研究表明，大型语言模型(LLMs)很容易被无关的上下文所分散注意力，而且拥有大量上下文（检索到的 topK 文档）可能会因为LLMs的注意力模式而错过某些上下文。因此，利用相关和多样化的文档来提高检索是至关重要的。让我们看一些提高检索的已证实技术。 假设文档嵌入（HyDE） 我们可以使用 HyDE 技术来解决检索性能差的问题，特别是在处理短查询或不匹配查询时，这可能会使查找信息变得困难。HyDE 采用独特的方法，通过使用 GPT 等模型创建的假设性文档来解决这个问题。这些假设性文档捕获了重要的模式，但可能具有虚构或不正确的细节。然后，一个智能文本编码器将这个假设性文档转换成一个向量嵌入。这个嵌入有助于在集合中找到类似的真实文档，比查询的嵌入更好。 实验表明，HyDE 比其他高级方法效果更好，使其成为提升 RAG 系统性能的有用工具。 查询路由 当处理多个索引时，查询路由会带来优势，将查询定向到最相关的索引，以实现高效的检索。这种方法通过确保每个查询被定向到适当的索引来简化搜索过程，优化信息检索的准确性和速度。 在企业搜索的背景下，数据来自各种来源，如技术文档、产品文档、任务和代码仓库，查询路由成为一个强大的工具。例如，如果用户搜索与特定产品功能相关的信息，则可以将查询智能地路由到包含产品文档的索引，从而提高搜索结果的准确性。 重新排名器 当从编码器检索的结果不能提供最佳质量时，会使用重新排名器来增强文档排名。利用开源的仅编码器变压器（如BGE-large）进行跨编码器设置已成为常见做法。最近的仅解码器方法，如RankVicuna、RankGPT和RankZephyr，进一步提高了重新排名器的性能。 引入重新排名器有其好处，可以减少LLM在响应中的幻觉，并改善系统的跨域泛化。但是，它也存在缺点。复杂的重新排名器可能会增加延迟，因为计算开销大，影响实时应用程序。此外，部署高级重新排名器可能会消耗资源，需要仔细考虑性能提升与资源利用之间的平衡。 最大边际相关性（MMR） MMR是一种旨在增强响应中检索项的多样性、避免冗余的方法。与仅关注检索最相关项不同，MMR在相关性和多样性之间取得了平衡。它就像是在派对上向朋友介绍人。首先，根据朋友的喜好，确定最匹配的人。然后，寻找略有不同的人。这个过程一直持续，直到达到所需的介绍人数。MMR确保呈现出更多样化和相关的项目集，最大限度地减少了冗余。 自动截断 Weaviate 中的自动截断功能旨在通过检测具有相近分数的对象组来限制返回的搜索结果数量。它通过分析搜索结果的分数并识别这些值中的显著跳跃来工作，这可能表明从高度相关到不太相关的结果的过渡。 例如，考虑一个返回具有以下距离值的对象的搜索： [0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23]。 自动截断返回以下结果： autocut: 1: [0.1899, 0.1901, 0.191] autocut: 2: [0.1899, 0.1901, 0.191, 0.21, 0.215] autocut: 3: [0.1899, 0.1901, 0.191, 0.21, 0.215, 0.23] 6377b36750c946ae2ba7c75ec47fca036324a1ec-2054x800.webp Source: https://youtu.be/TRjq7t2Ms5I?si=D0z5sHKW4SMqMgSG&t=742 递归检索，又称为由小到大的检索技术，将较小的块嵌入以进行检索，同时返回更大的父上下文给语言模型进行综合。较小的文本块有助于更准确地进行检索，而较大的块则为语言模型提供了更丰富的上下文信息。这个连续的过程通过最初集中于较小、信息更密集的单元来优化检索的准确性，然后将它们高效地链接到更广泛的上下文父块以进行综合。 句子窗口检索 检索过程获取一个单独的句子，并返回该特定句子周围的文本窗口。句子窗口检索确保检索到的信息不仅准确，而且在语境上相关，提供了主要句子周围的全面信息。 生成器 现在我们已经讨论了所有的检索组件，让我们来谈谈生成器。这需要仔细考虑和权衡，主要是在自托管推断部署和私有 API 服务之间进行。这本身是一个大话题，我会简要提及，以避免让您感到不知所措。 API 考虑因素 在评估 LLMs 的 API 服务器时，优先考虑确保无缝集成和强大性能的功能至关重要。一个设计良好的 API 应该作为流行的 LLMs 的简单启动器，同时还要解决关键考虑因素，如生产就绪性、安全性和幻觉检测。值得注意的是，HuggingFace 的 TGI 服务器体现了这些原则的一套全面功能。让我们了解一下在 LLM 服务器中需要的一些最受欢迎的功能。 性能 高效的 API 必须优先考虑性能，以满足不同用户需求。张量并行性是一种在多个 GPU 上实现更快推断的功能，增强了整体处理速度。此外，持续批处理传入请求确保了总吞吐量的增加，有助于实现更响应迅速和可扩展的系统。使用位和字节以及 GPT-Q 进行量化进一步优化了 API，在各种用例中提高了效率。利用优化的变压器代码确保了在最流行的架构上无缝推断。 生成质量增强器 为了提高生成质量，API 应该包含能够转换输出的功能。对数处理器包括温度缩放、top-p、top-k 和重复惩罚，允许用户根据自己的偏好自定义输出。此外，停止序列提供了对生成的控制，使用户可以管理和优化内容生成过程。对数概率对幻觉检测至关重要，它作为一种额外的精炼层，确保生成的输出与预期的上下文一致，避免误导性信息。 安全性 API 的安全性至关重要，特别是在处理 LLMs 和企业用例时。安全张量权重加载是一个重要功能，通过防止未经授权的模型参数篡改来有助于模型的安全部署。此外，包含水印技术增加了一层额外的安全性，使得追踪和追责在 LLMs 的使用中成为可能。 用户体验 在用户体验领域，标记流是一种关键功能，用于实现无缝交互。利用服务器发送事件（SSE）进行标记流增强了 API 的实时响应性，为用户提供了更流畅和更具交互性的体验。这确保了用户可以逐步接收生成的内容，提高了 LLM 的整体参与度和可用性。 自托管推断 自托管推断涉及将 LLMs 部署到由云服务提供商（如 AWS、GCP 或 Azure）提供的服务器上。服务器的选择，例如 TGI、Ray 或 FastAPI，是一个关键决定，直接影响系统的性能和成本。考虑因素包括计算效率、部署便利性和与所选 LLM 的兼容性。 衡量 LLM 推断性能至关重要，Anyscale 的 LLMPerf 排行榜等排行榜至关重要。它根据关键性能指标，包括首个令牌的到达时间（TTFT）、令牌间延迟（ITL）和成功率，对推断提供者进行排名。负载测试和正确性测试对评估托管模型的不同特性至关重要。 在新方法中，Predibase 的 LoRAX 以一种创新的方式高效地提供了精细调整的 LLMs。它解决了使用共享 GPU 资源服务多个精细调整模型的挑战。 私有 API 服务 像 OpenAI、Fireworks、Anyscale、Replicate、Mistral、Perplexity 和 Together 这样的公司提供的 LLM API 服务提供了替代部署方法。了解它们的功能、定价模型和 LLM 性能指标至关重要。例如，OpenAI 的基于令牌的定价模型，区分输入和输出令牌，可以极大地影响使用 API 的总成本。在比较私有 API 服务与自托管 LLMs 的成本时，必须考虑 GPU 成本、利用率和可扩展性等因素。对于一些情况来说，速率限制可能是一个限制因素。 改进 RAG 的提示技术 存在许多用于改进 RAG 输出的提示技术。在我们的 RAG 掌握系列的第二部分中，我们深入探讨了前 5 种最有效的方法。许多这些新技术超越了 CoT（思维链）的性能。您还可以将它们组合起来，以最小化幻觉。 输出保护栏 输出保护栏的功能与其输入对应物类似，但专门设计用于检测生成的输出中的问题。它侧重于识别幻觉、竞争对手提及以及可能导致品牌损害的问题，作为 RAG 评估的一部分。其目标是防止生成不准确或伦理上可疑的信息，这些信息可能与品牌的价值观不符。通过积极监控和分析输出，这个保护栏确保生成的内容保持事实准确、符合道德标准，并与品牌的准则一致。 以下是一个可能会损害企业品牌的回复示例，但会被适当的输出保护栏屏蔽： 用户反馈 一旦生成并提供输出，从用户那里获得积极或消极的反馈是非常有帮助的。用户反馈对于改进 RAG 系统的推动力量非常重要，这是一个持续的过程，而不是一次性的努力。这不仅包括定期执行自动化任务，如重新索引和实验重新运行，还包括系统性地整合用户见解以实现实质性的系统增强。 系统改进中最具影响力的杠杆在于积极解决底层数据中的问题。RAG 系统应包括一个用于处理用户反馈和推动持续改进的迭代工作流程。 用户互动和反馈收集 用户与 RAG 应用进行互动，并利用诸如👍/👎或星级评价等功能提供反馈。这一多样化的反馈机制集合起来作为用户对系统性能的体验和感知的宝贵库存。 问题识别和诊断检查 收集反馈后，团队可以进行全面的分析，以识别可能性能不佳的查询。这涉及检查检索到的资源并仔细审查，以确定性能不佳是否源自检索、生成或底层数据源。 数据改进策略 一旦识别出问题，特别是那些根源于数据本身的问题，团队就可以战略性地制定计划来提升数据质量。这可能涉及纠正不完整的信息或重组组织不佳的内容。 评估和测试协议 在实施数据改进后，系统必须经过严格的评估，以前性能不佳的查询。从这些评估中获得的见解可以系统地整合到测试套件中，确保根据真实世界的交互进行持续的审查和完善。 通过积极参与用户在这一全面反馈循环中，RAG 系统不仅解决了通过自动化过程识别出的问题，还利用了用户体验的丰富性。 可观测性 建立 RAG 系统并不仅仅是将系统投入生产。即使具有健壮的防护措施和用于微调的高质量数据，模型在投入生产后仍需要进行持续监控。生成式人工智能应用程序除了标准指标如延迟和成本之外，还需要特定的LLM可观测性来检测和纠正幻觉、域外查询和链路失败等问题。现在让我们来看看LLM可观测性的支柱。 提示分析和优化 使用实时生产数据识别与提示相关的问题，并通过强大的评估机制迭代，以识别和解决幻觉等问题。 LLM应用的可追溯性 从像Langchain和LlamaIndex这样的常见框架中捕获LLM的追踪数据，以调试提示和步骤。 信息检索增强 排除故障并评估RAG参数，以优化对LLM性能至关重要的检索过程。 警报 如果系统行为与预期不符，例如错误增加、高延迟和幻觉等，即可收到警报。 首先和最重要的是，实时监控对于观察应用程序在生产环境中的性能、行为和整体健康状况至关重要。要密切关注 SLA 符合情况，并设置警报，以及时解决任何偏差。通过分析使用模式和资源消耗来有效跟踪运行LLM应用程序所涉及的成本，以帮助您进行成本优化。 Galileo 的 LLM Studio 提供了专门设计的LLM可观测性，以在用户投诉之前主动发出警报并立即采取纠正措施。Galileo 的防护指标旨在监控您模型的质量和安全性，涵盖基础、不确定性、真实性、语调、毒性、PII 等方面。这些指标先前用于评估和实验，现在可以无缝集成到监控阶段。 此外，您还可以灵活注册自定义指标，以定制监控过程以满足您的具体需求。利用从监控数据中生成的见解和警报，了解需要关注的潜在问题、异常情况或改进领域。这种全面的方法确保您的LLM应用程序在现实场景中高效安全地运行。 缓存 对于大规模运营的公司来说，成本可能成为障碍。缓存是一种在这种情况下节省资金的好方法。缓存涉及将提示及其对应的响应存储在数据库中，以便以后检索使用。这种战略性缓存机制使大型语言模型应用程序能够通过以下三个显着优势来加快和节省响应成本： 「增强生产推理」：缓存有助于在生产过程中更快速、更经济地进行推理。通过利用缓存的响应，某些查询可以实现接近于零的延迟，从而简化用户体验。 「加速开发周期」：在开发阶段，缓存被证明是一项福音，因为它消除了为相同的提示重复调用 API 的需要。这可以带来更快速、更经济的开发周期。 「数据存储」：一个存储所有提示的综合数据库的存在简化了大型语言模型的微调过程。利用存储的提示-响应对可以简化基于累积数据的模型优化。 如果您想要认真实施缓存，您可以利用 GPTCache 来缓存完全匹配和相似匹配的响应。它提供了诸如缓存命中率、延迟和召回率等值得的指标，这些指标可以洞察缓存的性能，从而实现持续优化以确保最佳效率。 多租户 SaaS 软件通常有多个租户，需要平衡简单性和隐私性。对于 RAG 系统的多租户，目标是构建一个不仅能有效地查找信息，而且还能尊重每个用户数据限制的系统。用更简单的术语来说，系统会隔离每个用户的交互，确保系统只会查看和使用针对该用户的相关信息。 构建多租户的一种简单方法是使用元数据。当我们将文档添加到系统时，我们在元数据中包含特定的用户信息。这样，每个文档都与特定用户相关联。当用户搜索时，系统会使用此元数据进行过滤，只显示与该用户相关的文档。然后，它会进行智能搜索以找到该用户最重要的信息。这种方法可以防止不同用户之间的私人信息混淆，从而保持每个人的数据安全和私密。 了解如何使用 Llamaindex 实现多租户。 总结 构建一个强大且可扩展的企业级 RAG 系统显然需要仔细协调互连的组件。从用户身份验证到输入护栏、查询重写、编码、文档摄取和检索组件（例如向量数据库和生成器），每个步骤都在塑造系统性能方面发挥着关键作用。 在不断发展的 RAG 系统领域，我们希望这份实用指南能帮助开发人员和领导者获得可操作的见解！ 如何学习AI大模型？ 作为一名热心肠的互联网老兵，我决定把宝贵的AI知识分享给大家。 至于能学习到多少就看你的学习毅力和能力了 。我已将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【保证100%免费】 一、全套AGI大模型学习路线 AI大模型时代的学习之旅：从基础到前沿，掌握人工智能的核心技能！ 二、640套AI大模型报告合集 这套包含640份报告的合集，涵盖了AI大模型的理论研究、技术实现、行业应用等多个方面。无论您是科研人员、工程师，还是对AI大模型感兴趣的爱好者，这套报告合集都将为您提供宝贵的信息和启示。 三、AI大模型经典PDF籍 随着人工智能技术的飞速发展，AI大模型已经成为了当今科技领域的一大热点。这些大型预训练模型，如GPT-3、BERT、XLNet等，以其强大的语言理解和生成能力，正在改变我们对人工智能的认识。 那以下这些PDF籍就是非常不错的学习资源。 四、AI大模型商业化落地方案 作为普通人，入局大模型时代需要持续学习和实践，不断提高自己的技能和认知水平，同时也需要有责任感和伦理意识，为人工智能的健康发展贡献力量。","source":"web","publishedAt":"2024-05-11T20:15:03+08:00"},{"id":"bocha-3","title":"ERP沙盘实训案例介绍.ppt","url":"https://max.book118.com/html/2016/1129/66642224.shtm","snippet":"————第五组 小组成员介绍 企业介绍: 蓝葛家具是一家以专业生产销售办公家具、高端别墅酒店家具产品为主,涉足国际贸易投资等领域的综合性现代化企业。 ERP是最前沿的企业管理模式,通过各模板之间数据的","source":"web","publishedAt":"2016-11-29T02:42:08+08:00"},{"id":"bocha-4","title":"AI学习指南RAG篇(14)-RAG企业级应用案例-CSDN博客","url":"https://yuzhaopeng.blog.csdn.net/article/details/145931623","snippet":"一、引言\nRAG(Retrieval-Augmented\nGeneration,检索增强生成)技术在企业级应用中展现出巨大的潜力和价值。通过结合检索和生成,RAG系统能够提供更准确、更相关的回答,满足","source":"web","publishedAt":"2025-03-15T06:30:00+08:00"},{"id":"bocha-7","title":"《管理信息系统》案例集.doc-文档可在线阅读","url":"https://max.book118.com/html/2014/0630/8923749.shtm","snippet":"《管理信息系统》案例集邵阳学院经济与管理系何宜军 目 录案例1 Dell电脑公司电子商务先锋 1案例2 海尔集团:信息化助力创造世界名牌 5案例3 美特斯邦威——从高效到整合 11案例4 战略信息系统","source":"web","publishedAt":"2017-09-09T22:22:17+08:00"},{"id":"bocha-8","title":"典型案例_企业服务平台","url":"http://www.hongdee.com.cn/hongdee20/vip_doc/22572660_0_0_1.html","snippet":"解决方案 专业创造价值,专注成就卓越 Professionalism creates value and focuses on excellence 典型案例 其他行业成功应用 其他行业成功应用 物流","source":"web","publishedAt":"2025-01-31T02:00:43+08:00"},{"id":"bocha-9","title":"(论文)ERP系统在企业成功实施和应用案例 - 道客巴巴","url":"https://www.doc88.com/p-4864434156072.html","snippet":"下载积分: 3000 内容提示: V al u e E n gin eer in g · 203 · E R P 系统在企业成功实施和应用案例 T he Successful Im plem enta","source":"web","publishedAt":"2015-07-06T23:33:23+08:00"},{"id":"onebound-0","title":"Beijing Tech and Entrepreneur Events July 4 &ndash; July 17","url":"https://mp.weixin.qq.com/s?src=3&timestamp=1768982936&ver=1&signature=JOkNLJn36Eoh1iPsWcz29LfNOobAXqXFY7c22CqQOu6qGX-94pWoYymoPal34JuELIklxI7qT8Z72rGvpBbE6D0H8ocDG1AbqbQ3fqJX7oGRATNer-kKoVmSh6E56X5hJF1R3YLYmor2Si5JLSU6d8*HebepZyu43*28E6Iu0v8=","snippet":"Enterprise Service Innovation Conference 2016ESIC 2016企业服... based on some existing successful case studies, will invite senior ...","source":"wechat"},{"id":"onebound-1","title":"【论文速递】MIS Quarterly, Volume 46, Issue 3, September 2022","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=KGy0-5mrJTL4BVtJS3foU6ca9IYFUSqP3hM8-InG40M45tK0QTHA3cbs5sE7J4JLfIo0gJO3Mg2MHaF76y71yB6PpCxCxmTnwJNo1YQc2milyGoHSteLvmYvPLGNM0zM&new=1","snippet":"Enterprise Systems and M&amp;A Outcomes for Acquirers and ...  three detailed case studies that compare the counterfactual ...","source":"wechat"},{"id":"onebound-2","title":"人工智能学术速递[10.7]","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=jgcfr1HkwPGz0XBxmxyW*sQeO4t140YttoIpMRP0WT4TEm6FPP57oU-Z9LnN99tI15TlnngvbA37KHncFwllANsJzBu8RB*ymvUDNkCg1TYgmjgluqNUxBQ-urThTpma&new=1","snippet":"System via Simulated Trading标题:QuantAgents:通过模拟交易... principles and pharmacovigilance case studies标题:人工智能用...","source":"wechat"},{"id":"onebound-3","title":"人工智能学术速递[10.14]","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=jgcfr1HkwPGz0XBxmxyW*sQeO4t140YttoIpMRP0WT7RdKTpqQpfjLsdWeVxqy*tpiE1dWCn37TXKRd8zhQ1YiRyrpyV*MAcmjnMgGL8nxlBGxMCmdNmTWp5pHVgKQw3&new=1","snippet":"RAG-Pull:对代码生成的RAG系统的不可感知攻击链接:https://... Case studies in software agent design标题:我们如何评估人机互...","source":"wechat"},{"id":"onebound-4","title":"2025 OCP Global Summit会议资料分享 (全部400+演讲)","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=12XGsFdCbjzneIajfzEGKPHeacMBBPRl7PDRwME3H-HuqORbK*jqgw7svxAnECMzMqs9AEdx67oiRdoHZ8yabZ2hBEtpIA3NA9eoKBtXmpJ4Q6it1kKcOD80aVyTit-1&new=1","snippet":"Real-World Case Studies from the OCP CommunityUsing MGX ... Enterprise: Lessons from Managing Heterogeneous Infrastructure...","source":"wechat"},{"id":"onebound-5","title":"ARXIV日报 | [2025年10月21日] 多智能体论文汇总","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=FVDJMtQLQ4dpGgMGdYJVPE*AlWNde-ab65pH6*Y4uSugMV3vQC8D*uc6YY*Qh3ZImCs-9zERiMGqESS7wovEGAPF3iRnDzoXSYz3PsxhMhH6RKXVBig3iTh-uFW6HI7j&new=1","snippet":"We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query ...","source":"wechat"},{"id":"onebound-6","title":"人工智能,计算机视觉,机器学习, 图像和视频处理论文速递--(2026-01-16)","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=6L45TIRuVgGWk6NQLHd-lJwl-EwsrKlAxOe-E9XgSGooHqc8tFvBRQkIgbKyJOZzIwZS14YH-WnnAYJtHN3mC1hH1OYBym26ShH3hulkUL2jI5J2pjHJOX-84HUH-9E6&new=1","snippet":" a case study that involved examining ten selected privacy policies ... 当前的检索增强生成(RAG)系统尝试使用一种生硬的工具来解决...","source":"wechat"},{"id":"onebound-7","title":"人工智能,计算机视觉,机器学习, 图像和视频处理论文速递--(2026-01-13)","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=6L45TIRuVgGWk6NQLHd-lJwl-EwsrKlAxOe-E9XgSGod9-J11C-Nzxp0gSot81wcaES*JAwmcjADDwCI7YhiWwFWPFrxFyh7iNL5H3kRM1*05MaJfGlQQ62C4h8B8Oht&new=1","snippet":"RAG 实现了最高准确率,达到 95.64%,而 Gemma3-1B 使用 ... -case error bound of our dual-stage NVFP4 quantization is ...","source":"wechat"},{"id":"onebound-8","title":"《旅游学刊》| 韦鸣秋等:旅游公共服务价值共创:概念模型、驱动因素与行为过程&mdash;&mdash;以杭州市社会资源国际访问点为例","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=FDewNKR8b4U80FPXPnaDqyQ-b5FhtoDVx0f724J9-rY762nXqI1TXC8vZ96uX3lOYCzMbPeleeXK6FmD1z3g46jiYfDZD7BlCKhwslEk54gR77UdUzXMTG7xhHMTkefd&new=1","snippet":" integration from the service eco-system,which is led by government and participated by tourism enterprise,resident and tourist. ...","source":"wechat"},{"id":"onebound-9","title":"新刊速递 |《Economics Letters》(经济学快报) 2023年10月刊 目录 及 内容提要","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982936&ver=6493&signature=eGsLXV644aHCSMqTs-22OLJmE0as5TzEicSZXrmnr6xJOUzTizAOxnCE2p8ekfJN3G2g8*DMmBPwoHHJNzwlX45BjLBHpg7YC7yMyajBFEJV1qEbyXh2NE6sQHxyw8xw&new=1","snippet":"How does social credit system constructions affect corporate carbon emissions? Empirical evidence from Chinese listed companies内...","source":"wechat"},{"id":"bocha-0","title":"高级RAG技术第1部分：数据处理","url":"https://cloud.tencent.com.cn/developer/article/2446232","snippet":"点火三周高级RAG技术第1部分：数据处理原创关注作者腾讯云开发者社区文档建议反馈控制台登录/注册首页学习活动专区圈层工具MCP广场文章/答案/技术大牛搜索搜索关闭发布点火三周社区首页 >专栏 >高级RAG技术第1部分：数据处理高级RAG技术第1部分：数据处理原创点火三周关注发布于 2024-08-19 22:41:54发布于 2024-08-19 22:41:548630举报文章被收录于专栏：Elastic Stack专栏Elastic Stack专栏最近的论文《搜索增强生成中的最佳实践》通过实证研究评估了各种增强RAG技术的效果，旨在汇聚一套RAG的最佳实践。Wang Pipeline由Wang及其同事推荐的RAG管道。我们将实现一些这些最佳实践，特别是那些旨在提高搜索质量的技术（句子分块、HyDE、反向打包）。为了简洁起见，我们将省略那些专注于提高效率的技术（查询分类和摘要生成）。我们还将实现一些未涉及但我个人认为有用且有趣的技术（元数据包含、复合多字段嵌入、查询扩展）。最后，我们将进行一个简短的测试，以查看我们的搜索结果和生成的答案是否比基线有所改进。让我们开始吧！概述RAG旨在通过从外部知识库中检索信息来增强LLM（大语言模型）的生成答案。通过提供领域特定的信息，LLM可以快速适应其训练数据范围之外的用例；这比微调便宜得多，也更容易保持最新。改善RAG质量的措施通常集中在两个方面：提高知识库的质量和清晰度。改进搜索查询的覆盖范围和具体性。这两种措施将提高LLM访问相关事实和信息的几率，从而减少幻觉或依赖其自身可能过时或无关的知识。这些方法的多样性难以在几句话中澄清。让我们直接进入实现，以便更清楚地理解。Han Pipeline图1：作者使用的RAG管道。目录设置文档的摄取、处理和嵌入数据摄取句子级别、基于令牌的分块元数据包含与生成复合多字段嵌入附录定义猫咪休息设置所有代码可以在 Searchlabs仓库中找到。首先，你需要以下内容：一个Elastic云部署一个LLM API - 我们在此笔记本中使用的是Azure OpenAI上的GPT-4o部署Python版本3.12.4或更高版本我们将从main.ipynb笔记本运行所有代码。继续git clone这个仓库，导航到supporting-blog-content/advanced-rag-techniques，然后运行以下命令：代码语言：bash复制# 创建一个名为'rag_env'的新虚拟环境 python -m venv rag_env # 激活虚拟环境（对于基于Unix的系统） source rag_env/bin/activate # （对于Windows） .\\rag_env\\Scripts\\activate # 安装requirements.txt中列出的包 pip install -r requirements.txt完成后，创建一个.env文件并填写以下字段（参考.env.example）。感谢我的合著者Claude-3.5的有用评论。代码语言：bash复制# Elastic Cloud: 在Elastic Cloud控制台的“Deployment”页面找到 ELASTIC_CLOUD_ENDPOINT=\"\" ELASTIC_CLOUD_ID=\"\" # Elastic Cloud: 在部署设置期间或在“Security”设置中创建 ELASTIC_USERNAME=\"\" ELASTIC_PASSWORD=\"\" # Elastic Cloud: 在Kibana或通过API创建的索引名称 ELASTIC_INDEX_NAME=\"\" # Azure AI Studio: 在Azure OpenAI资源的“Keys and Endpoint”部分找到 AZURE_OPENAI_KEY_1=\"\" AZURE_OPENAI_KEY_2=\"\" AZURE_OPENAI_REGION=\"\" AZURE_OPENAI_ENDPOINT=\"\" # Azure AI Studio: 在Azure OpenAI资源的“Deployments”部分找到 AZURE_OPENAI_DEPLOYMENT_NAME=\"\" # 使用BAAI/bge-small-en-v1.5，因为我认为它在资源效率和性能之间取得了良好的平衡。 HUGGINGFACE_EMBEDDING_MODEL=\"BAAI/bge-small-en-v1.5\"接下来，我们将选择要摄取的文档，并将其放置在documents文件夹中。对于本文，我们将使用Elastic N.V. 2023年年度报告。这是一个相当具有挑战性和密集的文档，非常适合压力测试我们的RAG技术。Han PipelineElastic 2023年年度报告现在一切准备就绪，让我们开始进行摄取。打开main.ipynb并执行前两个单元格以导入所有包并初始化所有服务。返回顶部文档的摄取、处理和嵌入数据摄取个人注释：LlamaIndex的便利性让我惊叹不已。在没有LLMs和LlamaIndex的旧时代，摄取各种格式的文档是一个痛苦的过程，需要从各处收集晦涩的包。现在，它只需一个函数调用。真是太神奇了。SimpleDirectoryReader将加载directory_path中的所有文档。对于.pdf文件，它返回一个文档对象列表，我将其转换为Python字典，因为我发现它们更容易处理。代码语言：python复制# llamaindex_processor.py from llama_index.core import SimpleDirectoryReader class LlamaIndexProcessor: def __init__(self): pass def load_documents(self, directory_path): '''加载目录中的所有文档''' reader = SimpleDirectoryReader(input_dir=directory_path) return reader.load_data() # main.ipynb llamaindex_processor = LlamaIndexProcessor() documents = llamaindex_processor.load_documents('./documents/') documents = [dict(doc_obj) for doc_obj in documents]每个字典包含text字段中的关键内容。它还包含一些有用的元数据，例如页码、文件名、文件大小和类型。代码语言：python复制{ 'id_': '5f76f0b3-22d8-49a8-9942-c2bbab14f63f', 'metadata': { 'page_label': '5', 'file_name': 'Elastic_NV_Annual-Report-Fiscal-Year-2023.pdf', 'file_path': '/Users/han/Desktop/Projects/truckasaurus/documents/Elastic_NV_Annual-Report-Fiscal-Year-2023.pdf', 'file_type': 'application/pdf', 'file_size': 3724426, 'creation_date': '2024-07-27', 'last_modified_date': '2024-07-27' }, 'text': '目录\\n页码\\n第一部分\\n项目1. 业务 3\\n15 项目1A. 风险因素\\n项目1B. 未解决的员工意见 48\\n项目2. 物业 48\\n项目3. 法律诉讼 48\\n项目4. 矿山安全披露 48\\n第二部分\\n项目5. 登记人普通股的市场、相关股东事项和发行人股票购买 49\\n项目6. [保留] 49\\n项目7. 财务状况和经营成果的管理层讨论与分析 50\\n项目7A. 关于市场风险的定量和定性披露 64\\n项目8. 财务报表和补充数据 66\\n项目9. 关于会计和财务披露的会计师变更和分歧 100\\n100\\n101 项目9A. 控制和程序\\n项目9B. 其他信息\\n项目9C. 关于防止检查的外国司法管辖区的披露 101\\n第三部分\\n102\\n102\\n102\\n102 项目10. 董事、高级管理人员和公司治理\\n项目11. 高级管理人员薪酬\\n项目12. 某些受益所有人和管理层的证券持有情况和相关股东事项\\n项目13. 某些关系和相关交易及董事独立性\\n项目14. 主要会计师费用和服务 102\\n第四部分\\n103\\n105 项目15. 附件和财务报表附表\\n项目16. 10-K表格摘要\\n签名 106\\ni', ... }返回顶部句子级别、基于令牌的分块首先，我们需要将文档减少到标准长度的块（以确保一致性和可管理性）。嵌入模型有唯一的令牌限制（它们可以处理的最大输入大小）。令牌是模型处理的基本文本单位。为了防止信息丢失（截断或遗漏内容），我们应提供不超过这些限制的文本（通过将较长的文本拆分为较小的段）。分块对性能有显著影响。理想情况下，每个块都应代表一个自包含的信息块，捕捉到单个主题的上下文信息。分块方法包括基于词汇的分块，其中文档按词数拆分，以及语义分块，它使用LLM识别逻辑断点。基于词汇的分块便宜、快速且简单，但有可能拆分句子，从而破坏上下文。语义分块变得缓慢且昂贵，特别是如果你处理像116页的Elastic年度报告这样的文档。让我们选择一种折中的方法。句子级分块仍然简单，但比基于词汇的分块更有效地保留上下文，同时成本和速度显著降低。此外，我们将实现一个滑动窗口，以捕捉周围的一些上下文，缓解拆分段落的影响。代码语言：python复制# chunker.py import uuid import re class Chunker: def __init__(self, tokenizer): self.tokenizer = tokenizer def split_into_sentences(self, text): \"\"\"将文本拆分成句子。\"\"\" return re.split(r'(?<=[.!?])\\s+', text) def sentence_wise_tokenized_chunk_documents(self, documents, chunk_size=512, overlap=20, min_chunk_size=50): ''' 1. 将文本拆分成句子。 2. 使用提供的分词器方法进行分词。 3. 构建最多chunk_size限制的块。 4. 基于令牌创建重叠 - 保留上下文。 5. 只保留符合最小令牌大小要求的块。 ''' chunked_documents = [] for doc in documents: sentences = self.split_into_sentences(doc['text']) tokens = [] sentence_boundaries = [0] # 分词所有句子并跟踪句子边界 for sentence in sentences: sentence_tokens = self.tokenizer.encode(sentence, add_special_tokens=True) tokens.extend(sentence_tokens) sentence_boundaries.append(len(tokens)) # 创建块 chunk_start = 0 while chunk_start < len(tokens): chunk_end = chunk_start + chunk_size # 找到块中合适的最后一个完整句子 sentence_end = next((i for i in sentence_boundaries if i > chunk_end), len(tokens)) chunk_end = min(chunk_end, sentence_end) # 创建块 chunk_tokens = tokens[chunk_start:chunk_end] # 检查块是否符合最小大小要求 if len(chunk_tokens) >= min_chunk_size: # 为此块创建一个新的文档对象 chunk_doc = { 'id_': str(uuid.uuid4()), 'chunk': chunk_tokens, 'original_text': self.tokenizer.decode(chunk_tokens), 'chunk_index': len(chunked_documents), 'parent_id': doc['id_'], 'chunk_token_count': len(chunk_tokens) } # 从原始文档复制所有其他字段 for key, value in doc.items(): if key != 'text' and key not in chunk_doc: chunk_doc[key] = value chunked_documents.append(chunk_doc) # 移动到下一个块开始，考虑重叠 chunk_start = max(chunk_start + chunk_size - overlap, chunk_end - overlap) return chunked_documents # main.ipynb # 初始化嵌入模型 HUGGINGFACE_EMBEDDING_MODEL = os.environ.get('HUGGINGFACE_EMBEDDING_MODEL') embedder = EmbeddingModel(model_name=HUGGINGFACE_EMBEDDING_MODEL) # 初始化分块器 chunker = Chunker(embedder.tokenizer)Chunker类接收嵌入模型的分词器来编码和解码文本。我们现在将构建每个512个令牌的块，重叠20个令牌。为此，我们将文本拆分成句子，对这些句子进行分词，然后将分词后的句子添加到当前块中，直到无法再添加而不超过令牌限制。最后，将句子解码回原始文本进行嵌入，并将其存储在名为original_text的字段中。块存储在名为chunk的字段中。为了减少噪音（即无用的文档），我们将丢弃任何小于50个令牌的文档。让我们在我们的文档上运行它：代码语言：python复制chunked_documents = chunker.sentence_wise_tokenized_chunk_documents(documents, chunk_size=512)并得到类似这样的文本块：代码语言：python复制print(chunked_documents[4]['original_text']) [CLS] the aggregate market value of the ordinary shares held by non - affiliates of the registrant, based on the closing price of the shares of ordinary shares on the new york stock exchange on october 31, 2022 ( the last business day of the registrant ’ s second fiscal quarter ), was approximately $ 6. 1 billion. [SEP] [CLS] as of may 31, 2023, the registrant had 97, 390, 886 ordinary shares, par value €0. 01 per share, outstanding. [SEP] [CLS] documents incorporated by reference portions of the registrant ’ s definitive proxy statement relating to the registrant ’ s 2023 annual general meeting of shareholders are incorporated by reference into part iii of this annual ......返回顶部元数据包含与生成我们已经对文档进行了分块。现在是时候丰富数据了。我想生成或提取额外的元数据。这些额外的元数据可以用于影响和增强搜索性能。我们将定义一个DocumentEnricher类，其作用是接收一个文档列表（Python字典）和一个处理函数列表。这些函数将在文档的original_text列上运行，并将其输出存储在新字段中。首先，我们使用TextRank提取关键短语。TextRank是一种基于图的算法，通过根据单词之间的关系对它们的重要性进行排序，从文本中提取关键短语和句子。接下来，我们使用GPT-4o生成潜在问题。最后，我们使用Spacy提取实体。由于每个文件的代码都相当冗长且复杂，我将在这里避免重复。如果你有兴趣，文件在下面的代码示例中标记。让我们运行数据丰富化：好的，我会根据你的需求对这篇文章进行优化，使其更加适合初学者阅读，并确保表达清晰流畅。代码语言：python复制# documentenricher.py from tqdm import tqdm class DocumentEnricher: def __init__(self): pass def enrich_document(self, documents, processors, text_col='text'): for doc in tqdm(documents, desc=\"Enriching documents using processors: \" + str(processors)): for (processor, field) in processors: metadata = processor(doc[text_col]) if isinstance(metadata, list): metadata = '\\n'.join(metadata) doc.update({field: metadata}) # main.ipynb # 初始化处理器类 nltkprocessor = NLTKProcessor() # nltk_processor.py entity_extractor = EntityExtractor() # entity_extractor.py gpt4o = LLMProcessor(model='gpt-4o') # llm.py # 初始化文档增强器 documentenricher = DocumentEnricher() # 在文档中创建新的字段 - 这些是处理器函数的输出。 processors = [ (nltkprocessor.textrank_phrases, \"keyphrases\"), (gpt4o.generate_questions, \"potential_questions\"), (entity_extractor.extract_entities, \"entities\") ] # .enrich_document() 将会直接修改 chunked_docs。 # 为了查看结果，我们将在接下来的几个单元中打印 chunked_docs！ documentenricher.enrich_document(chunked_docs, text_col='original_text', processors=processors)让我们看看结果：TextRank 提取的关键词这些关键词代表了段落的核心主题。如果查询与网络安全有关，这段内容的评分将会提高。代码语言：javascript复制print(chunked_documents[25]['keyphrases']) 'elastic agent stop', 'agent stop malware', 'stop malware ransomware', 'malware ransomware environment', 'ransomware environment wide', 'environment wide visibility', 'wide visibility threat', 'visibility threat detection', 'sep cl key', 'cl key feature'GPT-4o 生成的潜在问题这些潜在问题可能会直接匹配用户查询，从而提高评分。我们提示 GPT-4o 生成可以用当前段落信息回答的问题。代码语言：javascript复制print(chunked_documents[25]['potential_questions']) 1. Elastic Agent 在网络安全方面的主要功能是什么？ 2. 描述 Logstash 如何在 IT 环境中贡献数据管理。 3. 列出并解释文档中提到的 Logstash 的关键特性。 4. Elastic Agent 如何增强威胁检测中的环境可见性？ 5. Logstash 提供哪些超越简单数据收集的功能？ 6. 文档中如何建议 Elastic Agent 阻止恶意软件和勒索软件？ 7. 能否识别 Elastic Agent 和 Logstash 在集成环境中的功能关系？ 8. Elastic Agent 的高级威胁检测能力对组织安全政策有何影响？ 9. 比较和对比 Elastic Agent 和 Logstash 的描述功能。 10. Logstash 的集中收集能力如何支持 Elastic Agent 的威胁检测能力？Spacy 提取的实体这些实体类似于关键词，但捕捉组织和个人的名字，而关键词提取可能会遗漏这些信息。代码语言：javascript复制print(chunked_documents[29]['entities']) 'appdynamics', 'apm data', 'azure sentinel', 'microsoft', 'mcafee', 'broadcom', 'cisco', 'dynatrace', 'coveo', 'lucidworks'返回顶部复合多字段嵌入现在我们已经用额外的元数据丰富了文档，可以利用这些信息创建更强大和上下文感知的嵌入。让我们回顾一下当前的处理进展。我们在每个文档中有四个感兴趣的字段。代码语言：javascript复制{ \"chunk\": \"...\", \"keyphrases\": \"...\", \"potential_questions\": \"...\", \"entities\": \"...\" }每个字段代表了文档上下文的不同视角，可能突出 LLM 需要关注的关键区域。Han Pipeline元数据增强流程计划是对每个字段进行嵌入，然后创建这些嵌入的加权和，称为复合嵌入。希望这个复合嵌入能使系统更加上下文感知，并引入另一个可调超参数以控制搜索行为。首先，让我们对每个字段进行嵌入，并使用我们在 main.ipynb 中定义的嵌入模型更新每个文档。代码语言：javascript复制# 在 embedding_model.py 中定义的嵌入模型 embedder = EmbeddingModel(model_name=HUGGINGFACE_EMBEDDING_MODEL) cols_to_embed = ['keyphrases', 'potential_questions', 'entities'] embedding_cols = [] for col in cols_to_embed: # 处理文本输入 embedding_col = embedder.embed_documents_text_wise(chunked_documents, text_field=col) embedding_cols.append(embedding_col) # 处理 token 输入 embedding_col = embedder.embed_documents_token_wise(chunked_documents, token_field=\"chunk\") embedding_cols.append(embedding_col)每个嵌入函数返回嵌入的字段，该字段是原始输入字段加上 _embedding 后缀。现在让我们定义复合嵌入的权重：代码语言：javascript复制embedding_cols = [ 'keyphrases_embedding', 'potential_questions_embedding', 'entities_embedding', 'chunk_embedding' ] combination_weights = [ 0.1, 0.15, 0.05, 0.7 ]权重允许你根据使用情况和数据质量为每个组件分配优先级。直观上，这些权重的大小取决于每个组件的语义价值。由于 chunk 文本本身最为丰富，我分配了 70% 的权重。因为实体是最小的，只是组织或个人名称的列表，所以我分配了 5% 的权重。这些值的精确设置需要根据具体使用情况进行实证确定。最后，让我们编写一个函数来应用这些权重，并创建我们的复合嵌入。同时删除所有组件嵌入以节省空间。代码语言：javascript复制from tqdm import tqdm def combine_embeddings(objects, embedding_cols, combination_weights, primary_embedding='primary_embedding'): # 确保权重数量与嵌入列数量匹配 assert len(embedding_cols) == len(combination_weights), \"嵌入列数量必须与权重数量匹配\" # 归一化权重使其总和为 1 weights = np.array(combination_weights) / np.sum(combination_weights) for obj in tqdm(objects, desc=\"Combining embeddings\"): # 初始化复合嵌入 combined = np.zeros_like(obj[embedding_cols[0]]) # 计算加权和 for col, weight in zip(embedding_cols, weights): combined += weight * np.array(obj[col]) # 将新的复合嵌入添加到对象中 obj.update({primary_embedding: combined.tolist()}) # 删除原始嵌入列 for col in embedding_cols: obj.pop(col, None) combine_embeddings(chunked_documents, embedding_cols, combination_weights)至此，我们完成了文档处理。现在我们有一个文档对象列表，它们的结构如下：代码语言：javascript复制{ 'id_': '7fe71686-5cd0-4831-9e79-998c6dbeae0c', 'chunk': [2312, 14613, ...], 'original_text': 'if an emerging growth company, indicate by check mark if the registrant has elected not to use the extended ...', 'chunk_index': 3, 'chunk_token_count': 399, 'metadata': { 'page_label': '3', 'file_name': 'Elastic_NV_Annual-Report-Fiscal-Year-2023.pdf', ... }, 'keyphrases': 'sep cl unk\\ncheck mark registrant\\ncl unk indicate\\nunk indicate check\\nindicate check mark\\nprincipal executive office\\naccelerate filer unk\\ncompany unk emerge\\nunk emerge growth\\nemerge growth company', 'potential_questions': '1. What are the different types of registrant statuses mentioned in the document?\\n2. Under what section of the Sarbanes-Oxley Act must registrants file a report on the effectiveness of their internal ...', 'entities': 'the effectiveness of\\nsection 13\\nSEP\\nUNK\\nsection 21e\\n1934\\n1933\\nu. s. c.\\nsection 404\\nsection 12\\nal', 'primary_embedding': [-0.3946287803351879, -0.17586839850991964, ...] }索引到 Elastic让我们将文档批量上传到 Elasticsearch。为此，我早在 elastic_helpers.py 中定义了一组 Elastic 辅助函数。这是一段很长的代码，所以我们只看函数调用。es_bulk_indexer.bulk_upload_documents 适用于任何字典对象列表，利用 Elasticsearch 的动态映射。代码语言：javascript复制# 初始化 Elasticsearch ELASTIC_CLOUD_ID = os.environ.get('ELASTIC_CLOUD_ID') ELASTIC_USERNAME = os.environ.get('ELASTIC_USERNAME') ELASTIC_PASSWORD = os.environ.get('ELASTIC_PASSWORD') ELASTIC_CLOUD_AUTH = (ELASTIC_USERNAME, ELASTIC_PASSWORD) es_bulk_indexer = ESBulkIndexer(cloud_id=ELASTIC_CLOUD_ID, credentials=ELASTIC_CLOUD_AUTH) es_query_maker = ESQueryMaker(cloud_id=ELASTIC_CLOUD_ID, credentials=ELASTIC_CLOUD_AUTH) # 定义索引名称 index_name = os.environ.get('ELASTIC_INDEX_NAME') # 创建索引并批量上传 index_exists = es_bulk_indexer.check_index_existence(index_name=index_name) if not index_exists: logger.info(f\"Creating new index: {index_name}\") es_bulk_indexer.create_es_index(es_configuration=BASIC_CONFIG, index_name=index_name) success_count = es_bulk_indexer.bulk_upload_documents( index_name=index_name, documents=chunked_documents, id_col='id_', batch_size=32 )前往 Kibana 验证所有文档已被索引。应该有 224 个文档。对于如此大的文档来说，这个结果还不错！Han Pipeline在 Kibana 中索引的年度报告文档原创声明：本文系作者授权腾讯云开发者社区发表，未经许可，不得转载。如有侵权，请联系 cloudcommunity@tencent.com 删除。Elasticsearch Service原创声明：本文系作者授权腾讯云开发者社区发表，未经许可，不得转载。如有侵权，请联系 cloudcommunity@tencent.com 删除。Elasticsearch Service评论登录后参与评论0 条评论热度最新登录 后参与评论推荐阅读目录概述目录设置文档的摄取、处理和嵌入数据摄取句子级别、基于令牌的分块元数据包含与生成TextRank 提取的关键词GPT-4o 生成的潜在问题Spacy 提取的实体复合多字段嵌入索引到 Elastic相关产品与服务Elasticsearch Service腾讯云 Elasticsearch Service（ES）是集文本搜索+向量搜索+AI 能力于一体的云端智能混合搜索服务，提供集群托管和 Serverless 两种模式，助您高效构建多样化搜索（文本/语义/图片/视频/地理空间等）、RAG 知识问答、日志分析、运维监控等应用。产品介绍产品文档ES特惠专场，新客首购1折起！ 领券社区技术文章技术问答技术沙龙技术视频学习中心技术百科技术专区活动自媒体同步曝光计划邀请作者入驻自荐上首页技术竞赛圈层腾讯云最具价值专家腾讯云架构师技术同盟腾讯云创作之星腾讯云TDP关于社区规范免责声明联系我们友情链接MCP广场开源版权声明腾讯云开发者扫码关注腾讯云开发者领取腾讯云代金券热门产品域名注册云服务器区块链服务消息队列网络加速云数据库域名解析云存储视频直播热门推荐人脸识别腾讯会议企业云CDN加速视频通话图像分析MySQL 数据库SSL 证书语音识别更多推荐数据安全负载均衡短信文字识别云点播大数据小程序开发网站监控数据迁移Copyright © 2013 - 2026 Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 深圳市腾讯计算机系统有限公司 ICP备案/许可证号：粤B2-20090059 粤公网安备44030502008569号腾讯云计算（北京）有限责任公司 京ICP证150476号 | 京ICP备11018762号问题归档专栏文章快讯文章归档关键词归档开发者手册归档开发者手册 Section 归档Copyright © 2013 - 2026 Tencent Cloud.All Rights Reserved. 腾讯云 版权所有登录 后参与评论000推荐","source":"web","publishedAt":"2024-08-20T06:41:54+08:00"},{"id":"bocha-1","title":"Advanced RAG techniques part 1: Data processing - Elasticsearch Labs","url":"https://www.elastic.co/search-labs/blog/advanced-rag-techniques-part-1","snippet":"From vector search to powerful REST APIs, Elasticsearch offers developers the most extensive search toolkit. Dive into sample notebooks on GitHub to try something new. You can also start your free trial or run Elasticsearch locally today.This is Part 1 of our exploration into Advanced RAG Techniques. Click here for Part 2!The recent paper Searching for Best Practices in Retrieval-Augmented Generation empirically assesses the efficacy of various RAG enhancing techniques, with the goal of converging on a set of best-practices for RAG.The RAG pipeline recommended by Wang and colleagues.We'll implement a few of these proposed best-practices, namely the ones which aim to improve the quality of search (Sentence Chunking, HyDE, Reverse Packing).For brevity, we will omit those techniques focused on improving efficiency (Query Classification and Summarization).We will also implement a few techniques that were not covered, but which I personally find useful and interesting (Metadata Inclusion, Composite Multi-Field Embeddings, Query Enrichment).Finally, we'll run a short test to see if the quality of our search results and generated answers has improved versus the baseline. Let's get to it!RAG overviewRAG aims to enhance LLMs by retrieving information from external knowledge bases to enrich generated answers. By providing domain-specific information, LLMs can be quickly adapted for use cases outside the scope of their training data; significantly cheaper than fine-tuning, and easier to keep up-to-date.Measures to improve the quality of RAG typically focus on two tracks:Enhancing the quality and clarity of the knowledge base.Improving the coverage and specificity of search queries.These two measures will achieve the goal of improving the odds that the LLM has access to relevant facts and information, and is thus less likely to hallucinate or draw upon its own knowledge - which may be outdated or irrelevant.The diversity of methods is difficult to clarify in just a few sentences. Let's go straight to implementation to make things clearer.Figure 1: The RAG pipeline used by the author.Table of contentsOverviewTable of contentsSet-upIngesting, processing, and embedding documents Data ingestionSentence-level, token-wise chunkingMetadata inclusion and generation Keyphrases extracted by TextRankPotential questions generated by GPT-4oEntities extracted by SpacyComposite multi-field embeddingsIndexing to ElasticCat breakAppendixDefinitionsSet-upAll code may be found in the Searchlabs repo.First things first. You will need the following:An Elastic Cloud DeploymentAn LLM API - We are using a GPT-4o deployment on Azure OpenAI in this notebookPython Version 3.12.4 or laterWe will be running all the code from the main.ipynb notebook.Go ahead and git clone the repo, navigate to supporting-blog-content/advanced-rag-techniques, then run the following commands:Once that's done, create a .env file and fill out the following fields (Referenced in .env.example). Credits to my co-author, Claude-3.5, for the helpful comments.Next, we'll choose the document to ingest, and place it in the documents folder. For this article, we'll be using the Elastic N.V. Annual Report 2023. It's a pretty challenging and dense document, perfect for stress testing our RAG techniques.Elastic Annual Report 2023Now we're all set, let's go to ingestion. Open main.ipynb and execute the first two cells to import all packages and intialize all services.Back to topIngesting, processing, and embedding documentsData ingestionPersonal note: I am stunned by LlamaIndex's convenience. In the olden days before LLMs and LlamaIndex, ingesting documents of various formats was a painful process of collecting esoteric packages from all over. Now it's reduced to a single function call. Wild.The SimpleDirectoryReader will load every document in the directory_path. For .pdf files, it returns a list of document objects, which I convert to Python dictionaries because I find them easier to work with.Each dictionary contains the key content in the text field. It also contains useful metadata such as page number, filename, file size, and type.Back to topSentence-level, token-wise chunkingThe first thing to do is reduce our documents to chunks of a standard length (to ensure consistency and manageability). Embedding models have unique token limits (maximum input size they can process). Tokens are the basic units of text that models process. To prevent information loss (truncation or omission of content), we should provide text that does not exceed those limits (by splitting longer texts into smaller segments).Chunking has a significant impact on performance. Ideally, each chunk would represent a self-contained piece of information, capturing contextual information about a single topic. Chunking methods include word-level chunking, where documents are split by word count, and semantic chunking which uses an LLM to identify logical breakpoints.Word-level chunking is cheap, fast, and easy, but runs a risk of splitting sentences and thus breaking context. Semantic chunking gets slow and expensive, especially if you're dealing with documents like the 116-page Elastic Annual Report.Let's choose a middleground approach. Sentence level chunking is still simple, but can preserve context more effectively than word-level chunking while being significantly cheaper and faster. Additionally, we'll implement a sliding window to capture some of the surrounding context, and alleviate the impact of splitting paragraphs.The Chunker class takes in the embedding model's tokenizer to encode and decode text. We'll now build chunks of 512 tokens each, with an overlap of 20 tokens. To do this, we'll split the text into sentences, tokenize those sentences, and then add the tokenized sentences to our current chunk until we cannot add more without breaching our token limit.Finally, decode the sentences back to the original text for embedding, storing it in a field called original_text. Chunks are stored in a field called chunk. To reduce noise (aka useless documents), we will discard any documents smaller than 50 tokens in length.Let's run it over our documents:And get back chunks of text that look like this:Back to topMetadata inclusion and generationWe've chunked our documents. Now it's time to enrich the data. I want to generate or extract additional metadata. This additional metadata can be used to influence and enhance search performance.We'll define a DocumentEnricher class, whose role is to take in a list of documents (Python dictionaries), and a list of processor functions. These functions will run over the documents' original_text column, and store their outputs in new fields.First, we extract keyphrases using TextRank. TextRank is a graph-based algorithm that extracts key phrases and sentences from text by ranking their importance based on the relationships between words.Next, we'll generate potential_questions using GPT-4o.Finally, we'll extract entities using Spacy.Since the code for each of these is quite lengthy and involved, I will refrain from reproducing it here. If you are interested, the files are marked in the code samples below.Let's run the data enrichment:And take a look at the results:Keyphrases extracted by TextRankThese keyphrases are a stand-in for the chunk's core topics. If a query has to do with cybersecurity, this chunk's score will be boosted.Potential questions generated by GPT-4oThese potential questions may directly match with user queries, offering a boost in score. We prompt GPT-4o to generate questions which can be answered using the information found in the current chunk.Entities extracted by SpacyThese entities serve a similar purpose to the keyphrases, but capture organizations' and individuals' names, which keyphrase extraction may miss.Back to topComposite multi-field embeddingsNow that we have enriched our documents with additional metadata, we can leverage this information to create more robust and context-aware embeddings.Let's review our current point in the process. We've got four fields of interest in each document.Each field represents a different perspective on the document's context, potentially highlighting a key area for the LLM to focus on.Metadata Enrichment PipelineThe plan is to embed each of these fields, and then create a weighted sum of the embeddings, known as a Composite Embedding.With luck, this Composite Embedding will allow the system to become more context aware, in addition to introducing another tunable hyperparameter from controlling the search behavior.First, let's embed each field and update each document in place, using our locally defined embedding model imported at the beginning of the main.ipynb notebook.Each embedding function returns the embedding's field, which is just the original input field with an _embedding postfix.Let's now define the weightings of our composite embedding:The weightings allow you to assign priorities to each component, based on your usecase and the quality of your data. Intuitively, the size of these weightings is dependent on the semantic value of each component. Since the chunk text itself is by far the richest, I assign a weighting of 70%. Since the entities are the smallest, being just a list of org or person names, I assign it a weighting of 5%. The precise setting for these values has to be determined empirically, on a use-case by use-case basis.Finally, let's write a function to apply the weightings, and create our composite embedding. We'll delete all the component embeddings as well to save space.With this, we've completed our document processing. We now have a list of document objects which look like this:Indexing to ElasticLet's bulk upload our documents to Elastic Search. For this purpose, I long-ago defined a set of Elastic Helper functions in elastic_helpers.py. It is a very lengthy piece of code so let's sticking to looking at the function calls.es_bulk_indexer.bulk_upload_documents works with any list of dictionary objects, taking advantage of Elasticsearch's convenient dynamic mappings.Head on over to Kibana and verify that all documents have been indexed. There should be 224 of them. Not bad for such a large document!Indexed Annual Report Documents in KibanaBack to topCat breakLet's take a break, article's a little heavy, I know. Check out my cat:look at how furious she isAdorable. The hat went missing and I half suspect she stole and hid it somewhere :(Congrats on making it this far :)Join me in Part 2 for testing and evaluation of our RAG pipeline!AppendixDefinitions1. Sentence ChunkingA preprocessing technique used in RAG systems to divide text into smaller, meaningful units.Process: Input: Large block of text (e.g., document, paragraph)Output: Smaller text segments (typically sentences or small groups of sentences)Purpose: Creates granular, context-specific text segmentsAllows for more precise indexing and retrievalImproves the relevance of retrieved information in RAG systemsCharacteristics: Segments are semantically meaningfulCan be independently indexed and retrievedOften preserves some context to ensure standalone comprehensibilityBenefits: Enhances retrieval precisionEnables more focused augmentation in RAG pipelines2. HyDE (Hypothetical Document Embedding)A technique that uses an LLM to generate a hypothetical document for query expansion in RAG systems.Process: Input query to an LLMLLM generates a hypothetical document answering the queryEmbed the generated documentUse the embedding for vector searchKey difference: Traditional RAG: Matches query to documentsHyDE: Matches documents to documentsPurpose: Improve retrieval performance, especially for complex or ambiguous queriesCapture richer semantic context than a short queryBenefits: Leverages LLM's knowledge to expand queriesCan potentially improve relevance of retrieved documentsChallenges: Requires additional LLM inference, increasing latency and costPerformance depends on quality of generated hypothetical document3. Reverse PackingA technique used in RAG systems to reorder search results before passing them to the LLM.Process: Search engine (e.g., Elasticsearch) returns documents in descending order of relevance.The order is reversed, placing the most relevant document last.Purpose: Exploits the recency bias of LLMs, which tend to focus more on the latest information in their context.Ensures the most relevant information is \"freshest\" in the LLM's context window.Example: Original order: [Most Relevant, Second Most, Third Most, ...] Reversed order: [..., Third Most, Second Most, Most Relevant]4. Query ClassificationA technique to optimize RAG system efficiency by determining whether a query requires RAG or can be answered directly by the LLM.Process: Develop a custom dataset specific to the LLM in useTrain a specialized classification modelUse the model to categorize incoming queriesPurpose: Improve system efficiency by avoiding unnecessary RAG processingDirect queries to the most appropriate response mechanismRequirements: LLM-specific dataset and modelOngoing refinement to maintain accuracyBenefits: Reduces computational overhead for simple queriesPotentially improves response time for non-RAG queries5. SummarizationA technique to condense retrieved documents in RAG systems.Process: Retrieve relevant documentsGenerate concise summaries of each documentUse summaries instead of full documents in the RAG pipelinePurpose: Improve RAG performance by focusing on essential informationReduce noise and interference from less relevant contentBenefits: Potentially improves relevance of LLM responsesAllows for inclusion of more documents within context limitsChallenges: Risk of losing important details in summarizationAdditional computational overhead for summary generation6. Metadata InclusionA technique to enrich documents with additional contextual information.Types of metadata: KeyphrasesTitlesDatesAuthorship detailsBlurbsPurpose: Increase contextual information available to the RAG systemProvide LLMs with clearer understanding of document content and relevanceBenefits: Potentially improves retrieval accuracyEnhances LLM's ability to assess document usefulnessImplementation: Can be done during document preprocessingMay require additional data extraction or generation steps7. Composite Multi-Field EmbeddingsAn advanced embedding technique for RAG systems that creates separate embeddings for different document components.Process: Identify relevant fields (e.g., title, keyphrases, blurb, main content)Generate separate embeddings for each fieldCombine or store these embeddings for use in retrievalDifference from standard approach: Traditional: Single embedding for entire documentComposite: Multiple embeddings for different document aspectsPurpose: Create more nuanced and context-aware document representationsCapture information from a wider variety of sources within a documentBenefits: Potentially improves performance on ambiguous or multi-faceted queriesAllows for more flexible weighting of different document aspects in retrievalChallenges: Increased complexity in embedding storage and retrieval processesMay require more sophisticated matching algorithms8. Query EnrichmentA technique to expand the original query with related terms to improve search coverage.Process: Analyze the original queryGenerate synonyms and semantically related phrasesAugment the query with these additional termsPurpose: Increase the range of potential matches in the document corpusImprove retrieval performance for queries with specific or technical languageBenefits: Potentially retrieves relevant documents that don't exactly match the original query termsCan help overcome vocabulary mismatch between queries and documentsChallenges: Risk of query drift if not carefully implementedMay increase computational overhead in the retrieval processBack to topCopyShareReport an issueRelated ContentAIJanuary 20, 2026Context engineering vs. prompt engineeringLearn how context engineering and prompt engineering differ and why mastering both is essential for building production AI agents and RAG systems.TMBy: Tomás MurúaML ResearchAIJanuary 2, 2026Automating log parsing in Streams with MLLearn how a hybrid ML approach achieved 94% log parsing and 91% log partitioning accuracy through automation experiments with log format fingerprinting in Streams.NHBy: Nastia HavriushenkoAgentic AIAIDecember 31, 2025How to build an agent knowledge base with LangChain and ElasticsearchLearn how to build an agent knowledge base and test its ability to query sources of information based on context, use WebSearch for out-of-scope queries, and refine recommendations based on user intention.HCBy: Han Xiang ChoongDeveloper ExperienceAIDecember 29, 2025Creating reliable agents with structured outputs in ElasticsearchExplore what structured outputs are and how to leverage them in Elasticsearch to ground agents in the most relevant context for data contracts.JABy: JD ArmadaVector DatabaseDecember 23, 2025Comparing dense vector search performance with the Profile API in ElasticsearchLearn how to use the Profile API in Elasticsearch to compare dense vector configurations and tune kNN performance with visual data from Kibana.ADBy: Alexander Dávila","source":"web","publishedAt":"2024-08-14T14:58:13+08:00"},{"id":"bocha-2","title":"高级RAG技术第1部分:数据处理-腾讯云开发者社区-腾讯云","url":"https://cloud.tencent.com/developer/article/2446232","snippet":"点火三周高级RAG技术第1部分：数据处理原创关注作者腾讯云开发者社区文档建议反馈控制台登录/注册首页学习活动专区圈层工具MCP广场文章/答案/技术大牛搜索搜索关闭发布点火三周社区首页 >专栏 >高级RAG技术第1部分：数据处理高级RAG技术第1部分：数据处理原创点火三周关注发布于 2024-08-19 22:41:54发布于 2024-08-19 22:41:548630举报文章被收录于专栏：Elastic Stack专栏Elastic Stack专栏最近的论文《搜索增强生成中的最佳实践》通过实证研究评估了各种增强RAG技术的效果，旨在汇聚一套RAG的最佳实践。Wang Pipeline由Wang及其同事推荐的RAG管道。我们将实现一些这些最佳实践，特别是那些旨在提高搜索质量的技术（句子分块、HyDE、反向打包）。为了简洁起见，我们将省略那些专注于提高效率的技术（查询分类和摘要生成）。我们还将实现一些未涉及但我个人认为有用且有趣的技术（元数据包含、复合多字段嵌入、查询扩展）。最后，我们将进行一个简短的测试，以查看我们的搜索结果和生成的答案是否比基线有所改进。让我们开始吧！概述RAG旨在通过从外部知识库中检索信息来增强LLM（大语言模型）的生成答案。通过提供领域特定的信息，LLM可以快速适应其训练数据范围之外的用例；这比微调便宜得多，也更容易保持最新。改善RAG质量的措施通常集中在两个方面：提高知识库的质量和清晰度。改进搜索查询的覆盖范围和具体性。这两种措施将提高LLM访问相关事实和信息的几率，从而减少幻觉或依赖其自身可能过时或无关的知识。这些方法的多样性难以在几句话中澄清。让我们直接进入实现，以便更清楚地理解。Han Pipeline图1：作者使用的RAG管道。目录设置文档的摄取、处理和嵌入数据摄取句子级别、基于令牌的分块元数据包含与生成复合多字段嵌入附录定义猫咪休息设置所有代码可以在 Searchlabs仓库中找到。首先，你需要以下内容：一个Elastic云部署一个LLM API - 我们在此笔记本中使用的是Azure OpenAI上的GPT-4o部署Python版本3.12.4或更高版本我们将从main.ipynb笔记本运行所有代码。继续git clone这个仓库，导航到supporting-blog-content/advanced-rag-techniques，然后运行以下命令：代码语言：bash复制# 创建一个名为'rag_env'的新虚拟环境 python -m venv rag_env # 激活虚拟环境（对于基于Unix的系统） source rag_env/bin/activate # （对于Windows） .\\rag_env\\Scripts\\activate # 安装requirements.txt中列出的包 pip install -r requirements.txt完成后，创建一个.env文件并填写以下字段（参考.env.example）。感谢我的合著者Claude-3.5的有用评论。代码语言：bash复制# Elastic Cloud: 在Elastic Cloud控制台的“Deployment”页面找到 ELASTIC_CLOUD_ENDPOINT=\"\" ELASTIC_CLOUD_ID=\"\" # Elastic Cloud: 在部署设置期间或在“Security”设置中创建 ELASTIC_USERNAME=\"\" ELASTIC_PASSWORD=\"\" # Elastic Cloud: 在Kibana或通过API创建的索引名称 ELASTIC_INDEX_NAME=\"\" # Azure AI Studio: 在Azure OpenAI资源的“Keys and Endpoint”部分找到 AZURE_OPENAI_KEY_1=\"\" AZURE_OPENAI_KEY_2=\"\" AZURE_OPENAI_REGION=\"\" AZURE_OPENAI_ENDPOINT=\"\" # Azure AI Studio: 在Azure OpenAI资源的“Deployments”部分找到 AZURE_OPENAI_DEPLOYMENT_NAME=\"\" # 使用BAAI/bge-small-en-v1.5，因为我认为它在资源效率和性能之间取得了良好的平衡。 HUGGINGFACE_EMBEDDING_MODEL=\"BAAI/bge-small-en-v1.5\"接下来，我们将选择要摄取的文档，并将其放置在documents文件夹中。对于本文，我们将使用Elastic N.V. 2023年年度报告。这是一个相当具有挑战性和密集的文档，非常适合压力测试我们的RAG技术。Han PipelineElastic 2023年年度报告现在一切准备就绪，让我们开始进行摄取。打开main.ipynb并执行前两个单元格以导入所有包并初始化所有服务。返回顶部文档的摄取、处理和嵌入数据摄取个人注释：LlamaIndex的便利性让我惊叹不已。在没有LLMs和LlamaIndex的旧时代，摄取各种格式的文档是一个痛苦的过程，需要从各处收集晦涩的包。现在，它只需一个函数调用。真是太神奇了。SimpleDirectoryReader将加载directory_path中的所有文档。对于.pdf文件，它返回一个文档对象列表，我将其转换为Python字典，因为我发现它们更容易处理。代码语言：python复制# llamaindex_processor.py from llama_index.core import SimpleDirectoryReader class LlamaIndexProcessor: def __init__(self): pass def load_documents(self, directory_path): '''加载目录中的所有文档''' reader = SimpleDirectoryReader(input_dir=directory_path) return reader.load_data() # main.ipynb llamaindex_processor = LlamaIndexProcessor() documents = llamaindex_processor.load_documents('./documents/') documents = [dict(doc_obj) for doc_obj in documents]每个字典包含text字段中的关键内容。它还包含一些有用的元数据，例如页码、文件名、文件大小和类型。代码语言：python复制{ 'id_': '5f76f0b3-22d8-49a8-9942-c2bbab14f63f', 'metadata': { 'page_label': '5', 'file_name': 'Elastic_NV_Annual-Report-Fiscal-Year-2023.pdf', 'file_path': '/Users/han/Desktop/Projects/truckasaurus/documents/Elastic_NV_Annual-Report-Fiscal-Year-2023.pdf', 'file_type': 'application/pdf', 'file_size': 3724426, 'creation_date': '2024-07-27', 'last_modified_date': '2024-07-27' }, 'text': '目录\\n页码\\n第一部分\\n项目1. 业务 3\\n15 项目1A. 风险因素\\n项目1B. 未解决的员工意见 48\\n项目2. 物业 48\\n项目3. 法律诉讼 48\\n项目4. 矿山安全披露 48\\n第二部分\\n项目5. 登记人普通股的市场、相关股东事项和发行人股票购买 49\\n项目6. [保留] 49\\n项目7. 财务状况和经营成果的管理层讨论与分析 50\\n项目7A. 关于市场风险的定量和定性披露 64\\n项目8. 财务报表和补充数据 66\\n项目9. 关于会计和财务披露的会计师变更和分歧 100\\n100\\n101 项目9A. 控制和程序\\n项目9B. 其他信息\\n项目9C. 关于防止检查的外国司法管辖区的披露 101\\n第三部分\\n102\\n102\\n102\\n102 项目10. 董事、高级管理人员和公司治理\\n项目11. 高级管理人员薪酬\\n项目12. 某些受益所有人和管理层的证券持有情况和相关股东事项\\n项目13. 某些关系和相关交易及董事独立性\\n项目14. 主要会计师费用和服务 102\\n第四部分\\n103\\n105 项目15. 附件和财务报表附表\\n项目16. 10-K表格摘要\\n签名 106\\ni', ... }返回顶部句子级别、基于令牌的分块首先，我们需要将文档减少到标准长度的块（以确保一致性和可管理性）。嵌入模型有唯一的令牌限制（它们可以处理的最大输入大小）。令牌是模型处理的基本文本单位。为了防止信息丢失（截断或遗漏内容），我们应提供不超过这些限制的文本（通过将较长的文本拆分为较小的段）。分块对性能有显著影响。理想情况下，每个块都应代表一个自包含的信息块，捕捉到单个主题的上下文信息。分块方法包括基于词汇的分块，其中文档按词数拆分，以及语义分块，它使用LLM识别逻辑断点。基于词汇的分块便宜、快速且简单，但有可能拆分句子，从而破坏上下文。语义分块变得缓慢且昂贵，特别是如果你处理像116页的Elastic年度报告这样的文档。让我们选择一种折中的方法。句子级分块仍然简单，但比基于词汇的分块更有效地保留上下文，同时成本和速度显著降低。此外，我们将实现一个滑动窗口，以捕捉周围的一些上下文，缓解拆分段落的影响。代码语言：python复制# chunker.py import uuid import re class Chunker: def __init__(self, tokenizer): self.tokenizer = tokenizer def split_into_sentences(self, text): \"\"\"将文本拆分成句子。\"\"\" return re.split(r'(?<=[.!?])\\s+', text) def sentence_wise_tokenized_chunk_documents(self, documents, chunk_size=512, overlap=20, min_chunk_size=50): ''' 1. 将文本拆分成句子。 2. 使用提供的分词器方法进行分词。 3. 构建最多chunk_size限制的块。 4. 基于令牌创建重叠 - 保留上下文。 5. 只保留符合最小令牌大小要求的块。 ''' chunked_documents = [] for doc in documents: sentences = self.split_into_sentences(doc['text']) tokens = [] sentence_boundaries = [0] # 分词所有句子并跟踪句子边界 for sentence in sentences: sentence_tokens = self.tokenizer.encode(sentence, add_special_tokens=True) tokens.extend(sentence_tokens) sentence_boundaries.append(len(tokens)) # 创建块 chunk_start = 0 while chunk_start < len(tokens): chunk_end = chunk_start + chunk_size # 找到块中合适的最后一个完整句子 sentence_end = next((i for i in sentence_boundaries if i > chunk_end), len(tokens)) chunk_end = min(chunk_end, sentence_end) # 创建块 chunk_tokens = tokens[chunk_start:chunk_end] # 检查块是否符合最小大小要求 if len(chunk_tokens) >= min_chunk_size: # 为此块创建一个新的文档对象 chunk_doc = { 'id_': str(uuid.uuid4()), 'chunk': chunk_tokens, 'original_text': self.tokenizer.decode(chunk_tokens), 'chunk_index': len(chunked_documents), 'parent_id': doc['id_'], 'chunk_token_count': len(chunk_tokens) } # 从原始文档复制所有其他字段 for key, value in doc.items(): if key != 'text' and key not in chunk_doc: chunk_doc[key] = value chunked_documents.append(chunk_doc) # 移动到下一个块开始，考虑重叠 chunk_start = max(chunk_start + chunk_size - overlap, chunk_end - overlap) return chunked_documents # main.ipynb # 初始化嵌入模型 HUGGINGFACE_EMBEDDING_MODEL = os.environ.get('HUGGINGFACE_EMBEDDING_MODEL') embedder = EmbeddingModel(model_name=HUGGINGFACE_EMBEDDING_MODEL) # 初始化分块器 chunker = Chunker(embedder.tokenizer)Chunker类接收嵌入模型的分词器来编码和解码文本。我们现在将构建每个512个令牌的块，重叠20个令牌。为此，我们将文本拆分成句子，对这些句子进行分词，然后将分词后的句子添加到当前块中，直到无法再添加而不超过令牌限制。最后，将句子解码回原始文本进行嵌入，并将其存储在名为original_text的字段中。块存储在名为chunk的字段中。为了减少噪音（即无用的文档），我们将丢弃任何小于50个令牌的文档。让我们在我们的文档上运行它：代码语言：python复制chunked_documents = chunker.sentence_wise_tokenized_chunk_documents(documents, chunk_size=512)并得到类似这样的文本块：代码语言：python复制print(chunked_documents[4]['original_text']) [CLS] the aggregate market value of the ordinary shares held by non - affiliates of the registrant, based on the closing price of the shares of ordinary shares on the new york stock exchange on october 31, 2022 ( the last business day of the registrant ’ s second fiscal quarter ), was approximately $ 6. 1 billion. [SEP] [CLS] as of may 31, 2023, the registrant had 97, 390, 886 ordinary shares, par value €0. 01 per share, outstanding. [SEP] [CLS] documents incorporated by reference portions of the registrant ’ s definitive proxy statement relating to the registrant ’ s 2023 annual general meeting of shareholders are incorporated by reference into part iii of this annual ......返回顶部元数据包含与生成我们已经对文档进行了分块。现在是时候丰富数据了。我想生成或提取额外的元数据。这些额外的元数据可以用于影响和增强搜索性能。我们将定义一个DocumentEnricher类，其作用是接收一个文档列表（Python字典）和一个处理函数列表。这些函数将在文档的original_text列上运行，并将其输出存储在新字段中。首先，我们使用TextRank提取关键短语。TextRank是一种基于图的算法，通过根据单词之间的关系对它们的重要性进行排序，从文本中提取关键短语和句子。接下来，我们使用GPT-4o生成潜在问题。最后，我们使用Spacy提取实体。由于每个文件的代码都相当冗长且复杂，我将在这里避免重复。如果你有兴趣，文件在下面的代码示例中标记。让我们运行数据丰富化：好的，我会根据你的需求对这篇文章进行优化，使其更加适合初学者阅读，并确保表达清晰流畅。代码语言：python复制# documentenricher.py from tqdm import tqdm class DocumentEnricher: def __init__(self): pass def enrich_document(self, documents, processors, text_col='text'): for doc in tqdm(documents, desc=\"Enriching documents using processors: \" + str(processors)): for (processor, field) in processors: metadata = processor(doc[text_col]) if isinstance(metadata, list): metadata = '\\n'.join(metadata) doc.update({field: metadata}) # main.ipynb # 初始化处理器类 nltkprocessor = NLTKProcessor() # nltk_processor.py entity_extractor = EntityExtractor() # entity_extractor.py gpt4o = LLMProcessor(model='gpt-4o') # llm.py # 初始化文档增强器 documentenricher = DocumentEnricher() # 在文档中创建新的字段 - 这些是处理器函数的输出。 processors = [ (nltkprocessor.textrank_phrases, \"keyphrases\"), (gpt4o.generate_questions, \"potential_questions\"), (entity_extractor.extract_entities, \"entities\") ] # .enrich_document() 将会直接修改 chunked_docs。 # 为了查看结果，我们将在接下来的几个单元中打印 chunked_docs！ documentenricher.enrich_document(chunked_docs, text_col='original_text', processors=processors)让我们看看结果：TextRank 提取的关键词这些关键词代表了段落的核心主题。如果查询与网络安全有关，这段内容的评分将会提高。代码语言：javascript复制print(chunked_documents[25]['keyphrases']) 'elastic agent stop', 'agent stop malware', 'stop malware ransomware', 'malware ransomware environment', 'ransomware environment wide', 'environment wide visibility', 'wide visibility threat', 'visibility threat detection', 'sep cl key', 'cl key feature'GPT-4o 生成的潜在问题这些潜在问题可能会直接匹配用户查询，从而提高评分。我们提示 GPT-4o 生成可以用当前段落信息回答的问题。代码语言：javascript复制print(chunked_documents[25]['potential_questions']) 1. Elastic Agent 在网络安全方面的主要功能是什么？ 2. 描述 Logstash 如何在 IT 环境中贡献数据管理。 3. 列出并解释文档中提到的 Logstash 的关键特性。 4. Elastic Agent 如何增强威胁检测中的环境可见性？ 5. Logstash 提供哪些超越简单数据收集的功能？ 6. 文档中如何建议 Elastic Agent 阻止恶意软件和勒索软件？ 7. 能否识别 Elastic Agent 和 Logstash 在集成环境中的功能关系？ 8. Elastic Agent 的高级威胁检测能力对组织安全政策有何影响？ 9. 比较和对比 Elastic Agent 和 Logstash 的描述功能。 10. Logstash 的集中收集能力如何支持 Elastic Agent 的威胁检测能力？Spacy 提取的实体这些实体类似于关键词，但捕捉组织和个人的名字，而关键词提取可能会遗漏这些信息。代码语言：javascript复制print(chunked_documents[29]['entities']) 'appdynamics', 'apm data', 'azure sentinel', 'microsoft', 'mcafee', 'broadcom', 'cisco', 'dynatrace', 'coveo', 'lucidworks'返回顶部复合多字段嵌入现在我们已经用额外的元数据丰富了文档，可以利用这些信息创建更强大和上下文感知的嵌入。让我们回顾一下当前的处理进展。我们在每个文档中有四个感兴趣的字段。代码语言：javascript复制{ \"chunk\": \"...\", \"keyphrases\": \"...\", \"potential_questions\": \"...\", \"entities\": \"...\" }每个字段代表了文档上下文的不同视角，可能突出 LLM 需要关注的关键区域。Han Pipeline元数据增强流程计划是对每个字段进行嵌入，然后创建这些嵌入的加权和，称为复合嵌入。希望这个复合嵌入能使系统更加上下文感知，并引入另一个可调超参数以控制搜索行为。首先，让我们对每个字段进行嵌入，并使用我们在 main.ipynb 中定义的嵌入模型更新每个文档。代码语言：javascript复制# 在 embedding_model.py 中定义的嵌入模型 embedder = EmbeddingModel(model_name=HUGGINGFACE_EMBEDDING_MODEL) cols_to_embed = ['keyphrases', 'potential_questions', 'entities'] embedding_cols = [] for col in cols_to_embed: # 处理文本输入 embedding_col = embedder.embed_documents_text_wise(chunked_documents, text_field=col) embedding_cols.append(embedding_col) # 处理 token 输入 embedding_col = embedder.embed_documents_token_wise(chunked_documents, token_field=\"chunk\") embedding_cols.append(embedding_col)每个嵌入函数返回嵌入的字段，该字段是原始输入字段加上 _embedding 后缀。现在让我们定义复合嵌入的权重：代码语言：javascript复制embedding_cols = [ 'keyphrases_embedding', 'potential_questions_embedding', 'entities_embedding', 'chunk_embedding' ] combination_weights = [ 0.1, 0.15, 0.05, 0.7 ]权重允许你根据使用情况和数据质量为每个组件分配优先级。直观上，这些权重的大小取决于每个组件的语义价值。由于 chunk 文本本身最为丰富，我分配了 70% 的权重。因为实体是最小的，只是组织或个人名称的列表，所以我分配了 5% 的权重。这些值的精确设置需要根据具体使用情况进行实证确定。最后，让我们编写一个函数来应用这些权重，并创建我们的复合嵌入。同时删除所有组件嵌入以节省空间。代码语言：javascript复制from tqdm import tqdm def combine_embeddings(objects, embedding_cols, combination_weights, primary_embedding='primary_embedding'): # 确保权重数量与嵌入列数量匹配 assert len(embedding_cols) == len(combination_weights), \"嵌入列数量必须与权重数量匹配\" # 归一化权重使其总和为 1 weights = np.array(combination_weights) / np.sum(combination_weights) for obj in tqdm(objects, desc=\"Combining embeddings\"): # 初始化复合嵌入 combined = np.zeros_like(obj[embedding_cols[0]]) # 计算加权和 for col, weight in zip(embedding_cols, weights): combined += weight * np.array(obj[col]) # 将新的复合嵌入添加到对象中 obj.update({primary_embedding: combined.tolist()}) # 删除原始嵌入列 for col in embedding_cols: obj.pop(col, None) combine_embeddings(chunked_documents, embedding_cols, combination_weights)至此，我们完成了文档处理。现在我们有一个文档对象列表，它们的结构如下：代码语言：javascript复制{ 'id_': '7fe71686-5cd0-4831-9e79-998c6dbeae0c', 'chunk': [2312, 14613, ...], 'original_text': 'if an emerging growth company, indicate by check mark if the registrant has elected not to use the extended ...', 'chunk_index': 3, 'chunk_token_count': 399, 'metadata': { 'page_label': '3', 'file_name': 'Elastic_NV_Annual-Report-Fiscal-Year-2023.pdf', ... }, 'keyphrases': 'sep cl unk\\ncheck mark registrant\\ncl unk indicate\\nunk indicate check\\nindicate check mark\\nprincipal executive office\\naccelerate filer unk\\ncompany unk emerge\\nunk emerge growth\\nemerge growth company', 'potential_questions': '1. What are the different types of registrant statuses mentioned in the document?\\n2. Under what section of the Sarbanes-Oxley Act must registrants file a report on the effectiveness of their internal ...', 'entities': 'the effectiveness of\\nsection 13\\nSEP\\nUNK\\nsection 21e\\n1934\\n1933\\nu. s. c.\\nsection 404\\nsection 12\\nal', 'primary_embedding': [-0.3946287803351879, -0.17586839850991964, ...] }索引到 Elastic让我们将文档批量上传到 Elasticsearch。为此，我早在 elastic_helpers.py 中定义了一组 Elastic 辅助函数。这是一段很长的代码，所以我们只看函数调用。es_bulk_indexer.bulk_upload_documents 适用于任何字典对象列表，利用 Elasticsearch 的动态映射。代码语言：javascript复制# 初始化 Elasticsearch ELASTIC_CLOUD_ID = os.environ.get('ELASTIC_CLOUD_ID') ELASTIC_USERNAME = os.environ.get('ELASTIC_USERNAME') ELASTIC_PASSWORD = os.environ.get('ELASTIC_PASSWORD') ELASTIC_CLOUD_AUTH = (ELASTIC_USERNAME, ELASTIC_PASSWORD) es_bulk_indexer = ESBulkIndexer(cloud_id=ELASTIC_CLOUD_ID, credentials=ELASTIC_CLOUD_AUTH) es_query_maker = ESQueryMaker(cloud_id=ELASTIC_CLOUD_ID, credentials=ELASTIC_CLOUD_AUTH) # 定义索引名称 index_name = os.environ.get('ELASTIC_INDEX_NAME') # 创建索引并批量上传 index_exists = es_bulk_indexer.check_index_existence(index_name=index_name) if not index_exists: logger.info(f\"Creating new index: {index_name}\") es_bulk_indexer.create_es_index(es_configuration=BASIC_CONFIG, index_name=index_name) success_count = es_bulk_indexer.bulk_upload_documents( index_name=index_name, documents=chunked_documents, id_col='id_', batch_size=32 )前往 Kibana 验证所有文档已被索引。应该有 224 个文档。对于如此大的文档来说，这个结果还不错！Han Pipeline在 Kibana 中索引的年度报告文档原创声明：本文系作者授权腾讯云开发者社区发表，未经许可，不得转载。如有侵权，请联系 cloudcommunity@tencent.com 删除。Elasticsearch Service原创声明：本文系作者授权腾讯云开发者社区发表，未经许可，不得转载。如有侵权，请联系 cloudcommunity@tencent.com 删除。Elasticsearch Service评论登录后参与评论0 条评论热度最新登录 后参与评论推荐阅读目录概述目录设置文档的摄取、处理和嵌入数据摄取句子级别、基于令牌的分块元数据包含与生成TextRank 提取的关键词GPT-4o 生成的潜在问题Spacy 提取的实体复合多字段嵌入索引到 Elastic相关产品与服务Elasticsearch Service腾讯云 Elasticsearch Service（ES）是集文本搜索+向量搜索+AI 能力于一体的云端智能混合搜索服务，提供集群托管和 Serverless 两种模式，助您高效构建多样化搜索（文本/语义/图片/视频/地理空间等）、RAG 知识问答、日志分析、运维监控等应用。产品介绍产品文档ES特惠专场，新客首购1折起！ 领券社区技术文章技术问答技术沙龙技术视频学习中心技术百科技术专区活动自媒体同步曝光计划邀请作者入驻自荐上首页技术竞赛圈层腾讯云最具价值专家腾讯云架构师技术同盟腾讯云创作之星腾讯云TDP关于社区规范免责声明联系我们友情链接MCP广场开源版权声明腾讯云开发者扫码关注腾讯云开发者领取腾讯云代金券热门产品域名注册云服务器区块链服务消息队列网络加速云数据库域名解析云存储视频直播热门推荐人脸识别腾讯会议企业云CDN加速视频通话图像分析MySQL 数据库SSL 证书语音识别更多推荐数据安全负载均衡短信文字识别云点播大数据小程序开发网站监控数据迁移Copyright © 2013 - 2026 Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 深圳市腾讯计算机系统有限公司 ICP备案/许可证号：粤B2-20090059 粤公网安备44030502008569号腾讯云计算（北京）有限责任公司 京ICP证150476号 | 京ICP备11018762号问题归档专栏文章快讯文章归档关键词归档开发者手册归档开发者手册 Section 归档Copyright © 2013 - 2026 Tencent Cloud.All Rights Reserved. 腾讯云 版权所有登录 后参与评论000推荐","source":"web","publishedAt":"2024-08-19T22:41:54+08:00"},{"id":"bocha-3","title":"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models - ADS","url":"https://ui.adsabs.harvard.edu/abs/2024arXiv240117043L/abstract","snippet":"ADS Abstract Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of l","source":"web","publishedAt":"2025-02-02T15:47:14+08:00"},{"id":"bocha-4","title":"GitHub - yangxikun/CRUD_RAG: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models","url":"https://github.com/yangxikun/CRUD_RAG","snippet":"Name Name Last commit message Last commit date Latest commit   History 20 Commits data data     src ","source":"web","publishedAt":"2024-12-21T22:05:34+08:00"},{"id":"bocha-5","title":"RAG流程优化(微调)的4个基本策略-腾讯云开发者社区-腾讯云","url":"https://cloud.tencent.com/developer/article/2433287","snippet":"在本文中,我们将介绍使用私有数据优化检索增强生成(RAG)的四种策略,可以提升生成任务的质量和准确性。通过使用一些优化策略,可以有效提升检索增强生成系统的性能和输出质量,使其在实际应用中能够更好地满足","source":"web","publishedAt":"2024-07-01T15:02:24+08:00"},{"id":"bocha-6","title":"Searching for Best Practices in Retrieval-Augmented Generation","url":"https://arxiv.org/html/2407.01219v1","snippet":"Xiaohua Wang,   Zhenghua Wang ,   Xuan Gao ,   Feiran Zhang , Yixin Wu ,   Zhibo Xu ,   Tianyuan Shi","source":"web","publishedAt":"2024-06-08T18:15:11+08:00"},{"id":"bocha-7","title":"RAG系统落地避坑指南:10条血与泪的教训_人工智能_ju7ran-葡萄城开发者空间","url":"https://grapecity.csdn.net/68a41151080e555a88dad25e.html","snippet":"RAG系统落地避坑指南:10条血与泪的教训\n本文较长,建议点赞收藏,以免遗失。更多AI大模型开发\n学习视频/籽料/面试题\n都在这>>Github<<\n>>gitee<<\n一、引言\n在当今\nAI\n领域,","source":"web","publishedAt":"2025-08-19T13:53:18+08:00"},{"id":"bocha-8","title":"构建企业级 RAG 系统的高级指南 [译]_galieo rag-CSDN博客","url":"https://blog.csdn.net/KQe397773106/article/details/136569442","snippet":"构建企业级 RAG 系统的高级指南 [译] 原文:Mastering RAG: How To Architect An Enterprise RAG System 欢迎再次加入我们的“RAG 系统高级","source":"web","publishedAt":"2024-03-08T18:30:24+08:00"},{"id":"bocha-9","title":"CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning","url":"https://www.mdpi.com/2079-9292/14/1/47","snippet":"Author / Affiliation / Email by Kehan Xu Kehan Xu Kun Zhang Kun Zhang Jingyuan Li Jingyuan Li Wei ","source":"web","publishedAt":"2024-11-20T23:49:01+08:00"},{"id":"bocha-0","title":"Enterprise Architecture-Based Project Model for AI Service System Development  SpringerLink","url":"https://link.springer.com/chapter/10.1007/978-3-031-08202-3_7","snippet":"Abstract In this chapter, we consider projects in which enterprise service systems are developed usi","source":"web","publishedAt":"2022-09-04T05:31:15+08:00"},{"id":"bocha-1","title":"deepseek","url":"https://deepseek-en.com/","snippet":"DeepSeek AI provides innovative enterprise artificial intelligence systems driven \nOur robust AI sol","source":"web","publishedAt":"2025-03-18T08:00:00+08:00"},{"id":"bocha-2","title":"CHOOSE metamodel [7]  Download Scientific Diagram","url":"https://www.researchgate.net/figure/CHOOSE-metamodel-7_fig1_268504769","snippet":"Source publication Evaluating and Improving the Visualisation of CHOOSE, an Enterprise Architecture ","source":"web","publishedAt":"2022-07-04T10:16:44+08:00"},{"id":"bocha-3","title":"Single viewpoint Fig. 6. Pairwise relationships  Download Scientific Diagram","url":"https://www.researchgate.net/figure/Single-viewpoint-Fig-6-Pairwise-relationships_fig5_268504769","snippet":"Source publication Evaluating and Improving the Visualisation of CHOOSE, an Enterprise Architecture ","source":"web","publishedAt":"2022-09-21T15:07:51+08:00"},{"id":"bocha-4","title":"EAT-NAS: elastic architecture transfer for accelerating large-scale neural architecture search  Science China Information Sciences","url":"https://link.springer.com/article/10.1007/s11432-020-3112-8","snippet":"Abstract Neural architecture search (NAS) methods have been proposed to relieve human experts from t","source":"web","publishedAt":"2021-08-06T14:53:04+08:00"},{"id":"bocha-5","title":"Home  INFINI Labs","url":"http://www.infinilabs.com/","snippet":"AI-Powered Enterprise Search One-Stop Enterprise Search Infrastructure Mission Critical Real-Time Se","source":"web","publishedAt":"2025-01-30T22:43:36+08:00"},{"id":"bocha-6","title":"Pre-defined Searches  Enterprise Architect User Guide","url":"https://www.sparxsystems.com/enterprise_architect_user_guide/13.0/model_navigation/pre-defined_search_definitions.html","snippet":"Enterprise Architect provides a range of pre-defined searches, grouped according to type. Each searc","source":"web","publishedAt":"2025-01-15T12:02:33+08:00"},{"id":"bocha-7","title":"An architecture for distributed enterprise data mining - 道客巴巴","url":"https://www.doc88.com/p-489420293564.html","snippet":"下载积分: 900 内容提示: An Architecture for Distributed Enterprise Data MiningJ. Chattratichat, J. Darlingto","source":"web","publishedAt":"2012-07-09T02:25:10+08:00"},{"id":"bocha-8","title":"Integrate with Sparx Systems Enterprise Architect ... - Esri Community","url":"https://community.esri.com/ideas/2161?commentID=25355","snippet":"Select to view content in your preferred language Translate Now Follow this Idea Labels (1) Labels E","source":"web","publishedAt":"2010-07-29T09:58:00+08:00"},{"id":"bocha-9","title":"Building Enterprise AI Solutions with DeepSeek Models培训","url":"https://www.nobleprog.cn/cc/beaisdeepseek","snippet":"Choose Sales Area 中国大陆 北京 上海 珠海 广州 深圳 成都 杭州 Building Enterprise AI Solutions with DeepSeek Models培训 ","source":"web","publishedAt":"2025-03-27T16:35:10+08:00"},{"id":"onebound-0","title":"《晚安咖啡》科技夜话:微软AI类人意识预警,Meta大规模AI裁员,OpenAI推企业知识库","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=NJ5S4ifBBcl8qVUGER0bGmqyVKFBMeRxbw7-rYWMN6ywOGxkpYaSCvfdKgX71Wq3PhpVJm3bc0Inngjt4Vu8UbhPywCv4TQxx3BxdNEJFYF-u3RqtatvPwLMDcB0oiYS&new=1","snippet":"the system also provides cited answers. Support for Asana, GitLab,... Gemini Enterprise in the corporate AI space.@@@According to ...","source":"wechat"},{"id":"onebound-1","title":"RegData语义保护体系深度解析:金融AI时代的数据安全范式革命","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=O5h7s1zuYz6gnDP9PPTSse2bjwyp6TEg-U3SL6D09CKMIx2oA8tNru6QACqG8aEeo-BJO2gQ2W5vK-PsfdIz-QOpOnJruIOYU3mRd0lAZe6tYcMqO78GX7bmjP2rFy9v&new=1","snippet":"《Architectures for the Intelligent AI-Ready Enterprise》一书中提出... (System Architecture)RegData体系并非空中楼阁,其背后是坚...","source":"wechat"},{"id":"onebound-2","title":"【品牌展商】&ldquo;AI智能决策系统服务商&rdquo; 云杉智控,3月亮相EAI SHOW 2026杭州展","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=lrt7jhyRqibjF*chszpYcrK99Atp7dKcwkscezFT3DHEdpJIEWDsPgNAoTk0T5WiVYw5*RH5UNirVO2KIN4D8BLR6wALA4WklTcUwzJXw0DF1MA3qpij7xqDMeMJVY-g&new=1","snippet":"successfully built a technical architecture with AI intelligent decision system as the core, forming key technologies such as data ...","source":"wechat"},{"id":"onebound-3","title":"【6月上旬】心仪offer在路上,最新北美热门岗位请查收!","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=RP0xoWrsxedtodVGwSGWBGkOnTlaSiFcbL*0hIGlOxA2VweRmPRo*iApKwzg*ROMDIc7vk74*M87Mh0h28dYtXc3mH4q4rC2K4Yfghcr1HGql1Thj-Cjy9fAIwKQWtJD&new=1","snippet":"Search Position Code:5509Location:Mountain ... embedded system hardware architecture, processor architecture, ...","source":"wechat"},{"id":"onebound-4","title":"加速数据中心基础架构之标准化及开放共建,从服务器走向边缘计算和物联网","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=Zjd4JSTDa0aPUNtJVDf0R55Ucj9OWFtC5OkrBmxUSzJ60Da2fbAjuTthyHR2s36rzXBT3dyK9c-4XeFUQXCU4AcxtNoGNYHh-b*XZvVt*5k79axJuBhiOpNvnJf0YWbU&new=1","snippet":"ACS test architecture manual in the ACS test repository (arm-enterprise-acs), and also you can visit Server systems &ndash; Arm Developer to ...","source":"wechat"},{"id":"onebound-5","title":"N2S: Why Orchestrating Systems Transform (1) (英/中)","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=lpzvHt-9x1bPPoCCpYeNniWaRw3CYQJevJplWfWziiDwKDnoy3jPrCKDXrtLJtsrxVTwyJV5QGSvvR8TRX74cRVboyd5*bdokdBB5uivrGRI6VAPRdpBFDykcuJu-wvl&new=1","snippet":"Enterprise‑grade infrastructure for  anyone, on demand- Startup ... System architecture&mdash;  messy, collaborative, and iterative&mdash;  doesn...","source":"wechat"},{"id":"onebound-6","title":"6.30-7.15截稿CCF会议交流群13条, 最高录用率46.84%! (附交流群)","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=HCm6rypq2NKN0fS0tHyTWyw7fWWNgp0ZDPi4-Kpuujq3EtGgpIctxwau8aTZ3BiT1YkARMxU*pQJXVQKbDI03NVkQ-1LSqnV80jLb6qdjYn7seLxQicJh7AJ6UBCXLNL&new=1","snippet":"web search, crowdsourcing applications, streaming applications)Scalable system software for various architectures (e.g., OpenPower, ...","source":"wechat"},{"id":"onebound-7","title":"NVIDIA 社招 | 致高考生:寻找新方向,快来 NVIDIA 软件专场做足功课吧!","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=1Mr3vww2GpE3eP1pWojarf6-Zve98YqI6emrmQ01F3T197w1lo1WL8INyk17gStC1JbVYSbNV65Ci47TCSLEVNEZxmy31UgVM4Y2QxnKlpoG9Gc7MetjFPCd212cDnA6&new=1","snippet":"Ubuntu rootfile system/Debian packaging/Bazel/docker 等相关经验... SQLSenior Enterprise Solution Engineer - AI深圳&bull; Develop ...","source":"wechat"},{"id":"onebound-8","title":"英伟达7月末最新社招岗位大汇总 - 可内推","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=0GyoX4CgvmKsEZm*J4*7Mp1iKC8G3BNVZSmEinS2woQUefSPhDO8cc2WDPA07PTeio*kLLgGKNIv499fWe97nT3OgqkL2a-teZsKKWjOV3bHElvYxJaQqBZI9hB99Wlb&new=1","snippet":"Enterprise Application Engineer (Open)&bull; 具有服务器系统架构相... Autonomous System Architecture Engineer&bull; MS, or PhD in ...","source":"wechat"},{"id":"onebound-9","title":"企业数字化转型和升级:架构设计方法与实践","url":"https://mp.weixin.qq.com/s?src=11&timestamp=1768982935&ver=6493&signature=2tO9lO3wgNA3GKIcrIKhrJu28P*OGthveH3MP9SXydBQKVOi0MIUzGHpLCfU0EfAVbwJCV1uksLcAHl5tM2ExYYdmUUQ36QwQCsfcUD*GlEQqBNyPvJkzmuAKNdvClC5&new=1","snippet":"Enterprise Architecture Methodologies》所述对四种主流的企业架... (5 System Design fundamentals)《人月神话》8 胸有成竹(...","source":"wechat"}]}